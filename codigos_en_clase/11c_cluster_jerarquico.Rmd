---
title: "Cluster Jer√°rquico en R"
author: "Nicol√°s Sidicaro"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    df_print: paged
  html_notebook:
    toc: true
    toc_float: true
    theme: flatly
    highlight: tango
subtitle: Un Enfoque Pr√°ctico para Aprendizaje No Supervisado
---

# Clustering Jer√°rquico - An√°lisis Completo en R
# Basado en "An Introduction to Statistical Learning"

## Instalaci√≥n y carga de librer√≠as necesarias

```{r setup, include=FALSE}
# Instalar paquetes si no est√°n instalados
required_packages <- c("cluster", "factoextra", "dendextend", "corrplot", 
                      "ggplot2", "gridExtra", "RColorBrewer", "NbClust",
                      "fpc", "dplyr", "reshape2", "tibble", "pheatmap", "fossil")

for(pkg in required_packages) {
  if(!require(pkg, character.only = TRUE)) {
    install.packages(pkg)
    library(pkg, character.only = TRUE)
  }
}
```

```{r libraries, message=FALSE, warning=FALSE}
library(cluster)      # Para clustering
library(factoextra)   # Para visualizaci√≥n de clusters
library(dendextend)   # Para manipular dendrogramas
library(corrplot)     # Para matriz de correlaci√≥n
library(ggplot2)      # Para gr√°ficos
library(gridExtra)    # Para organizar gr√°ficos
library(RColorBrewer) # Para paletas de colores
library(NbClust)      # Para determinar n√∫mero √≥ptimo de clusters
library(fpc)          # Para estad√≠sticas de clustering
library(dplyr)        # Para manipulaci√≥n de datos
library(reshape2)     # Para reestructurar datos
library(tibble)       # Para column_to_rownames
library(pheatmap)     # Para heatmaps
library(fossil)       # Para Adjusted Rand Index
```

## 1. Preparaci√≥n de los Datos

Usaremos m√∫ltiples datasets para mostrar diferentes aspectos del clustering jer√°rquico.

```{r data_preparation}
# Dataset 1: USArrests (datos reales con diferentes escalas)
data("USArrests")
head(USArrests)
summary(USArrests)

# Verificar si hay valores faltantes
sum(is.na(USArrests))

# Dataset 2: Datos sint√©ticos para mostrar diferentes estructuras
set.seed(123)
# Crear clusters bien separados
cluster1 <- data.frame(x = rnorm(30, mean = 2, sd = 0.5), 
                       y = rnorm(30, mean = 2, sd = 0.5))
cluster2 <- data.frame(x = rnorm(30, mean = 6, sd = 0.5), 
                       y = rnorm(30, mean = 6, sd = 0.5))
cluster3 <- data.frame(x = rnorm(30, mean = 2, sd = 0.5), 
                       y = rnorm(30, mean = 6, sd = 0.5))

synthetic_data <- rbind(cluster1, cluster2, cluster3)
synthetic_data$true_cluster <- rep(1:3, each = 30)

# Visualizar datos sint√©ticos
ggplot(synthetic_data, aes(x = x, y = y, color = factor(true_cluster))) +
  geom_point(size = 3) +
  labs(title = "Datos Sint√©ticos con 3 Clusters Verdaderos",
       color = "Cluster Real") +
  theme_minimal()
```

## 2. Escalado de Variables

**¬øQu√© buscar en el escalado?**

- **Antes del escalado**: Variables tienen escalas muy diferentes (UrbanPop: 32-91 vs Assault: 45-337)

- **Sin escalado**: Assault dominar√≠a completamente las distancias

- **Despu√©s del escalado**: Todas las variables tienen media=0 y sd=1

- **Resultado**: Cada variable contribuye equitativamente a las distancias

- **Recomendaci√≥n**: SIEMPRE escalar cuando las unidades son diferentes

```{r scaling}
# Importancia del escalado - Comparaci√≥n con y sin escalar
par(mfrow = c(1, 2))

# Sin escalar
boxplot(USArrests, main = "USArrests - Sin Escalar", las = 2)

# Escalado (estandarizaci√≥n)
USArrests_scaled <- scale(USArrests)
boxplot(USArrests_scaled, main = "USArrests - Escalado", las = 2)
```

```{r scaling_verification, echo=FALSE}
# Verificar el escalado
cat("Medias despu√©s del escalado:\n")
print(round(colMeans(USArrests_scaled), 3))
cat("\nDesviaciones est√°ndar despu√©s del escalado:\n")
print(round(apply(USArrests_scaled, 2, sd), 3))
```

## 3. Diferentes Medidas de Distancia

**¬øQu√© buscar en las distancias?**

- **Colores en heatmap**: Rojo = mayor distancia (m√°s diferentes), Blanco = menor distancia (m√°s similares)

- **Euclidiana**: Distancia 'en l√≠nea recta', sensible a outliers

- **Manhattan**: Suma diferencias absolutas, m√°s robusta a outliers  

- **Correlaci√≥n**: Mide patrones similares, no magnitudes

- **Comparaci√≥n**: Si los valores son muy diferentes entre m√©todos, indica que la elecci√≥n de distancia S√ç importa

```{r distance_measures}
# Calcular diferentes matrices de distancia
dist_euclidean <- dist(USArrests_scaled, method = "euclidean")
dist_manhattan <- dist(USArrests_scaled, method = "manhattan")
dist_maximum <- dist(USArrests_scaled, method = "maximum")

# Para distancia de correlaci√≥n, necesitamos transponer
dist_correlation <- as.dist(1 - cor(t(USArrests_scaled)))

# Funci√≥n para visualizar matriz de distancias
plot_distance_matrix <- function(dist_matrix, title) {
  dist_df <- as.matrix(dist_matrix)
  melted_dist <- melt(dist_df)
  
  ggplot(melted_dist, aes(Var1, Var2, fill = value)) +
    geom_tile() +
    scale_fill_gradient(low = "white", high = "red") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = title, x = "", y = "", fill = "Distancia")
}

# Visualizar las diferentes matrices de distancia
p1 <- plot_distance_matrix(dist_euclidean, "Distancia Euclidiana")
p2 <- plot_distance_matrix(dist_manhattan, "Distancia Manhattan")

grid.arrange(p1, p2, ncol = 2)
```

```{r distance_comparison_stats, echo=FALSE}
# Comparar distancias para algunos estados espec√≠ficos
cat("Comparaci√≥n de distancias entre estados seleccionados:\n")
cat("Euclidiana vs Manhattan para California-Texas:\n")
cat("Euclidiana:", round(as.matrix(dist_euclidean)["California", "Texas"], 3), "\n")
cat("Manhattan:", round(as.matrix(dist_manhattan)["California", "Texas"], 3), "\n")
```

## 4. Diferentes M√©todos de Enlace (Linkage)

**¬øQu√© buscar en los dendrogramas?**

- **Altura**: Distancia a la cual se fusionan clusters (mayor altura = clusters m√°s diferentes al fusionarse)

- **Forma del √°rbol**:

  - **COMPLETE**: Clusters compactos, alturas bien diferenciadas

  - **SINGLE**: Tiende a crear 'cadenas', sensible a outliers

  - **WARD**: Clusters balanceados, minimiza varianza intra-cluster

  - **AVERAGE**: Compromiso entre Complete y Single

- **Rect√°ngulos**: Muestran los k=4 clusters seleccionados

- **Comparar**: ¬øQu√© m√©todo produce clusters m√°s interpretables? ¬øLos grupos tienen sentido conceptual?

```{r linkage_methods}
# Aplicar diferentes m√©todos de enlace
hc_complete <- hclust(dist_euclidean, method = "complete")
hc_single <- hclust(dist_euclidean, method = "single")
hc_average <- hclust(dist_euclidean, method = "average")
hc_ward <- hclust(dist_euclidean, method = "ward.D2")
hc_centroid <- hclust(dist_euclidean, method = "centroid")

# Funci√≥n para plotear dendrogramas
plot_dendrogram <- function(hc_object, title, k = 4) {
  plot(hc_object, main = title, cex = 0.6, hang = -1)
  rect.hclust(hc_object, k = k, border = 2:5)
}

# Crear gr√°ficos de dendrogramas
par(mfrow = c(2, 3))
plot_dendrogram(hc_complete, "Complete Linkage")
plot_dendrogram(hc_single, "Single Linkage")
plot_dendrogram(hc_average, "Average Linkage")
plot_dendrogram(hc_ward, "Ward Linkage")
plot_dendrogram(hc_centroid, "Centroid Linkage")

# Resetear par√°metros gr√°ficos
par(mfrow = c(1, 1))
```

## 5. M√©todos para Determinar el N√∫mero √ìptimo de Clusters

### 5.1 M√©todo del Codo (Elbow Method)

**¬øQu√© buscar en el m√©todo del codo?**

- **Objetivo**: Encontrar el 'codo' donde la mejora marginal se vuelve peque√±a

- **WCSS**: Within Cluster Sum of Squares - menor es mejor

- **Forma ideal**: Curva que baja r√°pido y luego se aplana

- **Buscar**: El punto donde la curva cambia de pendiente pronunciada a suave

- **Limitaci√≥n**: A veces el codo no es obvio - m√©todo subjetivo

```{r elbow_method}
# Funci√≥n para calcular WCSS (Within Cluster Sum of Squares)
calculate_wcss <- function(data, max_k = 10, method = "ward.D2") {
  wcss <- numeric(max_k)
  
  for(k in 1:max_k) {
    # Clustering jer√°rquico
    hc <- hclust(dist(data), method = method)
    clusters <- cutree(hc, k = k)
    
    # Calcular WCSS
    wcss[k] <- sum(sapply(1:k, function(i) {
      cluster_data <- data[clusters == i, , drop = FALSE]
      if(nrow(cluster_data) > 1) {
        sum(dist(cluster_data)^2) / (2 * nrow(cluster_data))
      } else {
        0
      }
    }))
  }
  
  return(wcss)
}

# Calcular WCSS para diferentes n√∫meros de clusters
wcss_values <- calculate_wcss(USArrests_scaled, max_k = 10)

# Plotear m√©todo del codo
elbow_data <- data.frame(k = 1:10, wcss = wcss_values)
ggplot(elbow_data, aes(x = k, y = wcss)) +
  geom_line(size = 1, color = "blue") +
  geom_point(size = 3, color = "red") +
  scale_x_continuous(breaks = 1:10) +
  labs(title = "M√©todo del Codo para Determinar k √ìptimo",
       x = "N√∫mero de Clusters (k)",
       y = "Within Cluster Sum of Squares") +
  theme_minimal()

# Encontrar el codo aproximado
diff1 <- diff(wcss_values)
diff2 <- diff(diff1)
suggested_k_elbow <- which.max(diff2) + 1
```

```{r elbow_suggestion, echo=FALSE}
cat("Sugerencia del m√©todo del codo: k =", suggested_k_elbow, "\n")
```


### 5.2 Gap Statistic

**¬øQu√© buscar en el Gap Statistic?**

- **Concepto**: Compara el clustering con datos aleatorios uniformes

- **Gap(k)**: Diferencia entre log(W_k) esperado vs observado

- **Mayor gap**: Indica que k produce clusters m√°s estructurados que el azar

- **Criterios**: FIRSTMAX (primer k donde Gap(k) ‚â• Gap(k+1) - SE(k+1)) vs GLOBALMAX (k con gap m√°s alto)

- **Barras de error**: Indican incertidumbre - importantes para la decisi√≥n


```{r gap_statistic}
# Calcular Gap Statistic
gap_stat <- clusGap(USArrests_scaled, 
                    FUN = function(x, k) {
                      hc <- hclust(dist(x), method = "ward.D2")
                      list(cluster = cutree(hc, k = k))
                    },
                    K.max = 10, 
                    B = 50)  # 50 simulaciones

# Plotear Gap Statistic
fviz_gap_stat(gap_stat) +
  labs(title = "Gap Statistic para Determinar k √ìptimo")

# Imprimir resultados
print(gap_stat, method = "firstmax")
```

### 5.3 Silhouette Analysis

**¬øQu√© buscar en el Silhouette Analysis?**

- **Silhouette Width**: Mide qu√© tan bien asignado est√° cada punto (rango: -1 a +1)

  - Cerca de +1: Punto bien asignado a su cluster

  - Cerca de 0: Punto est√° en la frontera entre clusters  

  - Cerca de -1: Punto probablemente mal asignado

- **Interpretaci√≥n de promedios**:

  - 0.71-1.00: Estructura fuerte

  - 0.51-0.70: Estructura razonable

  - 0.26-0.50: Estructura d√©bil

  - < 0.25: Sin estructura sustancial

- **Buscar**: El k que maximice el Average Silhouette Width

```{r silhouette_analysis}
# Funci√≥n para calcular average silhouette width
calc_silhouette <- function(data, max_k = 10, method = "ward.D2") {
  sil_width <- numeric(max_k - 1)
  
  for(k in 2:max_k) {
    hc <- hclust(dist(data), method = method)
    clusters <- cutree(hc, k = k)
    sil <- silhouette(clusters, dist(data))
    sil_width[k-1] <- mean(sil[, 3])
  }
  
  return(sil_width)
}

# Calcular silhouette widths
sil_widths <- calc_silhouette(USArrests_scaled, max_k = 10)

# Plotear
sil_data <- data.frame(k = 2:10, avg_sil = sil_widths)
ggplot(sil_data, aes(x = k, y = avg_sil)) +
  geom_line(size = 1, color = "blue") +
  geom_point(size = 3, color = "red") +
  scale_x_continuous(breaks = 2:10) +
  labs(title = "Average Silhouette Width por N√∫mero de Clusters",
       x = "N√∫mero de Clusters (k)",
       y = "Average Silhouette Width") +
  theme_minimal()

# Encontrar k √≥ptimo
optimal_k_sil <- which.max(sil_widths) + 1
```

```{r silhouette_results, echo=FALSE}
cat("N√∫mero √≥ptimo de clusters seg√∫n Silhouette:", optimal_k_sil, "\n")
cat("Valor de Silhouette Width:", round(max(sil_widths), 3), "\n")
```

### 5.4 NbClust - M√∫ltiples Criterios

```{r nbclust_analysis}
# Usar NbClust para m√∫ltiples criterios
nb_clust_result <- NbClust(USArrests_scaled, 
                          distance = "euclidean",
                          min.nc = 2, max.nc = 10,
                          method = "ward.D2",
                          index = "all")

# Crear visualizaci√≥n manual de los resultados de NbClust
# Extraer el n√∫mero de clusters sugerido por cada criterio
best_nc_table <- table(nb_clust_result$Best.nc[1,])
best_nc_df <- data.frame(
  clusters = as.numeric(names(best_nc_table)),
  frequency = as.numeric(best_nc_table)
)

# Plotear resultados
ggplot(best_nc_df, aes(x = factor(clusters), y = frequency)) +
  geom_bar(stat = "identity", fill = "steelblue", alpha = 0.7) +
  geom_text(aes(label = frequency), vjust = -0.3, size = 4) +
  labs(title = "N√∫mero √ìptimo de Clusters seg√∫n M√∫ltiples Criterios",
       subtitle = paste("Criterios totales evaluados:", length(nb_clust_result$Best.nc[1,])),
       x = "N√∫mero de Clusters",
       y = "Frecuencia (n√∫mero de criterios)") +
  theme_minimal()

# Mostrar resumen
cat("Resumen de NbClust:\n")
cat("N√∫mero de clusters m√°s frecuentemente sugerido:", 
    names(sort(best_nc_table, decreasing = TRUE))[1], "\n")
cat("Criterios que lo sugieren:", max(best_nc_table), "de", 
    length(nb_clust_result$Best.nc[1,]), "\n\n")

# Mostrar algunos criterios espec√≠ficos
if("Best.nc" %in% names(nb_clust_result)) {
  cat("Algunos criterios individuales:\n")
  criterios_importantes <- c("KL", "CH", "Hartigan", "CCC", "Scott", "Marriot", "TrCovW", "TraceW")
  for(criterio in criterios_importantes) {
    if(criterio %in% colnames(nb_clust_result$Best.nc)) {
      cat(criterio, ":", nb_clust_result$Best.nc[1, criterio], "clusters\n")
    }
  }
}
```

## 6. Comparaci√≥n de Resultados con Diferentes M√©todos

**¬øQu√© buscar en la comparaci√≥n de m√©todos?**

- **Adjusted Rand Index (ARI)**: Mide similitud entre dos clusterings

- ARI = 1: Clusterings id√©nticos

- ARI = 0: Similitud igual al azar

- ARI < 0: Similitud peor que el azar

- **Interpretaci√≥n ARI**: 0.90-1.00 (casi id√©nticos), 0.80-0.90 (muy similares), 0.65-0.80 (moderadamente similares), < 0.65 (diferentes)

- **Matriz de confusi√≥n**: Muestra c√≥mo se redistribuyen los puntos

- **Si ARI es bajo**: Los m√©todos producen resultados MUY diferentes - necesitas decidir cu√°l tiene m√°s sentido te√≥rico

```{r comparison_methods}
# Aplicar clustering con k=4 usando diferentes m√©todos de enlace
k_optimal <- 4

clusters_complete <- cutree(hc_complete, k = k_optimal)
clusters_single <- cutree(hc_single, k = k_optimal)
clusters_average <- cutree(hc_average, k = k_optimal)
clusters_ward <- cutree(hc_ward, k = k_optimal)

# Crear dataframe con resultados
comparison_df <- data.frame(
  State = rownames(USArrests),
  Complete = clusters_complete,
  Single = clusters_single,
  Average = clusters_average,
  Ward = clusters_ward
)

# Mostrar primeras filas
head(comparison_df, 10)

# Adjusted Rand Index para comparar similitud entre m√©todos
ari_complete_ward <- fossil::adj.rand.index(clusters_complete, clusters_ward)
ari_complete_single <- fossil::adj.rand.index(clusters_complete, clusters_single)
```

```{r method_comparison_stats, echo=FALSE}
cat("Matriz de confusi√≥n: Complete vs Ward\n")
print(table(clusters_complete, clusters_ward))

cat("\nMatriz de confusi√≥n: Complete vs Single\n")
print(table(clusters_complete, clusters_single))

cat("\nAdjusted Rand Index (Complete vs Ward):", round(ari_complete_ward, 3), "\n")
cat("Adjusted Rand Index (Complete vs Single):", round(ari_complete_single, 3), "\n")
```

## 7. An√°lisis de Calidad de Clustering

**¬øQu√© buscar en el an√°lisis de calidad?**

**Silhouette Plot Detallado:**

- Cada barra = un estado, ordenados por cluster y valor silhouette

- Barras rojas/negativas: Estados mal asignados

- Ancho promedio por cluster: ¬øHay clusters internamente cohesivos?

- Clusters con muchas barras rojas: Problemas de asignaci√≥n

**Dunn Index:** Cociente entre menor distancia inter-cluster y mayor intra-cluster

- Valores m√°s altos = mejor separaci√≥n

- > 0.1 se considera aceptable

**Estad√≠sticas por Cluster:**

- ¬øLos clusters tienen interpretaci√≥n sustantiva?

- ¬øLas medias por cluster son distintivas?

- ¬øLos tama√±os de cluster son razonables?

- **Coherencia**: Los clusters deben tener sentido en el dominio del problema

```{r clustering_quality}
# Usar clustering Ward (generalmente da buenos resultados)
final_clusters <- cutree(hc_ward, k = k_optimal)

# Silhouette plot detallado
sil_ward <- silhouette(final_clusters, dist_euclidean)
fviz_silhouette(sil_ward) +
  labs(title = "Silhouette Plot - Ward Linkage (k=4)")

# Estad√≠sticas de clustering
cluster_stats <- cluster.stats(dist_euclidean, final_clusters)

# Funci√≥n auxiliar para verificar y redondear valores
safe_round <- function(value, digits = 3) {
  if(is.null(value) || !is.numeric(value) || is.na(value)) {
    return("No disponible")
  } else {
    return(round(value, digits))
  }
}

# Estad√≠sticas de clustering
cluster_stats <- cluster.stats(dist_euclidean, final_clusters)

# Funci√≥n auxiliar para verificar y redondear valores
safe_round <- function(value, digits = 3) {
  if(is.null(value) || !is.numeric(value) || is.na(value)) {
    return("No disponible")
  } else {
    return(round(value, digits))
  }
}
```

```{r cluster_quality_stats, echo=FALSE}
cat("Estad√≠sticas de Clustering (Ward, k=4):\n")
cat("Average Silhouette Width:", safe_round(cluster_stats$avg.silwidth), "\n")
cat("Dunn Index:", safe_round(cluster_stats$dunn), "\n")
cat("Within Sum of Squares:", safe_round(cluster_stats$within.cluster.ss), "\n")
cat("Between Sum of Squares:", safe_round(cluster_stats$between.cluster.ss), "\n")
cat("N√∫mero de clusters:", cluster_stats$cluster.number, "\n")
cat("N√∫mero de observaciones:", cluster_stats$n, "\n")
```

```{r}
# An√°lisis por cluster
USArrests_clustered <- USArrests_scaled
USArrests_clustered <- data.frame(USArrests_clustered)
USArrests_clustered$Cluster <- final_clusters

# Estad√≠sticas por cluster
cluster_summary <- USArrests_clustered %>%
  group_by(Cluster) %>%
  summarise(
    n = n(),
    Murder_mean = round(mean(Murder), 2),
    Assault_mean = round(mean(Assault), 2),
    UrbanPop_mean = round(mean(UrbanPop), 2),
    Rape_mean = round(mean(Rape), 2),
    .groups = 'drop'
  )

print(cluster_summary)
```

## 8. Visualizaci√≥n de Resultados

**¬øQu√© buscar en las visualizaciones?**

**Gr√°fico PCA:**

- Proyecci√≥n de datos 4D en 2D para visualizaci√≥n

- % en ejes: proporci√≥n de varianza explicada por cada PC

- **Clusters bien separados**: Buenos resultados de clustering

- **Clusters superpuestos**: Puede indicar sobre-segmentaci√≥n

- **Outliers visibles**: Estados que no encajan bien en ning√∫n cluster

**Heatmap de Perfiles:**

- Muestra caracter√≠sticas promedio de cada cluster

- **Colores**: Azul = bajo, Blanco = promedio, Rojo = alto

- **Buscar**: Patrones distintivos por cluster

- **Interpretabilidad**: ¬øLos perfiles tienen sentido conceptual?

**Validaci√≥n Visual:**

- ¬øLos clusters se ven naturales en PCA?

- ¬øLos perfiles son distintivos y coherentes?

- ¬øEstados similares est√°n en el mismo cluster?

```{r visualization}
# PCA para visualizaci√≥n en 2D
pca_result <- prcomp(USArrests_scaled)
pca_data <- data.frame(pca_result$x[, 1:2])
pca_data$Cluster <- factor(final_clusters)
pca_data$State <- rownames(USArrests)

# Plot PCA con clusters
ggplot(pca_data, aes(x = PC1, y = PC2, color = Cluster)) +
  geom_point(size = 3) +
  geom_text(aes(label = State), vjust = -0.5, size = 2.5) +
  labs(title = "Clustering Jer√°rquico - Visualizaci√≥n PCA",
       subtitle = "Ward Linkage, k=4",
       x = paste("PC1 (", round(summary(pca_result)$importance[2,1] * 100, 1), "%)"),
       y = paste("PC2 (", round(summary(pca_result)$importance[2,2] * 100, 1), "%)")) +
  theme_minimal() +
  scale_color_brewer(type = "qual", palette = "Set1")

# Heatmap de caracter√≠sticas por cluster
USArrests_means <- USArrests_clustered %>%
  group_by(Cluster) %>%
  summarise_all(mean, na.rm = TRUE)

# Convertir a matriz con nombres de fila (m√©todo base R)
USArrests_means_matrix <- as.matrix(USArrests_means[, -1])  # Excluir columna Cluster
rownames(USArrests_means_matrix) <- paste("Cluster", USArrests_means$Cluster)

# Crear heatmap
pheatmap::pheatmap(USArrests_means_matrix,
                   scale = "column",
                   cluster_rows = FALSE,
                   cluster_cols = FALSE,
                   main = "Perfil de Clusters - Caracter√≠sticas Promedio",
                   color = colorRampPalette(c("blue", "white", "red"))(50))
```

## 9. Comparaci√≥n con Diferentes Distancias

**¬øQu√© buscar en la comparaci√≥n de distancias?**

- **ARI entre distancias**: Valores altos indican que la elecci√≥n de distancia no afecta mucho

- **ARI bajos**: Significan que la distancia elegida S√ç importa para estos datos

- **Consistencia**: Si los resultados son muy diferentes, evaluar cu√°l tiene m√°s sentido te√≥rico

```{r distance_comparison}
# Aplicar clustering con diferentes distancias
hc_euclidean <- hclust(dist_euclidean, method = "ward.D2")
hc_manhattan <- hclust(dist_manhattan, method = "ward.D2")
hc_correlation <- hclust(dist_correlation, method = "ward.D2")

# Obtener clusters
clusters_euclidean <- cutree(hc_euclidean, k = 4)
clusters_manhattan <- cutree(hc_manhattan, k = 4)
clusters_correlation <- cutree(hc_correlation, k = 4)

# Comparar resultados
distance_comparison <- data.frame(
  State = rownames(USArrests),
  Euclidean = clusters_euclidean,
  Manhattan = clusters_manhattan,
  Correlation = clusters_correlation
)

head(distance_comparison, 10)

# ARI entre diferentes distancias
ari_euc_man <- fossil::adj.rand.index(clusters_euclidean, clusters_manhattan)
ari_euc_cor <- fossil::adj.rand.index(clusters_euclidean, clusters_correlation)
ari_man_cor <- fossil::adj.rand.index(clusters_manhattan, clusters_correlation)

```

```{r distance_ari_comparison, echo=FALSE}
cat("Comparaci√≥n de m√©todos de distancia (ARI):\n")
cat("Euclidean vs Manhattan:", round(ari_euc_man, 3), "\n")
cat("Euclidean vs Correlation:", round(ari_euc_cor, 3), "\n")
cat("Manhattan vs Correlation:", round(ari_man_cor, 3), "\n")
```

```{r}
# Visualizar dendrogramas lado a lado
par(mfrow = c(1, 3))
plot(hc_euclidean, main = "Distancia Euclidiana", cex = 0.6, hang = -1)
rect.hclust(hc_euclidean, k = 4, border = 2:5)

plot(hc_manhattan, main = "Distancia Manhattan", cex = 0.6, hang = -1)
rect.hclust(hc_manhattan, k = 4, border = 2:5)

plot(hc_correlation, main = "Distancia Correlaci√≥n", cex = 0.6, hang = -1)
rect.hclust(hc_correlation, k = 4, border = 2:5)

par(mfrow = c(1, 1))
```

## 10. Aplicaci√≥n en Datos Sint√©ticos

**¬øQu√© buscar en datos sint√©ticos?**

- **Ventaja**: Conocemos la "verdad" - podemos evaluar qu√© tan bien funciona el algoritmo

- **ARI alto**: El algoritmo recuper√≥ correctamente la estructura conocida

- **Matriz de confusi√≥n**: Muestra exactamente qu√© puntos se asignaron incorrectamente

- **Gap statistic**: Deber√≠a sugerir el n√∫mero correcto de clusters (k=3)

```{r synthetic_example}
# Aplicar clustering jer√°rquico a datos sint√©ticos
synthetic_matrix <- as.matrix(synthetic_data[, 1:2])
dist_synthetic <- dist(synthetic_matrix)
hc_synthetic <- hclust(dist_synthetic, method = "ward.D2")

# Determinar n√∫mero √≥ptimo de clusters
gap_synthetic <- clusGap(synthetic_matrix, 
                        FUN = function(x, k) {
                          hc <- hclust(dist(x), method = "ward.D2")
                          list(cluster = cutree(hc, k = k))
                        },
                        K.max = 8, B = 20)

# Plot gap statistic
fviz_gap_stat(gap_synthetic) +
  labs(title = "Gap Statistic - Datos Sint√©ticos")

# Aplicar clustering con k=3
clusters_synthetic <- cutree(hc_synthetic, k = 3)

# Comparar con clusters verdaderos
synthetic_results <- data.frame(
  x = synthetic_data$x,
  y = synthetic_data$y,
  True_Cluster = synthetic_data$true_cluster,
  Predicted_Cluster = clusters_synthetic
)

# Visualizar resultados
p1 <- ggplot(synthetic_results, aes(x = x, y = y, color = factor(True_Cluster))) +
  geom_point(size = 3) +
  labs(title = "Clusters Verdaderos", color = "Cluster") +
  theme_minimal()

p2 <- ggplot(synthetic_results, aes(x = x, y = y, color = factor(Predicted_Cluster))) +
  geom_point(size = 3) +
  labs(title = "Clusters Predichos", color = "Cluster") +
  theme_minimal()

grid.arrange(p1, p2, ncol = 2)

# Calcular ARI
ari_synthetic <- fossil::adj.rand.index(synthetic_data$true_cluster, clusters_synthetic)

```

```{r synthetic_validation, echo=FALSE}
cat("Adjusted Rand Index (Sint√©ticos):", round(ari_synthetic, 3), "\n\n")
cat("Matriz de confusi√≥n:\n")
print(table(Verdadero = synthetic_data$true_cluster, 
            Predicho = clusters_synthetic))
```

## 11. Sensibilidad a Outliers

**¬øQu√© buscar en el an√°lisis de outliers?**

- **Single linkage**: Muy sensible a outliers - puede crear cadenas artificiales

- **Complete linkage**: M√°s robusto - mantiene clusters compactos

- **Comparaci√≥n visual**: ¬øC√≥mo cambia la estructura del dendrograma?

- **Identificaci√≥n**: Los outliers suelen aparecer como ramas separadas en el dendrograma

```{r outlier_sensitivity}
# Crear datos con outliers
set.seed(456)
data_with_outliers <- USArrests_scaled
# A√±adir outliers extremos
outlier_indices <- sample(nrow(data_with_outliers), 3)
data_with_outliers[outlier_indices, ] <- data_with_outliers[outlier_indices, ] * 3

# Comparar clustering con y sin outliers
hc_original <- hclust(dist(USArrests_scaled), method = "single")
hc_outliers <- hclust(dist(data_with_outliers), method = "single")

# Visualizar diferencias
par(mfrow = c(1, 3))
plot(hc_original, main = "Sin Outliers (Single Linkage)", cex = 0.6, hang = -1)
plot(hc_outliers, main = "Con Outliers (Single Linkage)", cex = 0.6, hang = -1)

# Comparar con m√©todo menos sensible a outliers
hc_complete_outliers <- hclust(dist(data_with_outliers), method = "complete")
plot(hc_complete_outliers, main = "Con Outliers (Complete Linkage)", cex = 0.6, hang = -1)

par(mfrow = c(1, 1))
```

```{r outlier_identification, echo=FALSE}
cat("Estados modificados como outliers:\n")
cat(paste(rownames(USArrests)[outlier_indices], collapse = ", "), "\n")
```

## 12. Resumen y Recomendaciones

```{r}
# Guardar resultados finales
final_results <- data.frame(
  State = rownames(USArrests),
  Cluster = final_clusters,
  USArrests[, ]
)
```

```{r executive_summary, echo=FALSE}
cat("=== RESUMEN EJECUTIVO ===\n\n")

cat("N√öMERO √ìPTIMO DE CLUSTERS:\n")
cat("‚Ä¢ Gap Statistic sugiere:", which.max(gap_stat$Tab[, "gap"]), "clusters\n")
cat("‚Ä¢ Silhouette sugiere:", optimal_k_sil, "clusters\n")
cat("‚Ä¢ Elbow method sugiere:", suggested_k_elbow, "clusters\n\n")

cat("CALIDAD DEL CLUSTERING FINAL (Ward, k=4):\n")
cat("‚Ä¢ Average Silhouette Width:", safe_round(cluster_stats$avg.silwidth), "\n")
cat("‚Ä¢ Dunn Index:", safe_round(cluster_stats$dunn), "\n\n")

cat("ESTADOS POR CLUSTER:\n")
for(i in 1:4) {
  cat("Cluster", i, ":\n")
  states_in_cluster <- final_results$State[final_results$Cluster == i]
  cat("  ", paste(states_in_cluster, collapse = ", "), "\n\n")
}
```

## Conclusiones y Gu√≠a de Interpretaci√≥n

### üìã Checklist de Interpretaci√≥n para Clustering Jer√°rquico

**Antes del an√°lisis:**

- [ ] ¬øEscal√© las variables apropiadamente?

- [ ] ¬øVerifiqu√© la presencia de outliers?

- [ ] ¬øLas variables est√°n en escalas comparables?

**Durante el an√°lisis:**

- [ ] ¬øCompar√© m√∫ltiples m√©todos de enlace?

- [ ] ¬øUs√© varios criterios para determinar k?

- [ ] ¬øEvalu√© diferentes medidas de distancia?

**Evaluaci√≥n de resultados:**

- [ ] ¬øVerifiqu√© la calidad con silhouette/Dunn?

- [ ] ¬øLos clusters tienen interpretaci√≥n sustantiva?

- [ ] ¬øLos resultados son estables ante peque√±os cambios?

### üéØ Valores de Referencia

| M√©trica | Excelente | Bueno | Aceptable | Problem√°tico |
|---------|-----------|-------|-----------|--------------|
| **Silhouette Width** | >0.7 | 0.5-0.7 | 0.25-0.5 | <0.25 |
| **Dunn Index** | >0.2 | 0.1-0.2 | 0.05-0.1 | <0.05 |
| **ARI** | >0.8 | 0.65-0.8 | 0.4-0.65 | <0.4 |

### ‚ö†Ô∏è Se√±ales de Alerta


- **Muchas barras rojas** en silhouette plot

- **Clusters muy desbalanceados** en tama√±o

- **ARI muy bajo** entre m√©todos razonables

- **Perfiles de cluster** no interpretables

- **Outliers dominando** la estructura

### ‚úÖ Recomendaciones por Situaci√≥n

**M√©todo de Enlace:**

- **Ward**: Para clusters compactos y balanceados

- **Complete**: Buena alternativa general, robusta

- **Average**: Compromiso entre Complete y Single

- **Evitar Single**: Con presencia de outliers

**Medida de Distancia:**

- **Euclidiana**: Standard para datos continuos escalados

- **Manhattan**: Menos sensible a outliers

- **Correlaci√≥n**: Cuando importan patrones vs magnitudes


**Preprocesamiento Cr√≠tico:**

- Escalado **esencial** con diferentes unidades

- Identificar y tratar outliers

- Considerar transformaciones si es necesario


### üîç Cu√°ndo Usar Clustering Jer√°rquico vs K-means

**Usar Clustering Jer√°rquico cuando:**


- La jerarqu√≠a natural es importante

- No conoces el n√∫mero de clusters

- Dataset peque√±o-mediano (<10,000 obs)

- Quieres explorar diferentes niveles de granularidad

**Usar K-means cuando:**

- Dataset grande (>10,000 obs)

- Conoces aproximadamente k

- Eficiencia computacional es cr√≠tica

- Clusters esf√©ricos son apropiados

### üìä Interpretaci√≥n de Este An√°lisis

Este notebook demuestra que el clustering jer√°rquico es especialmente valioso para:

- **An√°lisis exploratorio** donde la estructura jer√°rquica revela insights

- **Datasets complejos** donde el n√∫mero √≥ptimo de clusters no es obvio

- **Validaci√≥n cruzada** de resultados usando m√∫ltiples criterios

- **Interpretaci√≥n sustantiva** de grupos naturales en los datos

El clustering jer√°rquico proporciona una herramienta robusta para descubrir estructuras ocultas en datos, especialmente cuando se combina con una evaluaci√≥n cuidadosa de la calidad y interpretabilidad de los resultados.

## Conclusiones

Este notebook demuestra los aspectos clave del clustering jer√°rquico:

- **Importancia del escalado**: Variables en diferentes escalas pueden dominar la distancia

- **Sensibilidad al m√©todo de enlace**: Single linkage es sensible a outliers, Ward produce clusters compactos

- **Selecci√≥n de k**: M√∫ltiples criterios pueden sugerir diferentes n√∫meros √≥ptimos

- **Interpretabilidad**: Los dendrogramas proporcionan insight sobre la estructura jer√°rquica natural

- **Validaci√≥n**: Siempre evaluar la calidad usando m√©tricas como silhouette width y Dunn index

El clustering jer√°rquico es especialmente valioso para an√°lisis exploratorio donde la estructura jer√°rquica es importante y el n√∫mero de clusters no est√° predeterminado.