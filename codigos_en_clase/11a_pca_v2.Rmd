---
title: "Análisis de Componentes Principales (PCA) en R"
author: "Tu Nombre"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    df_print: paged
  html_notebook:
    toc: true
    toc_float: true
    theme: flatly
    highlight: tango
subtitle: Un Enfoque Práctico para Aprendizaje No Supervisado
---

# Introducción al Análisis Práctico de PCA

En este notebook vamos a explorar el Análisis de Componentes Principales (PCA) de manera práctica usando R. Nuestro objetivo es que puedas experimentar con datos reales y ver cómo cada concepto teórico se traduce en código y resultados interpretables.

Vamos a trabajar con varios datasets diferentes para que puedas observar cómo se comporta PCA en distintos contextos. Comenzaremos con un ejemplo simple y progresaremos hacia análisis más complejos que reflejan situaciones del mundo real.

## Preparación del Entorno

Primero, vamos a cargar las librerías que necesitaremos. Cada una cumple un propósito específico en nuestro análisis:

```{r setup, message=FALSE, warning=FALSE}
# Librerías fundamentales para análisis de datos
library(tidyverse)      # Para manipulación y visualización de datos
library(corrplot)       # Para visualizar matrices de correlación
library(factoextra)     # Para visualizaciones avanzadas de PCA
library(FactoMineR)     # Para análisis factorial y PCA
library(plotly)         # Para gráficos interactivos
library(knitr)          # Para tablas elegantes
library(gridExtra)      # Para combinar múltiples gráficos

# Configuración de opciones globales para los chunks
knitr::opts_chunk$set(
  fig.width = 10, 
  fig.height = 6, 
  warning = FALSE, 
  message = FALSE,
  comment = ""
)

# Establecer semilla para reproducibilidad
set.seed(2024)
```

# Ejemplo 1: Dataset Iris - Tu Primer PCA

Comenzaremos con el famoso dataset de iris de Edgar Anderson. Este dataset es perfecto para aprender PCA porque tiene solo 4 variables numéricas y una estructura conocida que nos permite verificar si nuestros resultados tienen sentido biológico.

## Exploración Inicial de los Datos

Antes de aplicar PCA, siempre es fundamental entender nuestros datos. Esta exploración inicial nos ayudará a interpretar mejor los resultados posteriores.

```{r decision_guide_exploration, echo=FALSE}
cat("=== GUÍA DE DECISIONES: Exploración Inicial ===\n\n")
cat("¿QUÉ DEBERÍAMOS ESPERAR Y BUSCAR?\n\n")
cat("1. ESTRUCTURA DE LOS DATOS:\n")
cat("   • Variables numéricas vs categóricas (PCA solo funciona con numéricas)\n")
cat("   • Escalas de medición diferentes (indicaría necesidad de estandarización)\n")
cat("   • Presencia de valores perdidos (requieren tratamiento previo)\n\n")
cat("2. DISTRIBUCIONES:\n")
cat("   • Variables con distribuciones muy sesgadas (considerar transformaciones)\n")
cat("   • Outliers extremos (pueden distorsionar los componentes principales)\n")
cat("   • Diferencias sustanciales en varianzas entre variables\n\n")
cat("3. RELACIONES ENTRE VARIABLES:\n")
cat("   • Variables que intuitivamente deberían estar relacionadas\n")
cat("   • Grupos naturales de variables (ej: medidas de tamaño, de color, etc.)\n\n")
cat("DECISIÓN CLAVE: Si las variables tienen escalas muy diferentes (ej: edad en años vs ingresos en miles),\n")
cat("SIEMPRE deberemos estandarizar antes de aplicar PCA.\n")
```

```{r iris_exploration}
# Cargar el dataset iris
data(iris)

# Examinar la estructura de los datos
str(iris)
head(iris)

# Estadísticas descriptivas básicas
summary(iris)

# Crear un resumen por especie para entender las diferencias naturales
iris %>% 
  group_by(Species) %>% 
  summarise(
    Media_Sepal_Length = mean(Sepal.Length),
    Media_Sepal_Width = mean(Sepal.Width),
    Media_Petal_Length = mean(Petal.Length),
    Media_Petal_Width = mean(Petal.Width),
    .groups = 'drop'
  ) %>% 
  kable(caption = "Medidas promedio por especie de iris")
```

```{r iris_exploration_interpretation, echo=FALSE}
cat("=== INTERPRETACIÓN DE LA EXPLORACIÓN INICIAL ===\n\n")
cat("¿QUÉ OBSERVAMOS EN ESTOS DATOS?\n\n")
cat("1. ESCALAS DE VARIABLES:\n")
cat("   • Todas las variables están en centímetros (escala similar)\n")
cat("   • Los rangos son comparables (no hay variables dominantes por escala)\n")
cat("   • Aún así, estandarizaremos para seguir las mejores prácticas\n\n")
cat("2. DIFERENCIAS ENTRE ESPECIES:\n")
cat("   • Setosa tiene pétalos notablemente más pequeños\n")
cat("   • Virginica tiende a tener las medidas más grandes\n")
cat("   • Versicolor está en el medio\n\n")
cat("3. EXPECTATIVA PARA PCA:\n")
cat("   • Esperamos que los primeros componentes capturen estas diferencias entre especies\n")
cat("   • Si PCA funciona bien, deberíamos poder separar las especies en pocas dimensiones\n")
```

## Visualización de Correlaciones

Una de las motivaciones principales para usar PCA es cuando tenemos variables correlacionadas. Veamos qué tan correlacionadas están nuestras variables:

```{r correlation_decision_guide, echo=FALSE}
cat("=== GUÍA DE DECISIONES: Análisis de Correlaciones ===\n\n")
cat("¿QUÉ BUSCAMOS EN LA MATRIZ DE CORRELACIÓN?\n\n")
cat("1. CORRELACIONES ALTAS (|r| > 0.7):\n")
cat("   • Indican información redundante entre variables\n")
cat("   • Justifican el uso de PCA para reducir dimensionalidad\n")
cat("   • Sugieren que existen factores latentes comunes\n\n")
cat("2. CORRELACIONES MODERADAS (0.3 < |r| < 0.7):\n")
cat("   • Aún pueden beneficiarse de PCA\n")
cat("   • Indican estructura parcial en los datos\n\n")
cat("3. CORRELACIONES BAJAS (|r| < 0.3):\n")
cat("   • PCA puede no ser muy efectivo\n")
cat("   • Las variables son mayormente independientes\n\n")
cat("CRITERIO DE DECISIÓN:\n")
cat("• Si la mayoría de correlaciones son < 0.3: considerar si PCA es necesario\n")
cat("• Si hay grupos de variables altamente correlacionadas: PCA será muy útil\n")
cat("• Si todas las correlaciones son muy altas (> 0.9): verificar multicolinealidad excesiva\n")
```

```{r iris_correlations}
# Extraer solo las variables numéricas
iris_numericas <- iris[, 1:4]

# Calcular matriz de correlación
cor_matrix <- cor(iris_numericas)
print(cor_matrix)

# Visualizar la matriz de correlación
corrplot(cor_matrix, 
         method = "color", 
         type = "upper", 
         order = "hclust",
         tl.cex = 0.8, 
         tl.col = "black",
         title = "Matriz de Correlación - Variables de Iris",
         mar = c(0,0,2,0))
```

```{r correlation_interpretation, echo=FALSE}
cat("=== INTERPRETACIÓN DE LAS CORRELACIONES ===\n\n")
cat("¿QUÉ NOS DICEN ESTAS CORRELACIONES?\n\n")
cat("CORRELACIONES ALTAS OBSERVADAS:\n")
cat("• Petal.Length vs Petal.Width: r ≈ 0.96 (muy alta)\n")
cat("• Petal.Length vs Sepal.Length: r ≈ 0.87 (alta)\n")
cat("• Petal.Width vs Sepal.Length: r ≈ 0.82 (alta)\n\n")
cat("IMPLICACIONES:\n")
cat("• Las variables de pétalos están fuertemente relacionadas\n")
cat("• Existe información redundante que PCA puede condensar\n")
cat("• Es probable que un solo componente capture la variación de pétalos\n\n")
cat("DECISIÓN: Proceder con PCA está JUSTIFICADO\n")
cat("• Las correlaciones altas indican estructura subyacente\n")
cat("• Esperamos reducción significativa de dimensionalidad\n")
cat("• Los primeros componentes deberían explicar mucha varianza\n")
```

**Interpretación de las Correlaciones:** Observamos que las variables relacionadas con pétalos están fuertemente correlacionadas entre sí (r > 0.9), y moderadamente correlacionadas con las variables de sépalos. Esta correlación sugiere que existe información redundante que PCA puede ayudarnos a reducir.

## Aplicación de PCA Paso a Paso

Ahora vamos a aplicar PCA siguiendo exactamente los pasos que discutimos en la teoría:

```{r pca_decision_guide, echo=FALSE}
cat("=== GUÍA DE DECISIONES: Aplicación de PCA ===\n\n")
cat("DECISIONES CRÍTICAS ANTES DE APLICAR PCA:\n\n")
cat("1. ¿ESTANDARIZAR O NO?\n")
cat("   ✓ ESTANDARIZAR cuando:\n")
cat("     • Variables tienen diferentes unidades (cm, kg, años)\n")
cat("     • Diferencias grandes en varianzas\n")
cat("     • Queremos que todas las variables contribuyan por igual\n\n")
cat("   ✗ NO ESTANDARIZAR cuando:\n")
cat("     • Todas las variables tienen la misma unidad y escala similar\n")
cat("     • Las diferencias en varianza son significativas y queremos preservarlas\n\n")
cat("2. ¿QUÉ ESPERAMOS DEL RESULTADO?\n")
cat("   • Con 4 variables y correlaciones altas: esperamos que 2-3 componentes\n")
cat("     expliquen >80% de la varianza\n")
cat("   • El primer componente debería capturar las correlaciones más fuertes\n")
cat("   • Los componentes siguientes capturan variación ortogonal (independiente)\n\n")
cat("PARÁMETROS TÉCNICOS:\n")
cat("• center=FALSE, scale.=FALSE porque ya estandarizamos manualmente\n")
cat("• Esto nos da control total sobre el preprocesamiento\n")
```

```{r iris_pca}
# Paso 1: Estandarizar los datos (fundamental cuando las variables tienen diferentes escalas)
iris_escalado <- scale(iris_numericas)

# Verificar que la estandarización funcionó correctamente
colMeans(iris_escalado)  # Deberían ser aproximadamente 0
apply(iris_escalado, 2, sd)  # Deberían ser 1

# Paso 2: Aplicar PCA
pca_iris <- prcomp(iris_escalado, center = FALSE, scale. = FALSE)
# Nota: usamos center=FALSE y scale.=FALSE porque ya estandarizamos manualmente

# Explorar los resultados del PCA
summary(pca_iris)
```

## Interpretación de los Autovalores

Los autovalores nos indican cuánta varianza explica cada componente principal. Esto es crucial para decidir cuántos componentes conservar:

```{r eigenvalue_decision_guide, echo=FALSE}
cat("=== GUÍA DE DECISIONES: Interpretación de Autovalores ===\n\n")
cat("¿CÓMO DECIDIR CUÁNTOS COMPONENTES CONSERVAR?\n\n")
cat("1. CRITERIO DE PORCENTAJE DE VARIANZA:\n")
cat("   • 80-85%: Mínimo recomendado para la mayoría de aplicaciones\n")
cat("   • 90-95%: Estándar para análisis exploratorio\n")
cat("   • >95%: Para análisis que requieren alta precisión\n\n")
cat("2. CRITERIO DEL CODO (SCREE PLOT):\n")
cat("   • Buscar el punto donde la pendiente cambia drásticamente\n")
cat("   • Componentes después del codo aportan poco valor adicional\n")
cat("   • Si no hay codo claro, usar criterio de varianza\n\n")
cat("3. CRITERIO DE KAISER (AUTOVALOR > 1):\n")
cat("   • Solo válido cuando los datos están estandarizados\n")
cat("   • Componentes con autovalor < 1 explican menos que una variable original\n")
cat("   • Útil como referencia, pero no es regla absoluta\n\n")
cat("¿QUÉ ESPERAMOS EN NUESTRO CASO?\n")
cat("• Con correlaciones altas observadas: los primeros 2 componentes\n")
cat("  deberían explicar >85% de la varianza\n")
cat("• El primer componente debería dominar (>50% de varianza)\n")
```

```{r iris_eigenvalues}
# Extraer los autovalores (varianzas de cada componente)
autovalores <- pca_iris$sdev^2
names(autovalores) <- paste0("PC", 1:length(autovalores))

# Calcular la proporción de varianza explicada
prop_varianza <- autovalores / sum(autovalores)
prop_varianza_acum <- cumsum(prop_varianza)

# Crear un data frame para visualización
varianza_df <- data.frame(
  Componente = names(autovalores),
  Autovalor = autovalores,
  Prop_Varianza = prop_varianza,
  Prop_Acumulada = prop_varianza_acum
)

print(varianza_df)

# Gráfico de sedimentación (Scree Plot)
p1 <- ggplot(varianza_df, aes(x = Componente, y = Prop_Varianza)) +
  geom_col(fill = "steelblue", alpha = 0.7) +
  geom_line(aes(group = 1), color = "red", size = 1) +
  geom_point(color = "red", size = 3) +
  labs(title = "Gráfico de Sedimentación (Scree Plot)",
       y = "Proporción de Varianza Explicada") +
  theme_minimal()

p2 <- ggplot(varianza_df, aes(x = Componente, y = Prop_Acumulada)) +
  geom_col(fill = "darkgreen", alpha = 0.7) +
  geom_hline(yintercept = 0.8, linetype = "dashed", color = "red") +
  geom_hline(yintercept = 0.95, linetype = "dashed", color = "orange") +
  labs(title = "Varianza Acumulada por Componente",
       y = "Proporción Acumulada") +
  theme_minimal()

grid.arrange(p1, p2, ncol = 2)
```

```{r eigenvalue_interpretation, echo=FALSE}
cat("=== INTERPRETACIÓN DE LOS RESULTADOS ===\n\n")
cat("¿QUÉ NOS DICEN ESTOS AUTOVALORES?\n\n")
cat("ANÁLISIS DE VARIANZA EXPLICADA:\n")
cat("• PC1: ~73% de la varianza (EXCELENTE - domina el análisis)\n")
cat("• PC2: ~23% de la varianza (MUY BUENO - información complementaria importante)\n")
cat("• PC1 + PC2: ~96% de la varianza (EXCEPCIONAL - casi toda la información)\n")
cat("• PC3 + PC4: Solo ~4% restante (información menor)\n\n")
cat("DECISIÓN TOMADA:\n")
cat("✓ CONSERVAR 2 COMPONENTES\n")
cat("  • Criterio de 95% de varianza: CUMPLIDO\n")
cat("  • Scree plot muestra codo claro después de PC2\n")
cat("  • Reducción de 4D a 2D con pérdida mínima de información\n\n")
cat("IMPLICACIONES:\n")
cat("• Los datos tienen estructura bidimensional subyacente\n")
cat("• Las 4 variables originales pueden representarse efectivamente en 2D\n")
cat("• Excelente candidato para visualización y análisis posteriores\n")
```

**Interpretación:** Los primeros dos componentes explican aproximadamente el 96% de la varianza total. Esto significa que podemos reducir nuestros datos de 4 dimensiones a 2 dimensiones perdiendo solo el 4% de la información. ¡Excelente resultado!

## Análisis de las Cargas (Loadings)

Ahora vamos a examinar las cargas para entender qué variables contribuyen más a cada componente:

```{r loadings_decision_guide, echo=FALSE}
cat("=== GUÍA DE DECISIONES: Interpretación de Cargas ===\n\n")
cat("¿CÓMO INTERPRETAR LAS CARGAS?\n\n")
cat("1. MAGNITUD DE LAS CARGAS:\n")
cat("   • |carga| > 0.6: Contribución ALTA (variable muy importante)\n")
cat("   • 0.4 < |carga| < 0.6: Contribución MODERADA\n")
cat("   • |carga| < 0.4: Contribución BAJA (variable menos relevante)\n\n")
cat("2. SIGNO DE LAS CARGAS:\n")
cat("   • Cargas del mismo signo: variables que 'se mueven juntas'\n")
cat("   • Cargas de signo opuesto: variables que se comportan inversamente\n")
cat("   • Magnitud similar + mismo signo = variables redundantes\n\n")
cat("3. PATRONES A BUSCAR:\n")
cat("   • Grupos de variables con cargas similares (factores comunes)\n")
cat("   • Variables dominantes en cada componente\n")
cat("   • Variables que cargan fuerte en múltiples componentes (complejas)\n\n")
cat("¿QUÉ ESPERAMOS EN IRIS?\n")
cat("• PC1: Cargas altas y positivas en todas las variables (tamaño general)\n")
cat("• PC2: Diferenciación entre tipos de medidas (sépalos vs pétalos)\n")
```

```{r iris_loadings}
# Extraer las cargas (los autovectores)
cargas <- pca_iris$rotation
print(cargas)

# Visualizar las cargas en un gráfico más interpretable
cargas_df <- as.data.frame(cargas) %>%
  rownames_to_column("Variable") %>%
  pivot_longer(cols = starts_with("PC"), names_to = "Componente", values_to = "Carga")

ggplot(cargas_df %>% filter(Componente %in% c("PC1", "PC2")), 
       aes(x = Variable, y = Carga, fill = Componente)) +
  geom_col(position = "dodge") +
  facet_wrap(~Componente) +
  labs(title = "Cargas de Variables en los Primeros Dos Componentes",
       y = "Valor de la Carga") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Biplot para visualizar tanto observaciones como cargas
fviz_pca_biplot(pca_iris, 
                geom.ind = "point",
                col.ind = iris$Species,
                palette = c("#E7B800", "#2E9FDF", "#FC4E07"),
                addEllipses = TRUE,
                label = "var",
                col.var = "black",
                repel = TRUE,
                title = "Biplot PCA - Dataset Iris")
```

```{r loadings_interpretation, echo=FALSE}
cat("=== INTERPRETACIÓN DE LAS CARGAS ===\n\n")
cat("ANÁLISIS DE PC1 (73% de varianza):\n")
cat("• Todas las variables tienen cargas positivas y altas\n")
cat("• Petal.Length y Petal.Width tienen las cargas más altas (~0.85)\n")
cat("• Sepal.Length también contribuye significativamente (~0.75)\n")
cat("• INTERPRETACIÓN: 'Tamaño general de la flor'\n\n")
cat("ANÁLISIS DE PC2 (23% de varianza):\n")
cat("• Sepal.Width tiene carga positiva alta (~0.6)\n")
cat("• Las variables de pétalos tienen cargas más bajas o negativas\n")
cat("• INTERPRETACIÓN: 'Forma de la flor' (sépalos anchos vs pétalos desarrollados)\n\n")
cat("CONCLUSIONES SOBRE LA ESTRUCTURA:\n")
cat("• PC1 separa flores grandes de flores pequeñas\n")
cat("• PC2 distingue entre diferentes proporciones de la flor\n")
cat("• Esta estructura bidimensional debería separar bien las especies\n\n")
cat("VALIDACIÓN BIOLÓGICA:\n")
cat("• Setosa: flores pequeñas (PC1 bajo) con sépalos anchos (PC2 alto)\n")
cat("• Virginica: flores grandes (PC1 alto)\n")
cat("• Versicolor: intermedia en ambas dimensiones\n")
```

**Interpretación de las Cargas:**

- **PC1 (73% de varianza):** Todas las variables tienen cargas positivas similares, especialmente las relacionadas con pétalos. Este componente parece representar el "tamaño general" de la flor.

- **PC2 (23% de varianza):** Las variables de sépalos tienen cargas positivas mientras que las de pétalos tienen cargas negativas. Este componente podría representar la diferencia entre el desarrollo de sépalos versus pétalos.

## Visualización en el Espacio de Componentes Principales

```{r visualization_decision_guide, echo=FALSE}
cat("=== GUÍA DE DECISIONES: Validación de Resultados ===\n\n")
cat("¿CÓMO VALIDAR QUE NUESTRO PCA ES EXITOSO?\n\n")
cat("1. SEPARACIÓN DE GRUPOS CONOCIDOS:\n")
cat("   • Si conocemos grupos reales (especies, clases), deberían separarse\n")
cat("   • Grupos bien separados = PCA capturó la estructura real\n")
cat("   • Grupos superpuestos = puede necesitar más componentes o PCA no es apropiado\n\n")
cat("2. INTERPRETABILIDAD BIOLÓGICA/DOMAIN:\n")
cat("   • Los componentes principales deben tener sentido en el contexto\n")
cat("   • PC1 como 'tamaño general' es interpretable biológicamente\n")
cat("   • Si los componentes no se pueden interpretar, revisar el análisis\n\n")
cat("3. DISTRIBUCIÓN EN EL ESPACIO PCA:\n")
cat("   • Puntos no deberían formar patrones artificiales (líneas, curvas perfectas)\n")
cat("   • Distribución natural indica que PCA preservó la estructura real\n")
cat("   • Outliers extremos pueden indicar problemas en los datos\n\n")
cat("¿QUÉ ESPERAMOS VER?\n")
cat("• Tres grupos claramente separados (una especie por grupo)\n")
cat("• Setosa separada del resto (por sus características distintivas)\n")
cat("• Versicolor y Virginica con cierta superposición (más similares entre sí)\n")
```

```{r iris_pca_visualization}
# Extraer las puntuaciones (scores) de cada observación en el espacio PCA
scores <- as.data.frame(pca_iris$x)
scores$Species <- iris$Species

# Gráfico de dispersión en el espacio de componentes principales
p_especies <- ggplot(scores, aes(x = PC1, y = PC2, color = Species)) +
  geom_point(size = 3, alpha = 0.7) +
  stat_ellipse(level = 0.68) +  # Elipses de confianza
  labs(title = "Observaciones en el Espacio de Componentes Principales",
       x = paste0("PC1 (", round(prop_varianza[1]*100, 1), "% de varianza)"),
       y = paste0("PC2 (", round(prop_varianza[2]*100, 1), "% de varianza)")) +
  theme_minimal() +
  scale_color_brewer(type = "qual", palette = "Set1")

print(p_especies)
```

```{r iris_results_validation, echo=FALSE}
cat("=== VALIDACIÓN DEL ÉXITO DE PCA ===\n\n")
cat("¿NUESTRO PCA FUE EXITOSO?\n\n")
cat("✓ SEPARACIÓN DE ESPECIES: EXCELENTE\n")
cat("  • Setosa completamente separada (cluster inferior)\n")
cat("  • Versicolor y Virginica parcialmente separadas\n")
cat("  • Separación clara a lo largo de PC1\n\n")
cat("✓ INTERPRETABILIDAD: EXCELENTE\n")
cat("  • PC1 = tamaño general (hace sentido biológico)\n")
cat("  • PC2 = forma/proporciones (también interpretable)\n")
cat("  • Setosa: flores pequeñas con sépalos relativamente anchos\n")
cat("  • Virginica: flores grandes\n\n")
cat("✓ EFICIENCIA: EXCELENTE\n")
cat("  • 96% de varianza en solo 2 dimensiones\n")
cat("  • Reducción de 4D a 2D con pérdida mínima\n\n")
cat("CONCLUSIÓN: PCA capturó exitosamente la estructura subyacente\n")
cat("de los datos. Los resultados son biológicamente interpretables\n")
cat("y estadísticamente robustos.\n")
```

**¡Resultado Impresionante!** Observamos que las tres especies de iris se separan claramente en el espacio de componentes principales, especialmente a lo largo de PC1. Esto confirma que PCA ha capturado exitosamente las diferencias biológicas principales entre las especies.

# Ejemplo 2: Dataset de Vinos - Un Caso Más Complejo

Ahora vamos a trabajar con un dataset más desafiante que tiene 13 variables químicas de vinos. Este ejemplo nos permitirá explorar técnicas más avanzadas de interpretación y visualización.

```{r wine_complexity_guide, echo=FALSE}
cat("=== GUÍA DE DECISIONES: Datos de Alta Dimensionalidad ===\n\n")
cat("¿QUÉ CAMBIA CON MÁS VARIABLES?\n\n")
cat("1. COMPLEJIDAD DE INTERPRETACIÓN:\n")
cat("   • Con 13 variables, es imposible visualizar correlaciones fácilmente\n")
cat("   • Las cargas se vuelven más difíciles de interpretar\n")
cat("   • Necesitamos estrategias sistemáticas para entender resultados\n\n")
cat("2. DECISIONES SOBRE DIMENSIONALIDAD:\n")
cat("   • Rara vez necesitamos conservar >5-6 componentes para aplicaciones prácticas\n")
cat("   • El criterio de 80% de varianza se vuelve más importante\n")
cat("   • Más componentes significa más análisis de interpretación\n\n")
cat("3. ESCALADO MÁS CRÍTICO:\n")
cat("   • Variables químicas pueden tener escalas muy diferentes\n")
cat("   • Alcohol (%) vs Prolina (mg/L) vs pH (escala logarítmica)\n")
cat("   • El escalado es OBLIGATORIO, no opcional\n\n")
cat("¿QUÉ ESPERAMOS?\n")
cat("• Primer componente explique 25-40% (menor % individual que en iris)\n")
cat("• Necesitar 4-6 componentes para 80% de varianza\n")
cat("• Patrones más complejos en las cargas\n")
```

```{r wine_data_preparation}
# Cargar el dataset de vinos (incluido en R)
# Si no tienes este dataset, puedes descargarlo de UCI Machine Learning Repository
# Para este ejemplo, vamos a simular un dataset similar

# Generar datos simulados de vinos con propiedades similares al dataset real
n_vinos <- 180
n_variables <- 13

# Crear variables correlacionadas que representen propiedades químicas
wine_data <- data.frame(
  Alcohol = rnorm(n_vinos, 13, 1.5),
  Acido_Malico = rnorm(n_vinos, 2.3, 1.1),
  Ceniza = rnorm(n_vinos, 2.4, 0.4),
  Alcalinidad_Ceniza = rnorm(n_vinos, 19, 3),
  Magnesio = rnorm(n_vinos, 100, 14),
  Fenoles_Totales = rnorm(n_vinos, 2.3, 0.6),
  Flavonoides = rnorm(n_vinos, 2, 1),
  Fenoles_No_Flavonoides = rnorm(n_vinos, 0.36, 0.12),
  Proantocianinas = rnorm(n_vinos, 1.6, 0.6),
  Intensidad_Color = rnorm(n_vinos, 5.1, 2.3),
  Tono = rnorm(n_vinos, 0.96, 0.23),
  OD280_OD315 = rnorm(n_vinos, 2.6, 0.7),
  Prolina = rnorm(n_vinos, 746, 315)
)

# Agregar correlaciones realistas entre variables
wine_data$Flavonoides <- wine_data$Flavonoides + 0.6 * wine_data$Fenoles_Totales + rnorm(n_vinos, 0, 0.3)
wine_data$Intensidad_Color <- wine_data$Intensidad_Color + 0.4 * wine_data$Alcohol + rnorm(n_vinos, 0, 0.5)
wine_data$Proantocianinas <- wine_data$Proantocianinas + 0.5 * wine_data$Fenoles_Totales + rnorm(n_vinos, 0, 0.2)

# Crear una variable categórica de tipo de vino basada en características
wine_data$Tipo_Vino <- ifelse(wine_data$Alcohol > 13.5 & wine_data$Fenoles_Totales > 2.5, "Premium",
                        ifelse(wine_data$Alcohol > 12 & wine_data$Fenoles_Totales > 2, "Estándar", "Básico"))

# Mostrar resumen de los datos
summary(wine_data[, 1:6])  # Primeras 6 variables para no sobrecargar
```

## PCA en Datos de Alta Dimensionalidad

```{r wine_pca_analysis}
# Separar variables numéricas
wine_numericas <- wine_data[, 1:13]

# Estandarizar los datos (crucial cuando las variables tienen diferentes unidades)
wine_escalado <- scale(wine_numericas)

# Aplicar PCA
pca_wine <- prcomp(wine_escalado)

# Resumen del análisis
summary(pca_wine)

# Análisis detallado de varianza explicada
autovalores_wine <- pca_wine$sdev^2
prop_var_wine <- autovalores_wine / sum(autovalores_wine)
prop_var_acum_wine <- cumsum(prop_var_wine)

# Determinar cuántos componentes necesitamos para explicar 80% y 95% de varianza
componentes_80 <- which(prop_var_acum_wine >= 0.80)[1]
componentes_95 <- which(prop_var_acum_wine >= 0.95)[1]

cat("Componentes necesarios para 80% de varianza:", componentes_80, "\n")
cat("Componentes necesarios para 95% de varianza:", componentes_95, "\n")

# Visualización de varianza explicada
var_wine_df <- data.frame(
  PC = 1:length(autovalores_wine),
  Varianza = prop_var_wine,
  Acumulada = prop_var_acum_wine
)

ggplot(var_wine_df, aes(x = PC)) +
  geom_col(aes(y = Varianza), fill = "lightblue", alpha = 0.7) +
  geom_line(aes(y = Acumulada), color = "red", size = 1.2) +
  geom_point(aes(y = Acumulada), color = "red", size = 2) +
  geom_hline(yintercept = 0.8, linetype = "dashed", color = "blue") +
  geom_hline(yintercept = 0.95, linetype = "dashed", color = "green") +
  labs(title = "Análisis de Varianza - Dataset de Vinos",
       x = "Componente Principal",
       y = "Proporción de Varianza") +
  scale_x_continuous(breaks = 1:13) +
  theme_minimal()
```

```{r wine_variance_interpretation, echo=FALSE}
cat("=== INTERPRETACIÓN: VARIANZA EN DATOS COMPLEJOS ===\n\n")
cat("¿QUÉ OBSERVAMOS CON 13 VARIABLES?\n\n")
cat("DISTRIBUCIÓN DE VARIANZA:\n")
cat("• PC1: ~", round(prop_var_wine[1]*100, 1), "% (menor dominancia que en iris)\n")
cat("• PC2: ~", round(prop_var_wine[2]*100, 1), "%\n") 
cat("• PC3: ~", round(prop_var_wine[3]*100, 1), "%\n")
cat("• Varianza más distribuida entre componentes (esperado con más variables)\n\n")
cat("DECISIONES BASADAS EN CRITERIOS:\n")
if(componentes_80 <= 5) {
  cat("✓ EXCELENTE: Solo", componentes_80, "componentes para 80% de varianza\n")
} else if(componentes_80 <= 7) {
  cat("✓ BUENO:", componentes_80, "componentes para 80% de varianza\n")
} else {
  cat("⚠ REGULAR:", componentes_80, "componentes para 80% de varianza\n")
}
cat("• Para análisis exploratorio: usar", min(componentes_80, 5), "componentes\n")
cat("• Para aplicaciones que requieren precisión: usar", componentes_95, "componentes\n\n")
cat("PATRÓN TÍPICO EN DATOS QUÍMICOS:\n")
cat("• Los primeros 3-4 componentes capturan factores principales\n")
cat("• Componentes posteriores pueden representar factores específicos o ruido\n")
cat("• Balance entre simplicidad e información preservada\n")
```

## Interpretación Profunda de las Cargas

```{r wine_loadings_guide, echo=FALSE}
cat("=== GUÍA DE DECISIONES: Cargas en Datos Complejos ===\n\n")
cat("ESTRATEGIA PARA INTERPRETAR MUCHAS VARIABLES:\n\n")
cat("1. IDENTIFICAR VARIABLES DOMINANTES:\n")
cat("   • Buscar las 3-4 variables con cargas más altas en cada PC\n")
cat("   • Ignorar variables con cargas < 0.3 para interpretación inicial\n")
cat("   • Enfocarse en patrones, no en valores individuales\n\n")
cat("2. BUSCAR AGRUPACIONES CONCEPTUALES:\n")
cat("   • Variables relacionadas químicamente deberían agruparse\n")
cat("   • Ej: fenoles, flavonoides, proantocianinas (compuestos relacionados)\n")
cat("   • Ej: alcohol, intensidad de color (características sensoriales)\n\n")
cat("3. INTERPRETAR EN CONTEXTO DEL DOMINIO:\n")
cat("   • PC1: a menudo representa 'calidad general' o 'intensidad'\n")
cat("   • PC2: puede representar 'estilo' o características complementarias\n")
cat("   • Validar interpretaciones con conocimiento enológico\n\n")
cat("HERRAMIENTAS DE VISUALIZACIÓN:\n")
cat("• Heatmap: para ver patrones en todas las cargas\n")
cat("• Ranking: para identificar variables más importantes\n")
cat("• Agrupación: para identificar clusters de variables relacionadas\n")
```

```{r wine_loadings_analysis}
# Examinar las cargas de los primeros componentes
cargas_wine <- pca_wine$rotation[, 1:4]  # Primeros 4 componentes
print(round(cargas_wine, 3))

# Identificar variables más importantes en cada componente
for(i in 1:4) {
  cat("\n--- PC", i, "---\n")
  cargas_pc <- abs(cargas_wine[, i])
  variables_importantes <- names(sort(cargas_pc, decreasing = TRUE)[1:3])
  cat("Variables más influyentes:", paste(variables_importantes, collapse = ", "), "\n")
}

# Heatmap de cargas para visualización
library(reshape2)
cargas_melt <- melt(cargas_wine)
colnames(cargas_melt) <- c("Variable", "Componente", "Carga")

ggplot(cargas_melt, aes(x = Componente, y = Variable, fill = Carga)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0) +
  labs(title = "Heatmap de Cargas - Primeros 4 Componentes",
       x = "Componente Principal", y = "Variable") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Visualización Avanzada del Espacio PCA

```{r wine_advanced_visualization}
# Proyectar datos al espacio PCA
scores_wine <- as.data.frame(pca_wine$x[, 1:4])
scores_wine$Tipo_Vino <- wine_data$Tipo_Vino

# Gráfico 3D interactivo usando los primeros 3 componentes
library(plotly)

p_3d <- plot_ly(scores_wine, 
                x = ~PC1, y = ~PC2, z = ~PC3,
                color = ~Tipo_Vino,
                type = "scatter3d",
                mode = "markers",
                marker = list(size = 5, opacity = 0.8)) %>%
  layout(title = "Visualización 3D - Espacio PCA de Vinos",
         scene = list(
           xaxis = list(title = paste0("PC1 (", round(prop_var_wine[1]*100, 1), "%)")),
           yaxis = list(title = paste0("PC2 (", round(prop_var_wine[2]*100, 1), "%)")),
           zaxis = list(title = paste0("PC3 (", round(prop_var_wine[3]*100, 1), "%)"))
         ))

p_3d

# Matriz de gráficos para explorar múltiples combinaciones
pairs_data <- scores_wine[, 1:4]
pairs_data$Tipo <- scores_wine$Tipo_Vino

# Función personalizada para el panel superior
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...) {
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- cor(x, y, use = "complete.obs")
  txt <- format(c(r, 0.123456789), digits = digits)[1]
  txt <- paste0(prefix, txt)
  text(0.5, 0.5, txt, cex = 1.5, font = 2)
}

# Crear matriz de dispersión
pairs(pairs_data[, 1:4], 
      col = as.numeric(as.factor(pairs_data$Tipo)),
      upper.panel = panel.cor,
      main = "Matriz de Correlación - Primeros 4 Componentes Principales")
```

# Análisis de Calidad del PCA

Una parte fundamental de cualquier análisis de PCA es evaluar qué tan bien nuestros componentes principales representan las variables originales y las observaciones individuales.

```{r pca_quality_assessment}
# Calidad de representación de las variables (cos2)
var_cos2 <- get_pca_var(pca_wine)$cos2

# Contribución de las variables a cada componente
var_contrib <- get_pca_var(pca_wine)$contrib

# Visualizar calidad de representación
fviz_cos2(pca_wine, choice = "var", axes = 1:2, 
          title = "Calidad de Representación de Variables (cos²)")

# Visualizar contribución de variables
fviz_contrib(pca_wine, choice = "var", axes = 1, top = 10,
             title = "Contribución de Variables al PC1")

fviz_contrib(pca_wine, choice = "var", axes = 2, top = 10,
             title = "Contribución de Variables al PC2")

# Crear un resumen de interpretación
interpretacion_componentes <- data.frame(
  Componente = paste0("PC", 1:4),
  Varianza_Explicada = paste0(round(prop_var_wine[1:4] * 100, 1), "%"),
  Variables_Principales = c(
    "Fenoles, Flavonoides, Proantocianinas",
    "Alcohol, Intensidad_Color",
    "Acido_Malico, Ceniza",
    "Magnesio, Tono"
  ),
  Interpretacion_Sugerida = c(
    "Compuestos fenólicos (calidad/cuerpo)",
    "Fuerza/intensidad del vino",
    "Acidez/mineralidad",
    "Características secundarias"
  )
)

kable(interpretacion_componentes, 
      caption = "Interpretación de los Primeros 4 Componentes Principales")
```

# Aplicación Práctica: Reducción de Dimensionalidad para Clustering

Finalmente, vamos a ver cómo PCA puede mejorar otros análisis al reducir la dimensionalidad de nuestros datos antes de aplicar técnicas como clustering.

```{r pca_clustering_application}
# Aplicar clustering en el espacio original (13 dimensiones)
set.seed(2024)
clusters_original <- kmeans(wine_escalado, centers = 3, nstart = 25)

# Aplicar clustering en el espacio PCA reducido (primeros 4 componentes)
wine_pca_reducido <- scores_wine[, 1:4]
clusters_pca <- kmeans(wine_pca_reducido, centers = 3, nstart = 25)

# Comparar resultados
table("Original" = clusters_original$cluster, "PCA" = clusters_pca$cluster)

# Visualizar clustering en espacio PCA
scores_wine$Cluster_Original <- as.factor(clusters_original$cluster)
scores_wine$Cluster_PCA <- as.factor(clusters_pca$cluster)

p1 <- ggplot(scores_wine, aes(x = PC1, y = PC2, color = Cluster_Original)) +
  geom_point(size = 3, alpha = 0.7) +
  labs(title = "Clustering en Espacio Original (proyectado a PC1-PC2)",
       color = "Cluster") +
  theme_minimal()

p2 <- ggplot(scores_wine, aes(x = PC1, y = PC2, color = Cluster_PCA)) +
  geom_point(size = 3, alpha = 0.7) +
  labs(title = "Clustering en Espacio PCA",
       color = "Cluster") +
  theme_minimal()

grid.arrange(p1, p2, ncol = 2)

# Calcular métricas de calidad del clustering
wss_original <- clusters_original$tot.withinss
wss_pca <- clusters_pca$tot.withinss

cat("Suma de cuadrados intra-cluster (espacio original):", round(wss_original, 2), "\n")
cat("Suma de cuadrados intra-cluster (espacio PCA):", round(wss_pca, 2), "\n")
cat("Mejora en compacidad:", round((wss_original - wss_pca)/wss_original * 100, 1), "%\n")
```

# Conclusiones y Reflexiones Finales

A través de estos ejemplos prácticos, hemos visto cómo PCA nos permite:

1. **Reducir la complejidad** sin perder información esencial (iris: de 4 a 2 dimensiones manteniendo 96% de varianza)

2. **Descubrir patrones ocultos** en los datos que no eran evidentes en el espacio original

3. **Mejorar visualizaciones** al proyectar datos multidimensionales a espacios de 2 o 3 dimensiones

4. **Facilitar análisis posteriores** como clustering al trabajar en espacios de menor dimensión

5. **Interpretar relaciones** entre variables a través del análisis de cargas

El dominio de PCA te abre las puertas a una comprensión más profunda de tus datos y te proporciona una herramienta fundamental para el análisis exploratorio de datos en aprendizaje no supervisado.

**Próximos pasos sugeridos:**
- Experimentá con tus propios datasets
- Probá diferentes criterios para seleccionar el número de componentes
- Explorá variantes como PCA robusto para datos con outliers
- Combiná PCA con otras técnicas de aprendizaje automático
