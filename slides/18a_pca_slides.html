<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Principal Component Analysis - Aprendizaje No Supervisado</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Georgia', serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            color: #333;
        }

        .slideshow-container {
            max-width: 1000px;
            margin: auto;
            position: relative;
            background: white;
            border-radius: 15px;
            box-shadow: 0 20px 40px rgba(0,0,0,0.1);
            overflow: hidden;
            margin-top: 20px;
            margin-bottom: 20px;
        }

        .slide {
            display: none;
            padding: 60px;
            min-height: 600px;
            animation: fadeIn 0.5s;
        }

        .slide.active {
            display: block;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(20px); }
            to { opacity: 1; transform: translateY(0); }
        }

        h1 {
            color: #4a5568;
            font-size: 2.5em;
            margin-bottom: 30px;
            text-align: center;
            border-bottom: 3px solid #667eea;
            padding-bottom: 20px;
        }

        h2 {
            color: #2d3748;
            font-size: 2em;
            margin-bottom: 25px;
            text-align: center;
        }

        h3 {
            color: #4a5568;
            font-size: 1.4em;
            margin-bottom: 20px;
            margin-top: 30px;
        }

        p {
            line-height: 1.8;
            margin-bottom: 20px;
            font-size: 1.1em;
            text-align: justify;
        }

        .equation {
            background: #f7fafc;
            padding: 20px;
            border-radius: 10px;
            border-left: 4px solid #667eea;
            margin: 25px 0;
            font-family: 'Courier New', monospace;
            text-align: center;
            font-size: 1.2em;
        }

        .highlight {
            background: linear-gradient(120deg, #a8e6cf 0%, #dcedc1 100%);
            padding: 20px;
            border-radius: 10px;
            margin: 25px 0;
            border-left: 4px solid #48bb78;
        }

        .example {
            background: linear-gradient(120deg, #ffeaa7 0%, #fab1a0 100%);
            padding: 20px;
            border-radius: 10px;
            margin: 25px 0;
            border-left: 4px solid #e17055;
        }

        .navigation {
            text-align: center;
            padding: 20px;
            background: #f8f9fa;
        }

        .nav-button {
            background: linear-gradient(45deg, #667eea, #764ba2);
            color: white;
            border: none;
            padding: 12px 25px;
            margin: 0 10px;
            border-radius: 25px;
            cursor: pointer;
            font-size: 1em;
            transition: all 0.3s ease;
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4);
        }

        .nav-button:disabled {
            background: #cbd5e0;
            cursor: not-allowed;
            transform: none;
            box-shadow: none;
        }

        .slide-counter {
            position: absolute;
            top: 20px;
            right: 30px;
            background: rgba(102, 126, 234, 0.1);
            padding: 10px 20px;
            border-radius: 20px;
            font-weight: bold;
            color: #4a5568;
        }

        .visual-demo {
            background: #f8f9fa;
            padding: 30px;
            border-radius: 15px;
            margin: 30px 0;
            text-align: center;
            border: 2px dashed #667eea;
        }

        .step-box {
            background: white;
            border: 2px solid #e2e8f0;
            border-radius: 10px;
            padding: 20px;
            margin: 15px 0;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .pros-cons {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin-top: 30px;
        }

        .pros {
            background: linear-gradient(120deg, #a8e6cf 0%, #dcedc1 100%);
            padding: 25px;
            border-radius: 15px;
        }

        .cons {
            background: linear-gradient(120deg, #fab1a0 0%, #ffeaa7 100%);
            padding: 25px;
            border-radius: 15px;
        }

        .title-slide {
            text-align: center;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
        }

        .title-slide h1 {
            color: white;
            border-bottom: 3px solid white;
            font-size: 3em;
        }

        .subtitle {
            font-size: 1.3em;
            margin-top: 30px;
            opacity: 0.9;
        }
    </style>
</head>
<body>
    <div class="slideshow-container">
        <div class="slide-counter">
            <span id="current-slide">1</span> / <span id="total-slides">15</span>
        </div>

        <!-- Slide 1: T√≠tulo -->
        <div class="slide active title-slide">
            <h1>Principal Component Analysis (PCA)</h1>
            <div class="subtitle">Aprendizaje No Supervisado</div>
            <div class="subtitle">Basado en "An Introduction to Statistical Learning"</div>
            <p style="margin-top: 50px; font-size: 1.2em; line-height: 1.6;">
                Una t√©cnica fundamental para reducir dimensionalidad y explorar patrones ocultos en nuestros datos
            </p>
        </div>

        <!-- Slide 2: ¬øQu√© es PCA? -->
        <div class="slide">
            <h2>¬øQu√© es el An√°lisis de Componentes Principales?</h2>
            
            <p>Imagina que tienes una gran colecci√≥n de fotograf√≠as tomadas desde diferentes √°ngulos de un mismo objeto. Aunque cada foto es diferente, todas comparten elementos esenciales que definen ese objeto. PCA es como encontrar esos elementos esenciales en nuestros datos.</p>

            <div class="highlight">
                <strong>Definici√≥n:</strong> PCA es una t√©cnica de reducci√≥n de dimensionalidad que encuentra las direcciones de m√°xima variabilidad en un conjunto de datos multidimensional.
            </div>

            <p>En lugar de trabajar con muchas variables que pueden estar correlacionadas entre s√≠, PCA nos ayuda a identificar un n√∫mero menor de "componentes principales" que capturan la mayor parte de la informaci√≥n importante de nuestros datos originales.</p>

            <div class="example">
                <strong>Analog√≠a:</strong> Es como tomar una fotograf√≠a de una escultura 3D. La foto 2D no captura toda la informaci√≥n, pero si elegimos el √°ngulo correcto, podemos preservar las caracter√≠sticas m√°s importantes de la escultura.
            </div>
        </div>

        <!-- Slide 3: ¬øPor qu√© necesitamos PCA? -->
        <div class="slide">
            <h2>¬øPor qu√© necesitamos PCA?</h2>
            
            <h3>La Maldici√≥n de la Dimensionalidad</h3>
            <p>Cuando trabajamos con datos que tienen muchas variables (alta dimensionalidad), enfrentamos varios problemas que pueden hacer que nuestros an√°lisis sean menos efectivos o incluso imposibles de realizar.</p>

            <div class="step-box">
                <strong>Problema 1: Visualizaci√≥n</strong><br>
                Es imposible visualizar datos con m√°s de 3 dimensiones de manera directa. ¬øC√≥mo podemos explorar y entender datos con 100 o 1000 variables?
            </div>

            <div class="step-box">
                <strong>Problema 2: Ruido</strong><br>
                Muchas variables pueden contener informaci√≥n redundante o ruido que oscurece los patrones reales en los datos.
            </div>

            <div class="step-box">
                <strong>Problema 3: Eficiencia Computacional</strong><br>
                Los algoritmos de aprendizaje autom√°tico se vuelven m√°s lentos y requieren m√°s memoria cuando trabajamos con muchas dimensiones.
            </div>

            <div class="highlight">
                <strong>La Soluci√≥n de PCA:</strong> Encuentra las direcciones m√°s importantes en los datos y proyecta toda la informaci√≥n a un espacio de menor dimensi√≥n, preservando la mayor cantidad de informaci√≥n posible.
            </div>
        </div>

        <!-- Slide 4: Conceptos Matem√°ticos Fundamentales -->
        <div class="slide">
            <h2>Conceptos Matem√°ticos Fundamentales</h2>
            
            <p>Antes de entender c√≥mo funciona PCA, necesitamos comprender algunos conceptos clave. No te preocupes, vamos a construir esta comprensi√≥n paso a paso.</p>

            <h3>1. Varianza</h3>
            <p>La varianza mide qu√© tan dispersos est√°n nuestros datos. Una variable con alta varianza tiene puntos muy esparcidos, mientras que una con baja varianza tiene puntos muy agrupados.</p>

            <div class="equation">
                Var(X) = E[(X - Œº)¬≤]
            </div>

            <h3>2. Covarianza</h3>
            <p>La covarianza mide c√≥mo dos variables cambian juntas. Si cuando una aumenta, la otra tambi√©n tiende a aumentar, tienen covarianza positiva.</p>

            <div class="equation">
                Cov(X,Y) = E[(X - Œº‚Çì)(Y - Œº·µß)]
            </div>

            <div class="highlight">
                <strong>Intuici√≥n Clave:</strong> PCA busca las direcciones donde los datos tienen la mayor varianza. Estas direcciones nos dicen d√≥nde est√° la mayor parte de la "acci√≥n" en nuestros datos.
            </div>
        </div>

        <!-- Slide 5: ¬øC√≥mo funciona PCA? -->
        <div class="slide">
            <h2>¬øC√≥mo funciona PCA? - El Algoritmo</h2>
            
            <p>Ahora vamos a entender el proceso paso a paso. Imagina que tienes datos en 2D y quieres reducirlos a 1D preservando la mayor informaci√≥n posible.</p>

            <div class="step-box">
                <strong>Paso 1: Centrar los datos</strong><br>
                Restamos la media de cada variable para que el centro de nuestros datos est√© en el origen (0,0).
            </div>

            <div class="step-box">
                <strong>Paso 2: Calcular la matriz de covarianza</strong><br>
                Esta matriz nos dice c√≥mo se relacionan todas las variables entre s√≠.
            </div>

            <div class="step-box">
                <strong>Paso 3: Encontrar vectores propios y valores propios</strong><br>
                Los vectores propios nos dan las direcciones principales, y los valores propios nos dicen cu√°nta varianza hay en cada direcci√≥n.
            </div>

            <div class="step-box">
                <strong>Paso 4: Ordenar por importancia</strong><br>
                Ordenamos los componentes principales por sus valores propios (de mayor a menor varianza).
            </div>

            <div class="step-box">
                <strong>Paso 5: Proyectar los datos</strong><br>
                Transformamos nuestros datos originales al nuevo espacio de componentes principales.
            </div>
        </div>

        <!-- Slide 6: C√°lculo de Autovalores y Autovectores -->
        <div class="slide">
            <h2>C√°lculo de Autovalores y Autovectores</h2>
            
            <p>Este es el coraz√≥n matem√°tico de PCA. Aunque puede parecer intimidante al principio, vamos a desmenuzar estos conceptos de manera que puedas comprenderlos profundamente.</p>

            <h3>¬øQu√© es un Autovector?</h3>
            <p>Un autovector es una direcci√≥n especial en el espacio. Cuando aplicamos una transformaci√≥n (multiplicamos por una matriz) a un autovector, este no cambia de direcci√≥n, solo se estira o se encoge. Es como si tuvi√©semos una flecha que, sin importar qu√© transformaci√≥n le apliques, siempre apunta hacia la misma direcci√≥n.</p>

            <div class="equation">
                A √ó v = Œª √ó v
            </div>
            <p style="text-align: center; font-style: italic;">Donde A es nuestra matriz, v es el autovector, y Œª es el autovalor</p>

            <h3>¬øQu√© es un Autovalor?</h3>
            <p>El autovalor (Œª) nos dice cu√°nto se estira o encoge el autovector. En PCA, este valor representa exactamente cu√°nta varianza hay en esa direcci√≥n particular de nuestros datos.</p>

            <div class="highlight">
                <strong>Conexi√≥n con PCA:</strong> La matriz de covarianza de nuestros datos tiene autovectores que nos muestran las direcciones de m√°xima variaci√≥n, y sus autovalores nos dicen cu√°nta variaci√≥n hay en cada direcci√≥n.
            </div>

            <div class="step-box">
                <strong>Proceso de C√°lculo:</strong><br>
                1. Formamos la ecuaci√≥n caracter√≠stica: det(C - ŒªI) = 0<br>
                2. Resolvemos esta ecuaci√≥n para encontrar los autovalores Œª<br>
                3. Para cada autovalor, resolvemos (C - ŒªI)v = 0 para encontrar su autovector<br>
                4. Normalizamos los autovectores para que tengan longitud unitaria
            </div>
        </div>

        <!-- Slide 7: Interpretaci√≥n de Cargas (Loadings) -->
        <div class="slide">
            <h2>C√°lculo e Interpretaci√≥n de las Cargas de Componentes</h2>
            
            <p>Una vez que tenemos nuestros componentes principales, surge la pregunta crucial: ¬øqu√© significa cada componente? Las cargas (loadings) nos revelan qu√© variables originales contribuyen m√°s a cada componente principal.</p>

            <h3>¬øQu√© son las Cargas?</h3>
            <p>Las cargas son los coeficientes que nos indican cu√°nto contribuye cada variable original a cada componente principal. Son como los "ingredientes" de la receta que crea cada componente.</p>

            <div class="equation">
                PC‚ÇÅ = w‚ÇÅ‚ÇÅ √ó X‚ÇÅ + w‚ÇÅ‚ÇÇ √ó X‚ÇÇ + w‚ÇÅ‚ÇÉ √ó X‚ÇÉ + ... + w‚ÇÅ‚Çö √ó X‚Çö
            </div>
            <p style="text-align: center; font-style: italic;">Donde w‚ÇÅ‚±º son las cargas del primer componente principal</p>

            <h3>¬øC√≥mo se Calculan las Cargas?</h3>
            <p>Aqu√≠ est√° la conexi√≥n directa con los autovectores que calculamos antes. Las cargas son exactamente los elementos de los autovectores normalizados de la matriz de covarianza.</p>

            <div class="step-box">
                <strong>Proceso de C√°lculo:</strong><br>
                1. Tomamos cada autovector obtenido de la matriz de covarianza<br>
                2. Los normalizamos para que tengan longitud unitaria (norma = 1)<br>
                3. Cada elemento del autovector normalizado es la carga de esa variable<br>
                4. Los autovectores se ordenan seg√∫n sus autovalores (mayor a menor)
            </div>

            <div class="example">
                <strong>Ejemplo Num√©rico:</strong><br>
                Si el primer autovector es [0.6, 0.8, 0.0] para variables X‚ÇÅ, X‚ÇÇ, X‚ÇÉ:<br>
                ‚Ä¢ Carga de X‚ÇÅ en PC1 = 0.6 (contribuci√≥n moderada-alta)<br>
                ‚Ä¢ Carga de X‚ÇÇ en PC1 = 0.8 (contribuci√≥n muy alta)<br>
                ‚Ä¢ Carga de X‚ÇÉ en PC1 = 0.0 (no contribuye a este componente)
            </div>

            <div class="highlight">
                <strong>Interpretaci√≥n Clave:</strong> Valores m√°s cercanos a +1 o -1 indican mayor contribuci√≥n. El signo nos indica si la variable se correlaciona positiva o negativamente con el componente.
            </div>
        </div>

        <!-- Slide 7bis: Interpretaci√≥n Pr√°ctica de Cargas -->
        <div class="slide">
            <h2>Interpretaci√≥n Pr√°ctica de las Cargas</h2>
            
            <p>Ahora que sabemos c√≥mo se calculan las cargas, veamos c√≥mo usarlas para comprender qu√© representan nuestros componentes principales en t√©rminos del dominio espec√≠fico de nuestros datos.</p>

            <h3>Estrategia de Interpretaci√≥n</h3>
            <p>Para interpretar un componente principal, buscamos patrones en las cargas. Variables con cargas altas y del mismo signo tienden a "moverse juntas" en ese componente.</p>

            <div class="example">
                <strong>Ejemplo con Datos de Estudiantes:</strong><br>
                Supongamos que tenemos notas en: Matem√°tica, F√≠sica, Qu√≠mica, Literatura, Historia<br><br>
                <strong>PC1:</strong> Math(0.7), F√≠sica(0.8), Qu√≠mica(0.6), Literatura(0.1), Historia(0.2)<br>
                <em>Interpretaci√≥n: "Habilidad en ciencias exactas"</em><br><br>
                <strong>PC2:</strong> Math(0.1), F√≠sica(0.2), Qu√≠mica(0.0), Literatura(0.8), Historia(0.7)<br>
                <em>Interpretaci√≥n: "Habilidad en humanidades"</em>
            </div>

            <div class="step-box">
                <strong>Reglas para Interpretaci√≥n:</strong><br>
                ‚Ä¢ Cargas con valor absoluto > 0.5 son consideradas importantes<br>
                ‚Ä¢ Cargas positivas y negativas nos indican direcciones opuestas<br>
                ‚Ä¢ La magnitud de la carga indica la importancia de esa variable<br>
                ‚Ä¢ Variables con cargas similares en un componente est√°n relacionadas entre s√≠
            </div>

            <div class="highlight">
                <strong>Herramienta Clave:</strong> El gr√°fico de cargas (loading plot) nos permite visualizar qu√© variables se agrupan y c√≥mo se relacionan en el espacio de componentes principales. Es fundamental para dar sentido real a nuestros resultados.
            </div>

            <div class="visual-demo">
                <h3>üí° Pens√° en esto</h3>
                <p>Si dos variables tienen cargas muy similares en el mismo componente, esto nos indica que est√°n midiendo aspectos relacionados del fen√≥meno subyacente. Esto puede ayudarnos a simplificar futuros an√°lisis o identificar redundancias en nuestros datos.</p>
            </div>
        </div>

        <!-- Slide 8: Interpretaci√≥n Geom√©trica -->
        <div class="slide">
            <h2>Interpretaci√≥n Geom√©trica</h2>
            
            <p>La mejor manera de entender PCA es visualizarlo geom√©tricamente. Imagina una nube de puntos en el espacio.</p>

            <div class="visual-demo">
                <h3>üéØ Visualizaci√≥n Conceptual</h3>
                <p><strong>Datos Originales:</strong> Una nube de puntos dispersos en 2D</p>
                <p><strong>Primer Componente Principal (PC1):</strong> La l√≠nea que mejor "atraviesa" la nube de puntos</p>
                <p><strong>Segundo Componente Principal (PC2):</strong> Perpendicular a PC1, captura la siguiente mayor variaci√≥n</p>
            </div>

            <div class="highlight">
                <strong>Analog√≠a del Proyector:</strong> Imagina que tienes una linterna y proyectas la sombra de tus datos en una pared. PCA encuentra el √°ngulo de proyecci√≥n que hace que la sombra conserve la mayor cantidad de informaci√≥n sobre la forma original de tus datos.
            </div>

            <p>Los componentes principales son como nuevas "coordenadas" para describir tus datos, pero estas coordenadas est√°n alineadas con las direcciones de mayor variaci√≥n natural en los datos.</p>

            <div class="example">
                <strong>Ejemplo Intuitivo:</strong> Si tienes datos sobre altura y peso de personas, el primer componente principal podr√≠a representar el "tama√±o general" de la persona, combinando tanto altura como peso en una sola medida.
            </div>
        </div>

        <!-- Slide 7: Selecci√≥n del N√∫mero de Componentes -->
        <div class="slide">
            <h2>¬øCu√°ntos Componentes Principales Necesitamos?</h2>
            
            <p>Una de las decisiones m√°s importantes en PCA es determinar cu√°ntos componentes principales conservar. Necesitamos un balance entre simplicidad y preservaci√≥n de informaci√≥n.</p>

            <h3>1. Criterio de Varianza Explicada</h3>
            <p>Cada componente principal explica un porcentaje de la varianza total de los datos. T√≠picamente buscamos conservar entre 80-95% de la varianza original.</p>

            <div class="equation">
                Varianza Explicada = (Valor Propio / Suma de Todos los Valores Propios) √ó 100%
            </div>

            <h3>2. Gr√°fico de Codo (Scree Plot)</h3>
            <p>Graficamos los valores propios en orden descendente y buscamos el "codo" donde la contribuci√≥n de cada componente adicional se vuelve marginal.</p>

            <h3>3. Regla de Kaiser</h3>
            <p>Conservar solo los componentes con valores propios mayores a 1 (cuando los datos est√°n estandarizados).</p>

            <div class="highlight">
                <strong>Regla Pr√°ctica:</strong> Comienza con suficientes componentes para explicar al menos 85% de la varianza, luego eval√∫a si puedes reducir m√°s sin perder informaci√≥n crucial para tu an√°lisis.
            </div>
        </div>

        <!-- Slide 8: Ejemplo Pr√°ctico -->
        <div class="slide">
            <h2>Ejemplo Pr√°ctico: An√°lisis de Datos de Vinos</h2>
            
            <p>Imaginemos que tenemos un dataset con 13 caracter√≠sticas qu√≠micas de diferentes vinos (acidez, alcohol, pH, etc.) y queremos reducir la dimensionalidad para visualizaci√≥n y an√°lisis.</p>

            <div class="example">
                <strong>Datos Originales:</strong> 178 vinos √ó 13 caracter√≠sticas qu√≠micas
            </div>

            <div class="step-box">
                <strong>Aplicando PCA:</strong><br>
                ‚Ä¢ PC1 explica 36% de la varianza (principalmente relacionado con fenoles y flavonoides)<br>
                ‚Ä¢ PC2 explica 19% de la varianza (relacionado con acidez y color)<br>
                ‚Ä¢ PC3 explica 11% de la varianza<br>
                <strong>Total: 66% de la varianza con solo 3 componentes</strong>
            </div>

            <div class="highlight">
                <strong>Interpretaci√≥n:</strong> El primer componente principal podr√≠a representar la "intensidad del sabor" del vino, combinando m√∫ltiples caracter√≠sticas relacionadas. El segundo podr√≠a relacionarse con la "acidez general" del vino.
            </div>

            <p>Con PCA podemos visualizar estos 178 vinos en un gr√°fico 2D usando PC1 y PC2, y potencialmente descubrir grupos naturales de vinos similares que no eran obvios en el espacio original de 13 dimensiones.</p>
        </div>

        <!-- Slide 9: Aplicaciones de PCA -->
        <div class="slide">
            <h2>Aplicaciones de PCA en el Mundo Real</h2>
            
            <h3>1. Procesamiento de Im√°genes</h3>
            <p>En reconocimiento facial, PCA puede reducir im√°genes de miles de p√≠xeles a unos pocos "eigenfaces" que capturan las caracter√≠sticas faciales m√°s importantes.</p>

            <h3>2. An√°lisis Financiero</h3>
            <p>Reducir cientos de indicadores econ√≥micos a unos pocos factores principales que explican los movimientos del mercado.</p>

            <h3>3. Bioinform√°tica</h3>
            <p>Analizar datos gen√≥micos con miles de genes, identificando patrones principales que diferencian entre condiciones de salud y enfermedad.</p>

            <h3>4. An√°lisis de Texto</h3>
            <p>Reducir la dimensionalidad en an√°lisis de documentos, donde cada palabra es una dimensi√≥n.</p>

            <div class="example">
                <strong>Caso de Netflix:</strong> PCA podr√≠a ayudar a reducir las miles de pel√≠culas y programas a unos pocos "g√©neros latentes" que capturan las preferencias fundamentales de los usuarios.
            </div>

            <div class="highlight">
                <strong>Clave del √âxito:</strong> PCA es especialmente √∫til cuando sospechamos que nuestros datos tienen estructura subyacente o cuando muchas variables est√°n correlacionadas entre s√≠.
            </div>
        </div>

        <!-- Slide 10: Ventajas y Limitaciones -->
        <div class="slide">
            <h2>Ventajas y Limitaciones de PCA</h2>
            
            <div class="pros-cons">
                <div class="pros">
                    <h3>‚úÖ Ventajas</h3>
                    <p><strong>Reducci√≥n de Ruido:</strong> Elimina variaciones menores que podr√≠an ser ruido</p>
                    <p><strong>Eficiencia:</strong> Acelera algoritmos posteriores al reducir dimensiones</p>
                    <p><strong>Visualizaci√≥n:</strong> Permite explorar datos complejos en 2D o 3D</p>
                    <p><strong>Decorrelaci√≥n:</strong> Los componentes principales son independientes entre s√≠</p>
                    <p><strong>Base Matem√°tica S√≥lida:</strong> M√©todo bien establecido con propiedades te√≥ricas claras</p>
                </div>
                
                <div class="cons">
                    <h3>‚ö†Ô∏è Limitaciones</h3>
                    <p><strong>Linealidad:</strong> Solo encuentra relaciones lineales entre variables</p>
                    <p><strong>Interpretabilidad:</strong> Los componentes pueden ser dif√≠ciles de interpretar</p>
                    <p><strong>Sensibilidad a Escala:</strong> Variables con escalas grandes dominan el an√°lisis</p>
                    <p><strong>P√©rdida de Informaci√≥n:</strong> Siempre se pierde algo de informaci√≥n original</p>
                    <p><strong>Outliers:</strong> Valores extremos pueden distorsionar los resultados</p>
                </div>
            </div>

            <div class="highlight">
                <strong>Recomendaci√≥n:</strong> Siempre estandariza tus variables antes de aplicar PCA y considera m√©todos no lineales como t-SNE o UMAP si sospechas relaciones no lineales complejas.
            </div>
        </div>

        <!-- Slide 11: PCA vs Otras T√©cnicas -->
        <div class="slide">
            <h2>PCA en el Contexto del Aprendizaje No Supervisado</h2>
            
            <p>PCA es una de varias t√©cnicas de aprendizaje no supervisado. Entender cu√°ndo usar cada una te ayudar√° a elegir la herramienta correcta.</p>

            <h3>PCA vs K-Means Clustering</h3>
            <p><strong>PCA:</strong> Encuentra direcciones de variaci√≥n. <strong>K-Means:</strong> Encuentra grupos de puntos similares. Ambos pueden usarse juntos: PCA para reducir dimensiones, luego K-Means para agrupar.</p>

            <h3>PCA vs Factor Analysis</h3>
            <p><strong>PCA:</strong> Explica varianza observada. <strong>Factor Analysis:</strong> Busca variables latentes que causan correlaciones observadas.</p>

            <h3>PCA vs t-SNE/UMAP</h3>
            <p><strong>PCA:</strong> Preserva distancias globales, m√©todo lineal. <strong>t-SNE/UMAP:</strong> Preservan estructura local, pueden capturar relaciones no lineales.</p>

            <div class="example">
                <strong>Flujo de Trabajo T√≠pico:</strong><br>
                1. Exploraci√≥n inicial con PCA para entender la estructura general<br>
                2. Si los datos parecen tener estructura no lineal, considerar t-SNE<br>
                3. Si buscas grupos, aplicar clustering despu√©s de reducir dimensiones
            </div>
        </div>

        <!-- Slide 12: Conclusiones y Pr√≥ximos Pasos -->
        <div class="slide">
            <h2>Conclusiones y Pr√≥ximos Pasos</h2>
            
            <div class="highlight">
                <h3>üéØ Puntos Clave para Recordar</h3>
                <p>PCA es una herramienta poderosa para simplificar datos complejos sin perder las caracter√≠sticas m√°s importantes. Es tu primera opci√≥n cuando necesitas reducir dimensionalidad manteniendo la m√°xima informaci√≥n posible.</p>
            </div>

            <h3>Para Profundizar tu Comprensi√≥n:</h3>

            <div class="step-box">
                <strong>1. Pr√°ctica con Datos Reales</strong><br>
                Implementa PCA en datasets como iris, wine, o breast cancer. Observa c√≥mo cambian los resultados con diferentes n√∫meros de componentes.
            </div>

            <div class="step-box">
                <strong>2. Experimenta con Preprocesamiento</strong><br>
                Compara resultados con y sin estandarizaci√≥n. Observa el impacto de outliers en los componentes principales.
            </div>

            <div class="step-box">
                <strong>3. Visualizaci√≥n Interactiva</strong><br>
                Crea gr√°ficos interactivos de tus componentes principales para entender mejor la estructura de tus datos.
            </div>

            <div class="example">
                <strong>Ejercicio Propuesto:</strong> Toma un dataset con m√°s de 5 variables, aplica PCA, y trata de interpretar qu√© representa cada componente principal en t√©rminos del dominio de tu problema.
            </div>

            <p style="text-align: center; margin-top: 40px; font-style: italic;">
                "La reducci√≥n de dimensionalidad no es solo una t√©cnica matem√°tica, es una forma de encontrar la esencia de nuestros datos."
            </p>
        </div>
    </div>

    <div class="navigation">
        <button class="nav-button" id="prevBtn" onclick="changeSlide(-1)">‚ùÆ Anterior</button>
        <button class="nav-button" id="nextBtn" onclick="changeSlide(1)">Siguiente ‚ùØ</button>
    </div>

    <script>
        let currentSlide = 1;
        const totalSlides = document.querySelectorAll('.slide').length;
        
        document.getElementById('total-slides').textContent = totalSlides;

        function showSlide(n) {
            const slides = document.querySelectorAll('.slide');
            
            if (n > totalSlides) currentSlide = 1;
            if (n < 1) currentSlide = totalSlides;
            
            slides.forEach(slide => slide.classList.remove('active'));
            slides[currentSlide - 1].classList.add('active');
            
            document.getElementById('current-slide').textContent = currentSlide;
            
            // Update navigation buttons
            document.getElementById('prevBtn').disabled = currentSlide === 1;
            document.getElementById('nextBtn').disabled = currentSlide === totalSlides;
        }

        function changeSlide(n) {
            currentSlide += n;
            showSlide(currentSlide);
        }

        // Keyboard navigation
        document.addEventListener('keydown', function(event) {
            if (event.key === 'ArrowLeft') changeSlide(-1);
            if (event.key === 'ArrowRight') changeSlide(1);
        });

        // Initialize
        showSlide(currentSlide);
    </script>
</body>
</html>