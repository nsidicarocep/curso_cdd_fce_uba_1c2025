<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Modelos de Clasificaci√≥n Supervisada</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            overflow: hidden;
        }

        .slideshow-container {
            position: relative;
            width: 100%;
            height: 100vh;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .slide {
            display: none;
            width: 90%;
            max-width: 1200px;
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            border-radius: 20px;
            padding: 50px;
            box-shadow: 0 25px 50px rgba(0, 0, 0, 0.25);
            animation: slideIn 0.5s ease-out;
            overflow-y: auto;
            max-height: 85vh;
        }

        .slide.active {
            display: block;
        }

        @keyframes slideIn {
            from { opacity: 0; transform: translateY(30px); }
            to { opacity: 1; transform: translateY(0); }
        }

        h1 {
            font-size: 3em;
            color: #2c3e50;
            text-align: center;
            margin-bottom: 30px;
            background: linear-gradient(135deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        h2 {
            font-size: 2.5em;
            color: #34495e;
            margin-bottom: 25px;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
        }

        h3 {
            font-size: 1.8em;
            color: #2c3e50;
            margin: 25px 0 15px 0;
            border-left: 5px solid #667eea;
            padding-left: 15px;
        }

        h4 {
            font-size: 1.4em;
            color: #34495e;
            margin: 20px 0 10px 0;
        }

        p, li {
            font-size: 1.2em;
            line-height: 1.6;
            color: #2c3e50;
            margin-bottom: 15px;
        }

        ul {
            margin-left: 25px;
            margin-bottom: 20px;
        }

        .math-formula {
            background: #f8f9ff;
            border: 2px solid #667eea;
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
            font-size: 1.1em;
            text-align: center;
            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.2);
        }

        .algorithm-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .algorithm-card {
            background: linear-gradient(135deg, #f8f9fa, #e9ecef);
            border-radius: 15px;
            padding: 25px;
            border: 2px solid #667eea;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .algorithm-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 15px 30px rgba(102, 126, 234, 0.3);
        }

        .complexity-badge {
            display: inline-block;
            padding: 8px 15px;
            border-radius: 20px;
            font-weight: bold;
            font-size: 0.9em;
            margin: 5px;
        }

        .low-complexity { background: #d4edda; color: #155724; }
        .medium-complexity { background: #fff3cd; color: #856404; }
        .high-complexity { background: #f8d7da; color: #721c24; }

        .hyperparams {
            background: #f8f9ff;
            border-left: 4px solid #667eea;
            padding: 15px;
            margin: 15px 0;
            border-radius: 5px;
        }

        .metrics-section {
            background: linear-gradient(135deg, #e8f4f8, #f1f8ff);
            border-radius: 15px;
            padding: 25px;
            margin: 25px 0;
        }

        .theory-section {
            background: linear-gradient(135deg, #fff5f5, #ffe6e6);
            border-left: 4px solid #e74c3c;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .navigation {
            position: fixed;
            bottom: 30px;
            left: 50%;
            transform: translateX(-50%);
            display: flex;
            gap: 15px;
            z-index: 1000;
        }

        .nav-btn {
            background: rgba(255, 255, 255, 0.9);
            border: none;
            padding: 15px 25px;
            border-radius: 30px;
            cursor: pointer;
            font-size: 1.1em;
            font-weight: bold;
            color: #2c3e50;
            transition: all 0.3s ease;
            backdrop-filter: blur(10px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
        }

        .nav-btn:hover {
            background: #667eea;
            color: white;
            transform: translateY(-2px);
        }

        .slide-counter {
            position: fixed;
            top: 30px;
            right: 30px;
            background: rgba(255, 255, 255, 0.9);
            padding: 10px 20px;
            border-radius: 20px;
            font-weight: bold;
            color: #2c3e50;
            backdrop-filter: blur(10px);
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .comparison-table th,
        .comparison-table td {
            padding: 15px;
            text-align: left;
            border-bottom: 1px solid #e9ecef;
        }

        .comparison-table th {
            background: #667eea;
            color: white;
            font-weight: bold;
        }

        .comparison-table tr:hover {
            background: #f8f9fa;
        }

        .intro-content {
            text-align: center;
            max-width: 800px;
            margin: 0 auto;
        }

        .key-points {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .point-card {
            background: linear-gradient(135deg, #fff, #f8f9ff);
            padding: 20px;
            border-radius: 15px;
            border: 2px solid #e9ecef;
            text-align: center;
            transition: transform 0.3s ease;
        }

        .point-card:hover {
            transform: scale(1.05);
            border-color: #667eea;
        }

        .emoji {
            font-size: 2.5em;
            margin-bottom: 10px;
            display: block;
        }

        .example-card {
            background: linear-gradient(135deg, #f0f9ff, #e0f2fe);
            border-radius: 12px;
            padding: 20px;
            margin: 15px 0;
            border-left: 5px solid #0ea5e9;
        }

        .example-card h4 {
            color: #0c4a6e;
            margin-bottom: 10px;
        }
    </style>
</head>
<body>
    <div class="slideshow-container">
        <div class="slide-counter">
            <span id="slideNum">1</span> / <span id="totalSlides">11</span>
        </div>

        <!-- Slide 1: T√≠tulo -->
        <div class="slide active">
            <div class="intro-content">
                <h1>Modelos de Clasificaci√≥n Supervisada</h1>
                <p style="font-size: 1.5em; margin: 30px 0; color: #667eea;">Una introducci√≥n te√≥rica y pr√°ctica basada en An Introduction to Statistical Learning</p>
                
                <div class="key-points">
                    <div class="point-card">
                        <span class="emoji">üéØ</span>
                        <h4>6 Algoritmos Clave</h4>
                        <p>Con fundamentos matem√°ticos</p>
                    </div>
                    <div class="point-card">
                        <span class="emoji">‚öôÔ∏è</span>
                        <h4>Hiperpar√°metros</h4>
                        <p>Optimizaci√≥n y tuning</p>
                    </div>
                    <div class="point-card">
                        <span class="emoji">üìä</span>
                        <h4>M√©tricas</h4>
                        <p>Evaluaci√≥n rigurosa</p>
                    </div>
                    <div class="point-card">
                        <span class="emoji">üèõÔ∏è</span>
                        <h4>Aplicaciones</h4>
                        <p>Ciencias sociales y econom√≠a</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 2: Overview -->
        <div class="slide">
            <h2>Clasificaci√≥n Supervisada: Panorama General</h2>
            
            <h3>üéØ Marco Te√≥rico</h3>
            <p>Dado un conjunto de entrenamiento {(x‚ÇÅ, y‚ÇÅ), (x‚ÇÇ, y‚ÇÇ), ..., (x‚Çô, y‚Çô)} donde x ‚àà ‚Ñù·µà son las caracter√≠sticas y y ‚àà {1, 2, ..., K} son las clases, buscamos una funci√≥n f: ‚Ñù·µà ‚Üí {1, 2, ..., K} que minimice el error de clasificaci√≥n esperado.</p>
            
            <div class="math-formula">
                <strong>Riesgo de Bayes:</strong> R* = E[min_k P(Y ‚â† k | X)]<br>
                <strong>Frontera de Bayes:</strong> {x : P(Y = j | X = x) = P(Y = k | X = x), j ‚â† k}
            </div>
            
            <h3>üìã Algoritmos por Paradigma</h3>
            <div class="algorithm-grid">
                <div class="algorithm-card">
                    <h4>Instance-Based</h4>
                    <span class="complexity-badge low-complexity">k-NN</span>
                    <p>Clasificaci√≥n por similaridad local</p>
                </div>
                <div class="algorithm-card">
                    <h4>Discriminativos</h4>
                    <span class="complexity-badge low-complexity">Reg. Log√≠stica</span>
                    <span class="complexity-badge high-complexity">SVM</span>
                    <p>Modelan P(Y|X) directamente</p>
                </div>
                <div class="algorithm-card">
                    <h4>Generativos</h4>
                    <span class="complexity-badge low-complexity">Naive Bayes</span>
                    <p>Modelan P(X|Y) y P(Y)</p>
                </div>
                <div class="algorithm-card">
                    <h4>Tree-Based</h4>
                    <span class="complexity-badge medium-complexity">√Årboles</span>
                    <span class="complexity-badge medium-complexity">Random Forest</span>
                    <p>Particionamiento recursivo</p>
                </div>
            </div>
        </div>

        <!-- Slide 3: Aplicaciones en Ciencias Sociales y Econom√≠a -->
        <div class="slide">
            <h2>Aplicaciones en Ciencias Sociales y Econom√≠a</h2>
            
            <div class="example-card">
                <h4>üèõÔ∏è Ciencia Pol√≠tica</h4>
                <ul>
                    <li><strong>Predicci√≥n electoral:</strong> Random Forest para predecir voto basado en demograf√≠a, historial electoral</li>
                    <li><strong>An√°lisis de sentimiento pol√≠tico:</strong> Naive Bayes en tweets/discursos para clasificar posiciones</li>
                    <li><strong>Detecci√≥n de polarizaci√≥n:</strong> SVM para clasificar textos legislativos por ideolog√≠a</li>
                </ul>
            </div>

            <div class="example-card">
                <h4>üí∞ Econom√≠a y Finanzas</h4>
                <ul>
                    <li><strong>Credit scoring:</strong> Regresi√≥n log√≠stica para aprobaci√≥n de pr√©stamos (interpretabilidad regulatoria)</li>
                    <li><strong>Detecci√≥n de fraude:</strong> Random Forest para transacciones an√≥malas</li>
                    <li><strong>Trading algor√≠tmico:</strong> SVM para se√±ales de compra/venta basadas en indicadores t√©cnicos</li>
                    <li><strong>Segmentaci√≥n de clientes:</strong> k-NN para clasificar comportamiento de compra</li>
                </ul>
            </div>

            <div class="example-card">
                <h4>üìä Sociolog√≠a y Demograf√≠a</h4>
                <ul>
                    <li><strong>Movilidad social:</strong> √Årboles de decisi√≥n para predecir clase socioecon√≥mica futura</li>
                    <li><strong>Abandono escolar:</strong> Random Forest usando variables socioecon√≥micas y acad√©micas</li>
                    <li><strong>Mercado laboral:</strong> Regresi√≥n log√≠stica para probabilidad de desempleo</li>
                </ul>
            </div>

            <div class="example-card">
                <h4>üè• Pol√≠ticas P√∫blicas</h4>
                <ul>
                    <li><strong>Asignaci√≥n de recursos:</strong> SVM para priorizar programas sociales</li>
                    <li><strong>Evaluaci√≥n de pol√≠ticas:</strong> Naive Bayes para clasificar √©xito/fracaso de intervenciones</li>
                    <li><strong>Planificaci√≥n urbana:</strong> k-NN para clasificar zonas de riesgo social</li>
                </ul>
            </div>
        </div>

        <!-- Slide 4: k-NN -->
        <div class="slide">
            <h2>k-Nearest Neighbors (k-NN)</h2>
            
            <div class="theory-section">
                <h3>üî¨ Fundamento Te√≥rico</h3>
                <p>k-NN es un m√©todo no param√©trico que estima la funci√≥n de regresi√≥n mediante promediado local. Para clasificaci√≥n, usa la regla de votaci√≥n mayoritaria entre los k vecinos m√°s cercanos.</p>
            </div>

            <div class="math-formula">
                <strong>Regla de decisi√≥n:</strong> ƒâ(x) = argmax_j Œ£_{i‚ààN_k(x)} I(y_i = j)<br><br>
                <strong>Distancia Minkowski:</strong> d_p(x,z) = (Œ£|x_j - z_j|^p)^(1/p)<br>
                <strong>Error de clasificaci√≥n:</strong> E[L] ‚Üí 2R* cuando k ‚Üí ‚àû, k/n ‚Üí 0 (Cover & Hart, 1967)
            </div>
            
            <h3>‚úÖ Ventajas Te√≥ricas</h3>
            <ul>
                <li><strong>Consistencia universal:</strong> Converge al clasificador de Bayes bajo condiciones generales</li>
                <li><strong>No asume distribuci√≥n:</strong> Free de suposiciones sobre P(X,Y)</li>
                <li><strong>Adaptabilidad local:</strong> Se ajusta autom√°ticamente a la complejidad local</li>
                <li><strong>Simplicidad conceptual:</strong> Interpretaci√≥n geom√©trica directa</li>
            </ul>
            
            <h3>‚ùå Limitaciones Te√≥ricas</h3>
            <ul>
                <li><strong>Maldici√≥n de la dimensionalidad:</strong> En alta dimensi√≥n, todos los puntos son equidistantes</li>
                <li><strong>Complejidad computacional:</strong> O(nd) por predicci√≥n sin estructuras de datos</li>
                <li><strong>Sensibilidad al ruido:</strong> Vecinos ruidosos afectan directamente la predicci√≥n</li>
                <li><strong>Frontera irregular:</strong> Puede generar fronteras de decisi√≥n muy complejas</li>
            </ul>
            
            <div class="hyperparams">
                <h4>üîß Hiperpar√°metros y Su Impacto</h4>
                <ul>
                    <li><strong>k:</strong> Controla el bias-variance tradeoff (k peque√±o ‚Üí alta varianza, k grande ‚Üí alto bias)</li>
                    <li><strong>M√©trica de distancia:</strong> p=1 (Manhattan, robusta a outliers), p=2 (Euclidiana, est√°ndar)</li>
                    <li><strong>Esquema de pesos:</strong> Uniforme vs. 1/distancia (reduce influencia de vecinos lejanos)</li>
                </ul>
            </div>
            
            <p><strong>Complejidad:</strong> Entrenamiento O(1), Predicci√≥n O(nd) donde n=muestras, d=dimensiones</p>
        </div>

        <!-- Slide 5: Regresi√≥n Log√≠stica -->
        <div class="slide">
            <h2>Regresi√≥n Log√≠stica</h2>
            
            <div class="theory-section">
                <h3>üî¨ Fundamento Te√≥rico</h3>
                <p>Modelo discriminativo que usa la funci√≥n log√≠stica para mapear cualquier valor real a (0,1), interpretable como probabilidad. Basado en el modelo lineal generalizado con funci√≥n de enlace logit.</p>
            </div>

            <div class="math-formula">
                <strong>Modelo:</strong> P(Y = 1|X) = 1/(1 + e^(-X^T Œ≤))<br>
                <strong>Log-odds:</strong> log(p/(1-p)) = X^T Œ≤<br>
                <strong>Verosimilitud:</strong> L(Œ≤) = Œ† P(Y_i|X_i)^Y_i (1-P(Y_i|X_i))^(1-Y_i)<br>
                <strong>Log-verosimilitud:</strong> ‚Ñì(Œ≤) = Œ£[Y_i X_i^T Œ≤ - log(1 + e^(X_i^T Œ≤))]
            </div>
            
            <h3>‚úÖ Propiedades Estad√≠sticas</h3>
            <ul>
                <li><strong>Estimador MLE:</strong> M√°xima verosimilitud asegura consistencia y eficiencia asint√≥tica</li>
                <li><strong>Interpretabilidad:</strong> e^Œ≤_j es el odds ratio para un cambio unitario en x_j</li>
                <li><strong>Probabilidades calibradas:</strong> Output directamente interpretable como probabilidad</li>
                <li><strong>Frontera lineal:</strong> Genera fronteras de decisi√≥n lineales en el espacio original</li>
            </ul>
            
            <h3>‚ùå Limitaciones Matem√°ticas</h3>
            <ul>
                <li><strong>Linealidad en log-odds:</strong> Asume relaci√≥n lineal entre X y log(p/(1-p))</li>
                <li><strong>Separaci√≥n perfecta:</strong> MLE no existe si las clases son linealmente separables</li>
                <li><strong>Multicolinealidad:</strong> Inestabilidad en estimaciones con X^T X singular</li>
                <li><strong>Distribuci√≥n asint√≥tica:</strong> Intervalos de confianza v√°lidos solo con muestras grandes</li>
            </ul>
            
            <div class="hyperparams">
                <h4>üîß Regularizaci√≥n y Optimizaci√≥n</h4>
                <ul>
                    <li><strong>C (regularizaci√≥n):</strong> C = 1/Œª donde Œª es par√°metro de regularizaci√≥n Ridge/Lasso</li>
                    <li><strong>penalty='l2':</strong> ||Œ≤||‚ÇÇ¬≤ (Ridge) - shrinkage continuo hacia 0</li>
                    <li><strong>penalty='l1':</strong> ||Œ≤||‚ÇÅ (Lasso) - selecci√≥n autom√°tica de variables</li>
                    <li><strong>solver:</strong> 'lbfgs' (L-BFGS), 'newton-cg' (Newton-CG), 'sag' (SAG)</li>
                </ul>
            </div>
            
            <p><strong>Complejidad:</strong> Entrenamiento O(nd √ó iter), Predicci√≥n O(d) donde iter depende de convergencia</p>
        </div>

        <!-- Slide 6: Naive Bayes -->
        <div class="slide">
            <h2>Naive Bayes</h2>
            
            <div class="theory-section">
                <h3>üî¨ Fundamento Te√≥rico</h3>
                <p>Clasificador probabil√≠stico basado en el teorema de Bayes con la suposici√≥n "naive" de independencia condicional entre caracter√≠sticas. Es un modelo generativo que estima P(X|Y) y P(Y).</p>
            </div>

            <div class="math-formula">
                <strong>Teorema de Bayes:</strong> P(Y=k|X) = P(X|Y=k)P(Y=k) / P(X)<br>
                <strong>Suposici√≥n Naive:</strong> P(X|Y=k) = Œ† P(X_j|Y=k)<br>
                <strong>Clasificador:</strong> ƒâ(x) = argmax_k P(Y=k) Œ† P(X_j=x_j|Y=k)<br>
                <strong>Log-probabilidades:</strong> log P(Y=k|X) ‚àù log P(Y=k) + Œ£ log P(X_j|Y=k)
            </div>
            
            <h3>‚úÖ Propiedades Te√≥ricas</h3>
            <ul>
                <li><strong>Optimal bajo independencia:</strong> Si las caracter√≠sticas son realmente independientes, es el clasificador de Bayes</li>
                <li><strong>Robustez sorprendente:</strong> Funciona bien incluso cuando la independencia se viola moderadamente</li>
                <li><strong>Eficiencia param√©trica:</strong> Requiere estimar solo O(dk) par√°metros</li>
                <li><strong>Manejo natural de m√∫ltiples clases:</strong> Extensi√≥n directa a K > 2</li>
            </ul>
            
            <h3>‚ùå Limitaciones Te√≥ricas</h3>
            <ul>
                <li><strong>Independencia condicional:</strong> Rara vez se cumple en datos reales</li>
                <li><strong>Estimaci√≥n de probabilidades:</strong> Puede asignar probabilidad 0 a combinaciones no vistas</li>
                <li><strong>Correlaciones perdidas:</strong> Ignora interacciones entre caracter√≠sticas</li>
                <li><strong>Calibraci√≥n:</strong> Probabilidades pueden estar mal calibradas</li>
            </ul>
            
            <div class="hyperparams">
                <h4>üîß Variantes y Par√°metros</h4>
                <ul>
                    <li><strong>alpha (Laplace smoothing):</strong> P(X_j|Y=k) = (N_jk + Œ±)/(N_k + Œ±|V_j|)</li>
                    <li><strong>Gaussian NB:</strong> Asume P(X_j|Y=k) ~ N(Œº_jk, œÉ¬≤_jk)</li>
                    <li><strong>Multinomial NB:</strong> Para conteos/frecuencias, com√∫n en texto</li>
                    <li><strong>Bernoulli NB:</strong> Para caracter√≠sticas binarias</li>
                </ul>
            </div>
            
            <p><strong>Complejidad:</strong> Entrenamiento O(nd), Predicci√≥n O(cd) donde c=clases, d=caracter√≠sticas</p>
        </div>

        <!-- Slide 7: √Årboles de Decisi√≥n -->
        <div class="slide">
            <h2>√Årboles de Decisi√≥n</h2>
            
            <div class="theory-section">
                <h3>üî¨ Fundamento Te√≥rico</h3>
                <p>Algoritmo de particionamiento recursivo que construye un √°rbol binario mediante la selecci√≥n greedy de splits que maximizan la pureza. Aproxima funciones mediante regiones constantes a trozos.</p>
            </div>

            <div class="math-formula">
                <strong>Impureza de Gini:</strong> G = Œ£ p_k(1-p_k) = 1 - Œ£ p_k¬≤<br>
                <strong>Entrop√≠a:</strong> H = -Œ£ p_k log(p_k)<br>
                <strong>Ganancia de informaci√≥n:</strong> IG(S,A) = H(S) - Œ£ |S_v|/|S| H(S_v)<br>
                <strong>Funci√≥n objetivo:</strong> min Œ£ N_L G_L + N_R G_R sujeto a split √≥ptimo
            </div>
            
            <h3>‚úÖ Propiedades Algor√≠tmicas</h3>
            <ul>
                <li><strong>Invarianza a transformaciones monot√≥nicas:</strong> Robustez a escalamiento de caracter√≠sticas</li>
                <li><strong>Selecci√≥n autom√°tica de caracter√≠sticas:</strong> Variables irrelevantes naturalmente ignoradas</li>
                <li><strong>Manejo de no linealidades:</strong> Captura interacciones complejas sin especificaci√≥n previa</li>
                <li><strong>Interpretabilidad completa:</strong> Cada predicci√≥n explicable por un conjunto de reglas</li>
            </ul>
            
            <h3>‚ùå Limitaciones Te√≥ricas</h3>
            <ul>
                <li><strong>Overfitting extremo:</strong> Sin poda, puede memorizar completamente los datos de entrenamiento</li>
                <li><strong>Inestabilidad:</strong> Alta varianza - peque√±os cambios pueden generar √°rboles muy diferentes</li>
                <li><strong>Bias en splits:</strong> Favorece caracter√≠sticas con m√°s valores √∫nicos</li>
                <li><strong>Fronteras ortogonales:</strong> Dificultad con fronteras diagonales</li>
            </ul>
            
            <div class="hyperparams">
                <h4>üîß Control de Complejidad</h4>
                <ul>
                    <li><strong>max_depth:</strong> Controla directamente la complejidad del modelo (depth = log‚ÇÇ(leaves))</li>
                    <li><strong>min_samples_split:</strong> Evita splits en muestras peque√±as (threshold estad√≠stico)</li>
                    <li><strong>min_samples_leaf:</strong> Asegura representatividad estad√≠stica en hojas</li>
                    <li><strong>min_impurity_decrease:</strong> Threshold para ganancia m√≠nima de informaci√≥n</li>
                    <li><strong>ccp_alpha:</strong> Costo-complejidad para poda post-construcci√≥n</li>
                </ul>
            </div>
            
            <p><strong>Complejidad:</strong> Entrenamiento O(n log n √ó d), Predicci√≥n O(log n) promedio, O(n) peor caso</p>
        </div>

        <!-- Slide 8: SVM -->
        <div class="slide">
            <h2>Support Vector Machines (SVM)</h2>
            
            <div class="theory-section">
                <h3>üî¨ Fundamento Te√≥rico</h3>
                <p>Basado en la teor√≠a de minimizaci√≥n del riesgo estructural (SRM). Encuentra el hiperplano que maximiza el margen entre clases, implementando el principio inductivo de m√°ximo margen.</p>
            </div>

            <div class="math-formula">
                <strong>Problema primal:</strong> min ¬Ω||w||¬≤ + C Œ£Œæ·µ¢ sujeto a y·µ¢(w^T x·µ¢ + b) ‚â• 1 - Œæ·µ¢<br>
                <strong>Problema dual:</strong> max Œ£Œ±·µ¢ - ¬ΩŒ£Œ£Œ±·µ¢Œ±‚±ºy·µ¢y‚±ºK(x·µ¢,x‚±º)<br>
                <strong>Funci√≥n de decisi√≥n:</strong> f(x) = sign(Œ£Œ±·µ¢y·µ¢K(x·µ¢,x) + b)<br>
                <strong>Condiciones KKT:</strong> Œ±·µ¢(y·µ¢f(x·µ¢) - 1 + Œæ·µ¢) = 0
            </div>
            
            <h3>‚úÖ Propiedades Te√≥ricas</h3>
            <ul>
                <li><strong>Principio de m√°ximo margen:</strong> Maximiza la distancia m√≠nima entre clases</li>
                <li><strong>Teor√≠a VC:</strong> Generalization bound basado en la dimensi√≥n VC del espacio de caracter√≠sticas</li>
                <li><strong>Kernel trick:</strong> Mapeo impl√≠cito a espacios de alta dimensi√≥n sin costo computacional expl√≠cito</li>
                <li><strong>Sparsity:</strong> Solo los vectores de soporte (Œ±·µ¢ > 0) influyen en la predicci√≥n</li>
            </ul>
            
            <h3>‚ùå Limitaciones Matem√°ticas</h3>
            <ul>
                <li><strong>Escalabilidad:</strong> Complejidad O(n¬≥) del problema de optimizaci√≥n cuadr√°tica</li>
                <li><strong>Selecci√≥n de kernel:</strong> No hay teor√≠a sistem√°tica para elegir K(¬∑,¬∑) √≥ptimo</li>
                <li><strong>Probabilidades:</strong> No produce probabilidades naturalmente (requiere calibraci√≥n)</li>
                <li><strong>Ruido y outliers:</strong> Sensible a puntos mal etiquetados en la frontera</li>
            </ul>
            
            <div class="hyperparams">
                <h4>üîß Kernels y Regularizaci√≥n</h4>
                <ul>
                    <li><strong>C:</strong> Trade-off entre margen y error de entrenamiento (C‚Üí‚àû: hard margin)</li>
                    <li><strong>kernel='rbf':</strong> K(x,z) = exp(-Œ≥||x-z||¬≤), Œ≥ controla la "localidad"</li>
                    <li><strong>kernel='poly':</strong> K(x,z) = (Œ≥x^T z + r)^d, captura interacciones de orden d</li>
                    <li><strong>gamma:</strong> 1/(2œÉ¬≤) en RBF, controla el radio de influencia</li>
                </ul>
            </div>
            
            <p><strong>Complejidad:</strong> Entrenamiento O(n¬≤ a n¬≥), Predicci√≥n O(|SV| √ó d) donde |SV| = n√∫mero de vectores de soporte</p>
        </div>

        <!-- Slide 9: Random Forest -->
        <div class="slide">
            <h2>Random Forest</h2>
            
            <div class="theory-section">
                <h3>üî¨ Fundamento Te√≥rico</h3>
                <p>M√©todo ensemble que combina bagging con selecci√≥n aleatoria de caracter√≠sticas. Basado en la teor√≠a de agregaci√≥n de predictores y la reducci√≥n de varianza mediante averaging.</p>
            </div>

            <div class="math-formula">
                <strong>Predicci√≥n ensemble:</strong> ƒâ(x) = argmax_k Œ£·µ¶ I(T_b(x) = k)<br>
                <strong>Bootstrap sample:</strong> S_b ~ Multinomial(n, 1/n, ..., 1/n)<br>
                <strong>Error OOB:</strong> err_OOB = (1/n) Œ£·µ¢ I(ƒâ‚Çã·µ¢(x·µ¢) ‚â† y·µ¢)<br>
                <strong>Varianza:</strong> Var[√ä] = œÅœÉ¬≤ + (1-œÅ)œÉ¬≤/B donde œÅ = correlaci√≥n entre √°rboles
            </div>
            
            <h3>‚úÖ Propiedades Estad√≠sticas</h3>
            <ul>
                <li><strong>Reducci√≥n de varianza:</strong> Averaging de B predictores reduce varianza por factor B</li>
                <li><strong>Consistencia:</strong> Converge al l√≠mite de Bagging cuando B ‚Üí ‚àû</li>
                <li><strong>Error OOB:</strong> Estimaci√≥n insesgada del error de generalizaci√≥n sin validation set</li>
                <li><strong>Feature importance:</strong> Medida natural via disminuci√≥n promedio de impureza</li>
            </ul>
            
            <h3>‚ùå Limitaciones Te√≥ricas</h3>
            <ul>
                <li><strong>Overfitting en ruido:</strong> Puede hacer overfitting en datasets muy ruidosos</li>
                <li><strong>Correlaci√≥n residual:</strong> √Årboles siguen correlacionados a pesar de randomizaci√≥n</li>
                <li><strong>Interpretabilidad perdida:</strong> Imposible extraer reglas simples del ensemble</li>
                <li><strong>Bias hacia categor√≠as frecuentes:</strong> Hereda sesgos de √°rboles individuales</li>
            </ul>
            
            <div class="hyperparams">
                <h4>üîß Control del Ensemble</h4>
                <ul>
                    <li><strong>n_estimators:</strong> N√∫mero de √°rboles B (m√°s √°rboles ‚Üí menor varianza, rendimientos decrecientes)</li>
                    <li><strong>max_features:</strong> m ‚âà ‚àöd reduce correlaci√≥n entre √°rboles</li>
                    <li><strong>bootstrap:</strong> True para bagging, False para pasting</li>
                    <li><strong>oob_score:</strong> Calcula estimaci√≥n OOB del error</li>
                    <li><strong>random_state:</strong> Controla reproducibilidad del proceso aleatorio</li>
                </ul>
            </div>
            
            <p><strong>Complejidad:</strong> Entrenamiento O(B √ó n log n √ó d √ó ‚àöd), Predicci√≥n O(B √ó log n √ó d)</p>
        </div>

        <!-- Slide 10: M√©tricas de Evaluaci√≥n -->
        <div class="slide">
            <h2>M√©tricas de Evaluaci√≥n</h2>
            
            <div class="theory-section">
                <h3>üî¨ Fundamento Estad√≠stico</h3>
                <p>Las m√©tricas de evaluaci√≥n cuantifican diferentes aspectos del performance del clasificador. Cada m√©trica captura un trade-off espec√≠fico entre tipos de errores.</p>
            </div>

            <div class="math-formula">
                <strong>Matriz de Confusi√≥n:</strong> C[i,j] = #{(x,y) : y = i, ƒâ(x) = j}<br>
                <strong>Accuracy:</strong> (TP + TN)/(TP + TN + FP + FN)<br>
                <strong>Precision:</strong> TP/(TP + FP), <strong>Recall:</strong> TP/(TP + FN)<br>
                <strong>F‚ÇÅ-Score:</strong> 2/(1/Precision + 1/Recall) = 2TP/(2TP + FP + FN)
            </div>
            
            <div class="metrics-section">
                <h3>üìä M√©tricas Avanzadas</h3>
                
                <h4>Para Clasificaci√≥n Probabil√≠stica</h4>
                <ul>
                    <li><strong>Log-Loss (Cross-entropy):</strong> -Œ£ y log(pÃÇ) + (1-y) log(1-pÃÇ)</li>
                    <li><strong>Brier Score:</strong> Œ£(pÃÇ - y)¬≤ (scoring rule propia)</li>
                    <li><strong>Calibration:</strong> E[Y|pÃÇ = p] = p para todo p</li>
                </ul>
                
                <h4>M√©tricas Robustas</h4>
                <ul>
                    <li><strong>AUC-ROC:</strong> ‚à´ TPR(FPR‚Åª¬π(t)) dt, invariante a threshold</li>
                    <li><strong>AUC-PR:</strong> Mejor para datos desbalanceados</li>
                    <li><strong>Matthews Correlation Coefficient:</strong> œÜ = (TP√óTN - FP√óFN)/‚àö((TP+FP)(TP+FN)(TN+FP)(TN+FN))</li>
                </ul>
                
                <h4>Multi-clase</h4>
                <ul>
                    <li><strong>Macro-averaging:</strong> (1/K) Œ£ metric_k (trata clases igualmente)</li>
                    <li><strong>Micro-averaging:</strong> metric(Œ£ TP_k, Œ£ FP_k, Œ£ FN_k) (ponderado por frecuencia)</li>
                    <li><strong>Weighted-averaging:</strong> Œ£ w_k √ó metric_k donde w_k = support_k</li>
                </ul>
            </div>
            
            <h3>üéØ Consideraciones Estad√≠sticas</h3>
            <ul>
                <li><strong>Intervalos de confianza:</strong> Bootstrap o aproximaci√≥n normal para cuantificar incertidumbre</li>
                <li><strong>Significancia estad√≠stica:</strong> Test de McNemar para comparar clasificadores</li>
                <li><strong>Cross-validation:</strong> k-fold o stratified para estimaci√≥n insesgada</li>
                <li><strong>Correcci√≥n de Bonferroni:</strong> Para comparaciones m√∫ltiples</li>
            </ul>
        </div>

        <!-- Slide 11: Comparaci√≥n y Resumen -->
        <div class="slide">
            <h2>Comparaci√≥n Te√≥rica y Gu√≠a de Selecci√≥n</h2>
            
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Algoritmo</th>
                        <th>Tipo</th>
                        <th>Suposiciones</th>
                        <th>Frontera de Decisi√≥n</th>
                        <th>Complejidad Modelo</th>
                        <th>Interpretabilidad</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>k-NN</strong></td>
                        <td>No param√©trico</td>
                        <td>Localidad + suavidad</td>
                        <td>Irregular, no linear</td>
                        <td>O(n) memoria</td>
                        <td>Media (vecindario)</td>
                    </tr>
                    <tr>
                        <td><strong>Reg. Log√≠stica</strong></td>
                        <td>Param√©trico</td>
                        <td>Linealidad en log-odds</td>
                        <td>Linear</td>
                        <td>O(d) par√°metros</td>
                        <td>Alta (coeficientes)</td>
                    </tr>
                    <tr>
                        <td><strong>Naive Bayes</strong></td>
                        <td>Generativo</td>
                        <td>Independencia condicional</td>
                        <td>Linear (log-prob)</td>
                        <td>O(dk) par√°metros</td>
                        <td>Media (probabilidades)</td>
                    </tr>
                    <tr>
                        <td><strong>√Årboles</strong></td>
                        <td>No param√©trico</td>
                        <td>Estructura jer√°rquica</td>
                        <td>Rect√°ngulos ortogonales</td>
                        <td>O(n) nodos</td>
                        <td>Muy alta (reglas)</td>
                    </tr>
                    <tr>
                        <td><strong>SVM</strong></td>
                        <td>No param√©trico</td>
                        <td>M√°ximo margen</td>
                        <td>Flexible (kernel)</td>
                        <td>O(|SV|) vectores</td>
                        <td>Baja (kernel)</td>
                    </tr>
                    <tr>
                        <td><strong>Random Forest</strong></td>
                        <td>Ensemble</td>
                        <td>Sabidur√≠a de multitudes</td>
                        <td>Promedio de rect√°ngulos</td>
                        <td>O(Bn) nodos</td>
                        <td>Media (importancia)</td>
                    </tr>
                </tbody>
            </table>
            
            <h3>üéØ Marco de Selecci√≥n Te√≥rico</h3>
            <div class="algorithm-grid">
                <div class="algorithm-card">
                    <h4>üìä Datos peque√±os (n < 1000)</h4>
                    <p>Naive Bayes, Regresi√≥n Log√≠stica, k-NN</p>
                </div>
                <div class="algorithm-card">
                    <h4>üîç Alta dimensionalidad (d > n)</h4>
                    <p>Regresi√≥n Log√≠stica (L1), SVM, Naive Bayes</p>
                </div>
                <div class="algorithm-card">
                    <h4>üé≠ No linealidad</h4>
                    <p>SVM (RBF), Random Forest, k-NN</p>
                </div>
                <div class="algorithm-card">
                    <h4>‚öñÔ∏è Interpretabilidad cr√≠tica</h4>
                    <p>√Årboles de Decisi√≥n, Regresi√≥n Log√≠stica</p>
                </div>
            </div>
            
            <div class="theory-section">
                <h3>üìà Principios de Selecci√≥n de Modelos</h3>
                <ol>
                    <li><strong>Principio de parsimonia (Occam's Razor):</strong> Modelo m√°s simple que explique los datos</li>
                    <li><strong>Bias-Variance Tradeoff:</strong> Balance entre sesgo y varianza seg√∫n tama√±o muestral</li>
                    <li><strong>No Free Lunch Theorem:</strong> No existe un algoritmo universalmente superior</li>
                    <li><strong>Validaci√≥n cruzada:</strong> Estimaci√≥n honesta del error de generalizaci√≥n</li>
                    <li><strong>Ensemble methods:</strong> Combinaci√≥n reduce riesgo de selecci√≥n sub√≥ptima</li>
                </ol>
            </div>
        </div>
    </div>

    <div class="navigation">
        <button class="nav-btn" onclick="changeSlide(-1)">‚óÄ Anterior</button>
        <button class="nav-btn" onclick="changeSlide(1)">Siguiente ‚ñ∂</button>
    </div>

    <script>
        let currentSlide = 0;
        const slides = document.querySelectorAll('.slide');
        const totalSlides = slides.length;
        
        document.getElementById('totalSlides').textContent = totalSlides;
        
        function showSlide(n) {
            slides[currentSlide].classList.remove('active');
            currentSlide = (n + totalSlides) % totalSlides;
            slides[currentSlide].classList.add('active');
            document.getElementById('slideNum').textContent = currentSlide + 1;
        }
        
        function changeSlide(direction) {
            showSlide(currentSlide + direction);
        }
        
        // Keyboard navigation
        document.addEventListener('keydown', function(e) {
            if (e.key === 'ArrowLeft') changeSlide(-1);
            if (e.key === 'ArrowRight') changeSlide(1);
        });
        
        // Touch/swipe support
        let touchStartX = 0;
        let touchEndX = 0;
        
        document.addEventListener('touchstart', function(e) {
            touchStartX = e.changedTouches[0].screenX;
        });
        
        document.addEventListener('touchend', function(e) {
            touchEndX = e.changedTouches[0].screenX;
            if (touchStartX - touchEndX > 50) changeSlide(1);  // Swipe left
            if (touchEndX - touchStartX > 50) changeSlide(-1); // Swipe right
        });
    </script>
</body>
</html>