<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Clase 1: Introducci√≥n a Machine Learning</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: #333;
            overflow: hidden;
        }

        .presentation-container {
            height: 100vh;
            display: flex;
            align-items: center;
            justify-content: center;
            position: relative;
        }

        .slide {
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 40px rgba(0,0,0,0.1);
            padding: 40px;
            max-width: 90%;
            max-height: 85%;
            overflow-y: auto;
            display: none;
            animation: slideIn 0.5s ease-out;
        }

        .slide.active {
            display: block;
        }

        @keyframes slideIn {
            from {
                opacity: 0;
                transform: translateX(30px);
            }
            to {
                opacity: 1;
                transform: translateX(0);
            }
        }

        h1 {
            color: #4a5568;
            font-size: 2.5em;
            margin-bottom: 30px;
            text-align: center;
            border-bottom: 3px solid #667eea;
            padding-bottom: 15px;
        }

        h2 {
            color: #667eea;
            font-size: 2em;
            margin: 25px 0 20px 0;
            border-left: 5px solid #667eea;
            padding-left: 15px;
        }

        h3 {
            color: #4a5568;
            font-size: 1.4em;
            margin: 20px 0 15px 0;
            font-weight: 600;
        }

        h4 {
            color: #764ba2;
            font-size: 1.2em;
            margin: 15px 0 10px 0;
            font-weight: 600;
        }

        p, li {
            line-height: 1.6;
            margin-bottom: 10px;
            font-size: 1.1em;
        }

        ul {
            margin-left: 20px;
            margin-bottom: 15px;
        }

        li {
            margin-bottom: 8px;
        }

        .highlight {
            background: linear-gradient(120deg, #a8edea 0%, #fed6e3 100%);
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 5px solid #667eea;
        }

        .example {
            background: #f8f9ff;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border: 1px solid #e2e8f0;
        }

        .code-block {
            background: #2d3748;
            color: #e2e8f0;
            padding: 15px;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            margin: 15px 0;
            overflow-x: auto;
        }

        .navigation {
            position: fixed;
            bottom: 30px;
            left: 50%;
            transform: translateX(-50%);
            display: flex;
            gap: 15px;
            z-index: 1000;
        }

        .nav-btn {
            background: rgba(255, 255, 255, 0.9);
            border: none;
            padding: 12px 20px;
            border-radius: 25px;
            cursor: pointer;
            font-weight: 600;
            color: #4a5568;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
            transition: all 0.3s ease;
            backdrop-filter: blur(10px);
        }

        .nav-btn:hover {
            background: #667eea;
            color: white;
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(0,0,0,0.15);
        }

        .nav-btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
        }

        .slide-counter {
            position: fixed;
            top: 30px;
            right: 30px;
            background: rgba(255, 255, 255, 0.9);
            padding: 10px 20px;
            border-radius: 20px;
            color: #4a5568;
            font-weight: 600;
            backdrop-filter: blur(10px);
        }

        .two-column {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 20px 0;
        }

        .metrics-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .metric-card {
            background: #f8f9ff;
            padding: 20px;
            border-radius: 10px;
            border: 1px solid #e2e8f0;
            text-align: center;
        }

        .emoji {
            font-size: 2em;
            margin-bottom: 10px;
            display: block;
        }

        .warning {
            background: #fed7d7;
            border: 1px solid #feb2b2;
            color: #c53030;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }

        .success {
            background: #c6f6d5;
            border: 1px solid #9ae6b4;
            color: #2f855a;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }

        @media (max-width: 768px) {
            .slide {
                padding: 20px;
                max-width: 95%;
                max-height: 90%;
            }
            
            h1 {
                font-size: 2em;
            }
            
            h2 {
                font-size: 1.5em;
            }
            
            .two-column {
                grid-template-columns: 1fr;
            }
            
            .navigation {
                bottom: 20px;
            }
        }
    </style>
</head>
<body>
    <div class="presentation-container">
        <div class="slide-counter">
            <span id="current-slide">1</span> / <span id="total-slides">0</span>
        </div>

        <!-- Slide 1: Portada -->
        <div class="slide active">
            <h1>ü§ñ Machine Learning</h1>
            <div style="text-align: center; margin: 50px 0;">
                <h2 style="border: none; color: #764ba2;">Conceptos Fundamentales y Principios Generales</h2>
                <div style="margin: 40px 0; font-size: 1.3em; color: #4a5568;">
                    <p><strong>Clase 1:</strong> Introducci√≥n a Machine Learning</p>
                    <p style="margin-top: 20px; font-size: 1.1em;">Unidad de Big Data y Machine Learning</p>
                </div>
            </div>
            <div class="highlight">
                <h3>üìã Agenda de Hoy</h3>
                <ul>
                    <li>¬øQu√© es Machine Learning?</li>
                    <li>Tipos de aprendizaje y algoritmos</li>
                    <li>El proceso completo de ML</li>
                    <li>Conceptos clave universales</li>
                    <li>Sesgos y consideraciones √©ticas</li>
                </ul>
            </div>
        </div>

        <!-- Slide 2: ¬øQu√© es ML? -->
        <div class="slide">
            <h1>¬øQu√© es Machine Learning? üß†</h1>
            
            <div class="highlight">
                <h3>Definici√≥n</h3>
                <p>Machine Learning es una rama de la inteligencia artificial que permite a las computadoras <strong>aprender y tomar decisiones a partir de datos</strong> sin ser expl√≠citamente programadas para cada tarea espec√≠fica.</p>
            </div>

            <div class="two-column">
                <div>
                    <h3>üßí Analog√≠a Simple</h3>
                    <div class="example">
                        <h4>Ense√±ar a reconocer perros:</h4>
                        <p><strong>Programaci√≥n tradicional:</strong><br>
                        "Tiene 4 patas, ladra, tiene pelo..."</p>
                        <p><strong>Machine Learning:</strong><br>
                        Mostrar miles de fotos de perros y no-perros, el algoritmo aprende por s√≠ mismo</p>
                    </div>
                </div>
                <div>
                    <h3>‚ö° Diferencia Clave</h3>
                    <div class="code-block">
Programaci√≥n tradicional:
Datos + Programa ‚Üí Resultado

Machine Learning:
Datos + Resultado ‚Üí Programa
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 3: Tipos de Aprendizaje -->
        <div class="slide">
            <h1>Tipos de Aprendizaje üìö</h1>
            
            <div class="metrics-grid">
                <div class="metric-card">
                    <span class="emoji">üë®‚Äçüè´</span>
                    <h3>Aprendizaje Supervisado</h3>
                    <p><strong>Qu√© es:</strong> Aprendemos con ejemplos que ya tienen la "respuesta correcta"</p>
                    <div class="example">
                        <strong>Ejemplos:</strong>
                        <ul style="text-align: left;">
                            <li>Predecir precios de casas</li>
                            <li>Detectar spam en emails</li>
                        </ul>
                    </div>
                </div>

                <div class="metric-card">
                    <span class="emoji">üîç</span>
                    <h3>Aprendizaje No Supervisado</h3>
                    <p><strong>Qu√© es:</strong> Encontramos patrones en datos sin "respuestas correctas"</p>
                    <div class="example">
                        <strong>Ejemplos:</strong>
                        <ul style="text-align: left;">
                            <li>Segmentar clientes</li>
                            <li>Detectar anomal√≠as</li>
                        </ul>
                    </div>
                </div>

                <div class="metric-card">
                    <span class="emoji">üéÆ</span>
                    <h3>Aprendizaje por Refuerzo</h3>
                    <p><strong>Qu√© es:</strong> Aprende a trav√©s de ensayo y error, recibiendo recompensas</p>
                    <div class="example">
                        <strong>Ejemplos:</strong>
                        <ul style="text-align: left;">
                            <li>Juegos (AlphaGo)</li>
                            <li>Autos aut√≥nomos</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="highlight">
                <h3>üìñ En este curso nos enfocaremos en:</h3>
                <ul>
                    <li><strong>Regresi√≥n:</strong> Predecir valores num√©ricos continuos</li>
                    <li><strong>Clasificaci√≥n:</strong> Predecir categor√≠as o clases</li>
                </ul>
            </div>
        </div>

        <!-- Slide 4: Aprendizaje No Supervisado - Panorama -->
        <div class="slide">
            <h1>Aprendizaje No Supervisado üîç</h1>
            
            <div class="highlight">
                <h3>üîç Diferencia Fundamental con Supervisado</h3>
                <div class="two-column">
                    <div class="success">
                        <h4>‚úÖ Aprendizaje Supervisado</h4>
                        <ul style="text-align: left;">
                            <li>Tenemos <strong>targets</strong> (respuestas correctas)</li>
                            <li>Ejemplos: X ‚Üí Y</li>
                            <li>Objetivo: Predecir Y para nuevas X</li>
                            <li>Evaluamos comparando predicci√≥n vs realidad</li>
                        </ul>
                        <div class="code-block">
Datos: [Edad, Ingresos] ‚Üí [Compra: S√≠/No]<br>
Objetivo: Predecir si comprar√°
                        </div>
                    </div>
                    <div class="warning">
                        <h4>üîç Aprendizaje No Supervisado</h4>
                        <ul style="text-align: left;">
                            <li><strong>NO hay targets</strong> (no respuestas correctas)</li>
                            <li>Solo tenemos X</li>
                            <li>Objetivo: Encontrar patrones ocultos</li>
                            <li>Evaluamos con m√©tricas internas</li>
                        </ul>
                        <div class="code-block">
Datos: [Edad, Ingresos, Gastos]<br>
Objetivo: ¬øQu√© grupos naturales existen?
                        </div>
                    </div>
                </div>
            </div>

            <div class="metrics-grid">
                <div class="metric-card">
                    <span class="emoji">üéØ</span>
                    <h3>Clustering</h3>
                    <p><strong>Objetivo:</strong> Agrupar datos similares</p>
                    <p><strong>Pregunta:</strong> "¬øQu√© segmentos naturales existen en mis datos?"</p>
                    <div class="example">
                        <strong>Casos de uso:</strong>
                        <ul style="text-align: left; font-size: 0.9em;">
                            <li>Segmentaci√≥n de clientes</li>
                            <li>An√°lisis de mercado</li>
                            <li>Organizaci√≥n de productos</li>
                        </ul>
                    </div>
                </div>

                <div class="metric-card">
                    <span class="emoji">üìâ</span>
                    <h3>Reducci√≥n de Dimensionalidad</h3>
                    <p><strong>Objetivo:</strong> Simplificar datos manteniendo informaci√≥n importante</p>
                    <p><strong>Pregunta:</strong> "¬øCu√°les son las variables m√°s importantes?"</p>
                    <div class="example">
                        <strong>Casos de uso:</strong>
                        <ul style="text-align: left; font-size: 0.9em;">
                            <li>Visualizaci√≥n de datos complejos</li>
                            <li>Compresi√≥n de informaci√≥n</li>
                            <li>Feature selection autom√°tica</li>
                        </ul>
                    </div>
                </div>

                <div class="metric-card">
                    <span class="emoji">üö®</span>
                    <h3>Detecci√≥n de Anomal√≠as</h3>
                    <p><strong>Objetivo:</strong> Identificar puntos "raros" o at√≠picos</p>
                    <p><strong>Pregunta:</strong> "¬øQu√© observaciones son diferentes al resto?"</p>
                    <div class="example">
                        <strong>Casos de uso:</strong>
                        <ul style="text-align: left; font-size: 0.9em;">
                            <li>Detecci√≥n de fraude</li>
                            <li>Control de calidad</li>
                            <li>Monitoreo de sistemas</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 5: Clustering - Detalles -->
        <div class="slide">
            <h1>Clustering - Encontrando Grupos Naturales üéØ</h1>
            
            <div class="example">
                <h3>üíº Ejemplo Pr√°ctico: Segmentaci√≥n de Clientes E-commerce</h3>
                <div class="two-column">
                    <div>
                        <h4>üìä Variables disponibles:</h4>
                        <ul style="font-size: 0.9em;">
                            <li>Frecuencia de compra</li>
                            <li>Monto promedio por compra</li>
                            <li>√öltima compra (recency)</li>
                            <li>Categor√≠as preferidas</li>
                            <li>Tiempo como cliente</li>
                        </ul>
                    </div>
                    <div>
                        <h4>üéØ Clusters encontrados:</h4>
                        <ul style="font-size: 0.9em;">
                            <li><strong>VIP:</strong> Alta frecuencia, alto monto</li>
                            <li><strong>Ocasionales:</strong> Baja frecuencia, alto monto</li>
                            <li><strong>Frecuentes:</strong> Alta frecuencia, bajo monto</li>
                            <li><strong>En riesgo:</strong> Sin compras recientes</li>
                        </ul>
                    </div>
                </div>
                <div class="success" style="margin-top: 15px;">
                    <strong>üí° Valor de negocio:</strong> Estrategias de marketing personalizadas para cada segmento
                </div>
            </div>

            <div class="metrics-grid">
                <div class="metric-card">
                    <h3>K-Means</h3>
                    <p><strong>¬øC√≥mo funciona?</strong> Divide datos en K grupos, minimizando distancia dentro de grupos</p>
                    <p><strong>Ventajas:</strong> Simple, r√°pido, escalable</p>
                    <p><strong>Desventajas:</strong> Requiere definir K a priori, asume clusters esf√©ricos</p>
                    <div class="code-block">
1. Elegir K (n√∫mero de clusters)<br>
2. Inicializar K centroides<br>
3. Asignar puntos al centroide m√°s cercano<br>
4. Actualizar centroides<br>
5. Repetir hasta convergencia
                    </div>
                </div>

                <div class="metric-card">
                    <h3>Hierarchical Clustering</h3>
                    <p><strong>¬øC√≥mo funciona?</strong> Construye jerarqu√≠a de clusters (dendrograma)</p>
                    <p><strong>Ventajas:</strong> No requiere definir K, visualizaci√≥n clara</p>
                    <p><strong>Desventajas:</strong> Computacionalmente costoso para datasets grandes</p>
                    <div class="example">
                        <strong>Tipos:</strong>
                        <ul style="text-align: left; font-size: 0.9em;">
                            <li><strong>Aglomerativo:</strong> Bottom-up (une clusters)</li>
                            <li><strong>Divisivo:</strong> Top-down (divide clusters)</li>
                        </ul>
                    </div>
                </div>

                <div class="metric-card">
                    <h3>DBSCAN</h3>
                    <p><strong>¬øC√≥mo funciona?</strong> Agrupa puntos densos, identifica outliers</p>
                    <p><strong>Ventajas:</strong> Encuentra clusters de forma arbitraria, detecta outliers</p>
                    <p><strong>Desventajas:</strong> Sensible a par√°metros, problemas con densidades variables</p>
                    <div class="example">
                        <strong>Conceptos clave:</strong>
                        <ul style="text-align: left; font-size: 0.9em;">
                            <li><strong>Core points:</strong> Puntos con suficientes vecinos</li>
                            <li><strong>Border points:</strong> Cerca de core points</li>
                            <li><strong>Noise:</strong> Outliers</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 6: M√©tricas No Supervisado -->
        <div class="slide">
            <h1>M√©tricas en Aprendizaje No Supervisado üìè</h1>
            
            <div class="warning">
                <h3>üö® El Desaf√≠o: ¬øC√≥mo evaluar sin "respuestas correctas"?</h3>
                <p>En supervisado comparamos predicci√≥n vs realidad. En no supervisado, <strong>no hay realidad conocida</strong>. Usamos m√©tricas <strong>internas</strong> que miden la calidad de los patrones encontrados.</p>
            </div>

            <div class="metrics-grid">
                <div class="metric-card">
                    <h3>Silhouette Score</h3>
                    <div class="code-block">
Silhouette = (b - a) / max(a, b)<br>
a = distancia promedio intra-cluster<br>
b = distancia promedio al cluster m√°s cercano
                    </div>
                    <p><strong>Rango:</strong> -1 a 1</p>
                    <ul style="text-align: left; font-size: 0.9em;">
                        <li><strong>Cerca de 1:</strong> Clusters bien separados</li>
                        <li><strong>Cerca de 0:</strong> Clusters superpuestos</li>
                        <li><strong>Negativo:</strong> Puntos en cluster incorrecto</li>
                    </ul>
                    <p><strong>Uso:</strong> Elegir n√∫mero √≥ptimo de clusters</p>
                </div>

                <div class="metric-card">
                    <h3>Inertia (Within-cluster sum of squares)</h3>
                    <div class="code-block">
Inertia = Œ£ ||xi - centroide||¬≤<br>
Para todos los puntos xi
                    </div>
                    <p><strong>¬øQu√© mide?</strong> Qu√© tan compactos son los clusters</p>
                    <p><strong>Menor es mejor</strong>, pero cuidado con overfitting</p>
                    <div class="example">
                        <strong>Elbow Method:</strong>
                        <ul style="text-align: left; font-size: 0.9em;">
                            <li>Graficar Inertia vs K</li>
                            <li>Buscar "codo" en la curva</li>
                            <li>Balance entre ajuste y simplicidad</li>
                        </ul>
                    </div>
                </div>

                <div class="metric-card">
                    <h3>Calinski-Harabasz Index</h3>
                    <div class="code-block">
CH = (Between-cluster dispersion / Within-cluster dispersion)<br>
* (n-k) / (k-1)
                    </div>
                    <p><strong>¬øQu√© mide?</strong> Ratio de separaci√≥n entre clusters vs compactaci√≥n dentro</p>
                    <p><strong>Mayor es mejor</strong></p>
                    <div class="example">
                        <strong>Ventajas:</strong>
                        <ul style="text-align: left; font-size: 0.9em;">
                            <li>R√°pido de calcular</li>
                            <li>No requiere distancias entre todos los puntos</li>
                            <li>Buen balance interpretabilidad-performance</li>
                        </ul>
                    </div>
                </div>

                <div class="metric-card">
                    <h3>Davies-Bouldin Index</h3>
                    <div class="code-block">
DB = (1/k) * Œ£ max(Rij)<br>
Rij = (Si + Sj) / Mij<br>
Si = dispersi√≥n cluster i<br>
Mij = distancia entre centroides
                    </div>
                    <p><strong>¬øQu√© mide?</strong> Promedio de similitud entre cada cluster y su m√°s similar</p>
                    <p><strong>Menor es mejor</strong></p>
                    <div class="example">
                        <strong>Interpretaci√≥n:</strong>
                        <ul style="text-align: left; font-size: 0.9em;">
                            <li>Clusters compactos y bien separados ‚Üí DB bajo</li>
                            <li>Clusters dispersos o superpuestos ‚Üí DB alto</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="highlight">
                <h3>üéØ ¬øC√≥mo elegir la m√©trica apropiada?</h3>
                <ul>
                    <li><strong>Silhouette:</strong> Cuando quieres interpretabilidad clara (-1 a 1)</li>
                    <li><strong>Inertia + Elbow:</strong> Para K-means, f√°cil de entender</li>
                    <li><strong>Calinski-Harabasz:</strong> Balance entre velocidad y calidad</li>
                    <li><strong>Combinar m√∫ltiples m√©tricas</strong> para decisi√≥n m√°s robusta</li>
                </ul>
            </div>
        </div>

        <!-- Slide 7: Reducci√≥n de Dimensionalidad -->
        <div class="slide">
            <h1>Reducci√≥n de Dimensionalidad üìâ</h1>
            
            <div class="highlight">
                <h3>üéØ ¬øPor qu√© reducir dimensiones?</h3>
                <div class="two-column">
                    <div>
                        <h4>üìä El problema de alta dimensionalidad:</h4>
                        <ul style="font-size: 0.9em;">
                            <li><strong>Curse of dimensionality:</strong> Datos se vuelven "dispersos"</li>
                            <li><strong>Visualizaci√≥n imposible:</strong> No podemos graficar 100 dimensiones</li>
                            <li><strong>Ruido:</strong> Muchas variables irrelevantes</li>
                            <li><strong>Computational cost:</strong> Procesamiento lento</li>
                        </ul>
                    </div>
                    <div>
                        <h4>‚úÖ Beneficios de reducir dimensiones:</h4>
                        <ul style="font-size: 0.9em;">
                            <li><strong>Visualizaci√≥n:</strong> Graficar en 2D/3D</li>
                            <li><strong>Storage:</strong> Menos espacio de almacenamiento</li>
                            <li><strong>Speed:</strong> Algoritmos m√°s r√°pidos</li>
                            <li><strong>Noise reduction:</strong> Eliminar informaci√≥n irrelevante</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="metrics-grid">
                <div class="metric-card">
                    <h3>PCA (Principal Component Analysis)</h3>
                    <p><strong>¬øQu√© hace?</strong> Encuentra las direcciones de m√°xima varianza en los datos</p>
                    <div class="example">
                        <h4>üîß Proceso:</h4>
                        <ol style="text-align: left; font-size: 0.9em;">
                            <li>Calcular matriz de covarianza</li>
                            <li>Encontrar eigenvectors (componentes principales)</li>
                            <li>Ordenar por eigenvalues (varianza explicada)</li>
                            <li>Proyectar datos en nuevas dimensiones</li>
                        </ol>
                    </div>
                    <div class="success">
                        <strong>‚úÖ Ventajas:</strong> Interpretable, preserva varianza m√°xima
                    </div>
                    <div class="warning">
                        <strong>‚ö†Ô∏è Limitaciones:</strong> Solo relaciones lineales, componentes dif√≠ciles de interpretar
                    </div>
                </div>

                <div class="metric-card">
                    <h3>t-SNE</h3>
                    <p><strong>¬øQu√© hace?</strong> Preserva similitudes locales, excelente para visualizaci√≥n</p>
                    <div class="example">
                        <h4>üéØ Caracter√≠sticas:</h4>
                        <ul style="text-align: left; font-size: 0.9em;">
                            <li>Convierte similitudes en probabilidades</li>
                            <li>Minimiza diferencia entre distribuciones</li>
                            <li>Preserva estructura local (vecinos cercanos)</li>
                        </ul>
                    </div>
                    <div class="success">
                        <strong>‚úÖ Ventajas:</strong> Visualizaciones hermosas, preserva clusters
                    </div>
                    <div class="warning">
                        <strong>‚ö†Ô∏è Limitaciones:</strong> Solo para visualizaci√≥n, no determin√≠stico, lento
                    </div>
                </div>

                <div class="metric-card">
                    <h3>UMAP</h3>
                    <p><strong>¬øQu√© hace?</strong> Similar a t-SNE pero m√°s r√°pido y preserva estructura global</p>
                    <div class="example">
                        <h4>üöÄ Ventajas sobre t-SNE:</h4>
                        <ul style="text-align: left; font-size: 0.9em;">
                            <li>M√°s r√°pido computacionalmente</li>
                            <li>Preserva estructura global Y local</li>
                            <li>M√°s estable entre ejecuciones</li>
                            <li>Mejor para datasets grandes</li>
                        </ul>
                    </div>
                    <div class="success">
                        <strong>üéØ Uso t√≠pico:</strong> Visualizaci√≥n de datasets complejos (genomics, im√°genes, texto)
                    </div>
                </div>
            </div>

            <div class="example">
                <h3>üíº Ejemplo Pr√°ctico: An√°lisis de Productos E-commerce</h3>
                <div class="two-column">
                    <div>
                        <h4>üìä Datos originales (50 dimensiones):</h4>
                        <ul style="font-size: 0.9em;">
                            <li>Caracter√≠sticas del producto (20 vars)</li>
                            <li>Comportamiento de compra (15 vars)</li>
                            <li>Reviews y ratings (10 vars)</li>
                            <li>Datos demogr√°ficos (5 vars)</li>
                        </ul>
                    </div>
                    <div>
                        <h4>üéØ Despu√©s de PCA (5 dimensiones):</h4>
                        <ul style="font-size: 0.9em;">
                            <li>PC1: "Popularidad general" (30% varianza)</li>
                            <li>PC2: "Precio vs calidad" (20% varianza)</li>
                            <li>PC3: "Categor√≠a espec√≠fica" (15% varianza)</li>
                            <li>PC4: "Estacionalidad" (10% varianza)</li>
                            <li>PC5: "Nicho specialty" (8% varianza)</li>
                        </ul>
                    </div>
                </div>
                <div class="success" style="margin-top: 15px;">
                    <strong>üìà Resultado:</strong> 83% de la varianza original preservada con solo 5 dimensiones. Visualizaci√≥n clara de segmentos de productos.
                </div>
            </div>
        </div>

        <!-- Slide 8: Transici√≥n a Supervisado -->
        <div class="slide">
            <h1>Enfoque del Curso: Aprendizaje Supervisado üéØ</h1>
            
            <div class="highlight">
                <h3>üìö Lo que acabamos de ver vs lo que sigue</h3>
                <div class="two-column">
                    <div class="example">
                        <h4>üîç Aprendizaje No Supervisado</h4>
                        <p><strong>Lo que vimos:</strong></p>
                        <ul style="font-size: 0.9em;">
                            <li>Clustering (K-means, Hierarchical, DBSCAN)</li>
                            <li>Reducci√≥n dimensionalidad (PCA, t-SNE, UMAP)</li>
                            <li>M√©tricas internas (Silhouette, Inertia)</li>
                            <li>Detecci√≥n de anomal√≠as</li>
                        </ul>
                        <p><strong>Futuro:</strong> Clase dedicada completa a estos temas</p>
                    </div>
                    <div class="success">
                        <h4>‚úÖ Aprendizaje Supervisado</h4>
                        <p><strong>Resto del curso:</strong></p>
                        <ul style="font-size: 0.9em;">
                            <li><strong>Clase 2:</strong> Modelos de Regresi√≥n</li>
                            <li><strong>Clase 3:</strong> Algoritmos de Clasificaci√≥n</li>
                            <li><strong>Clase 4:</strong> Modelos de Negocio (Churn, LTV)</li>
                            <li><strong>Clase 5:</strong> Pr√°ctica en R</li>
                        </ul>
                        <p><strong>Por qu√© este enfoque:</strong> Aplicaciones directas en business analytics</p>
                    </div>
                </div>
            </div>

            <div class="warning">
                <h3>‚ö†Ô∏è Aclaraci√≥n Importante</h3>
                <p><strong>Todos los conceptos que siguen son espec√≠ficos de aprendizaje supervisado:</strong></p>
                <ul>
                    <li><strong>Train/Validation/Test split</strong> ‚Üí Requiere targets conocidos</li>
                    <li><strong>M√©tricas de evaluaci√≥n</strong> (MSE, Accuracy, F1) ‚Üí Comparan predicci√≥n vs realidad</li>
                    <li><strong>Overfitting/Underfitting</strong> ‚Üí Como los definimos, necesita validation con targets</li>
                    <li><strong>Cross-validation</strong> ‚Üí Eval√∫a predicciones contra targets conocidos</li>
                </ul>
            </div>

            <div class="metrics-grid">
                <div class="metric-card">
                    <h3>üíº ¬øPor qu√© enfocarnos en Supervisado?</h3>
                    <ul style="text-align: left; font-size: 0.9em;">
                        <li><strong>Business value directo:</strong> Predicciones accionables</li>
                        <li><strong>Casos de uso comunes:</strong> Ventas, churn, pricing</li>
                        <li><strong>ROI medible:</strong> Impacto cuantificable en m√©tricas de negocio</li>
                        <li><strong>Base s√≥lida:</strong> Conceptos transferibles a otros tipos</li>
                    </ul>
                </div>

                <div class="metric-card">
                    <h3>üîÑ ¬øCu√°ndo usar cada tipo?</h3>
                    <div class="example">
                        <p><strong>Usa No Supervisado cuando:</strong></p>
                        <ul style="text-align: left; font-size: 0.9em;">
                            <li>Explorar datos nuevos</li>
                            <li>No hay targets claros</li>
                            <li>Buscas insights exploratorios</li>
                        </ul>
                        <p><strong>Usa Supervisado cuando:</strong></p>
                        <ul style="text-align: left; font-size: 0.9em;">
                            <li>Hay ejemplos hist√≥ricos</li>
                            <li>Objetivo predictivo claro</li>
                            <li>Necesitas decisiones automatizadas</li>
                        </ul>
                    </div>
                </div>

                <div class="metric-card">
                    <h3>üîó Combinaci√≥n en la pr√°ctica</h3>
                    <p><strong>Pipeline t√≠pico:</strong></p>
                    <div class="code-block">
1. EDA + No supervisado<br>
   (entender datos)<br>
2. Feature engineering<br>
3. Supervisado<br>
   (modelo predictivo)<br>
4. Deployment y monitoreo
                    </div>
                    <p style="font-size: 0.9em;"><strong>Ejemplo:</strong> Clustering para segmentar clientes, luego modelos supervisados para predecir churn en cada segmento</p>
                </div>
            </div>
        </div>

        <!-- Slide 9: El Proceso de ML (ajustado) -->
        <div class="slide">
            <h1>El Proceso de Machine Learning (Supervisado) üîÑ</h1>
            
            <div class="warning">
                <h3>üéØ Nota: Proceso espec√≠fico para Aprendizaje Supervisado</h3>
                <p>Los siguientes pasos asumen que tenemos <strong>targets/etiquetas conocidas</strong>. Para no supervisado, el proceso ser√≠a diferente (sin divisi√≥n test/validation, m√©tricas internas, etc.)</p>
            </div>
            
            <div style="display: flex; flex-direction: column; gap: 20px;">
                <div class="example">
                    <h3>1Ô∏è‚É£ Definici√≥n del Problema</h3>
                    <ul>
                        <li>¬øQu√© queremos predecir? (Variable objetivo/target)</li>
                        <li>¬øEs un problema de regresi√≥n o clasificaci√≥n?</li>
                        <li>¬øQu√© datos necesitamos y est√°n disponibles?</li>
                        <li>¬øC√≥mo se medir√° el √©xito del modelo?</li>
                    </ul>
                </div>

                <div class="example">
                    <h3>2Ô∏è‚É£ Recolecci√≥n y Preparaci√≥n de Datos</h3>
                    <ul>
                        <li><strong>Recolecci√≥n:</strong> Obtener datos relevantes y suficientes (features + targets)</li>
                        <li><strong>Limpieza:</strong> Manejar valores faltantes, outliers, errores</li>
                        <li><strong>Transformaci√≥n:</strong> Normalizaci√≥n, encoding de variables categ√≥ricas</li>
                        <li><strong>Feature Engineering:</strong> Crear variables m√°s informativas</li>
                    </ul>
                </div>

                <div class="example">
                    <h3>3Ô∏è‚É£ Exploraci√≥n de Datos (EDA)</h3>
                    <ul>
                        <li>Entender la distribuci√≥n de features y target</li>
                        <li>Identificar correlaciones entre features y target</li>
                        <li>Detectar patrones, anomal√≠as y visualizar relaciones</li>
                        <li>Evaluar calidad y completitud de los targets</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Slide 10: Proceso ML (continuaci√≥n) -->
        <div class="slide">
            <h1>El Proceso de ML (continuaci√≥n) üîÑ</h1>
            
            <div style="display: flex; flex-direction: column; gap: 20px;">
                <div class="example">
                    <h3>4Ô∏è‚É£ Selecci√≥n y Entrenamiento del Modelo</h3>
                    <div class="two-column">
                        <div>
                            <h4>Selecci√≥n del Algoritmo:</h4>
                            <ul>
                                <li><strong>Tipo de problema:</strong> Regresi√≥n vs Clasificaci√≥n</li>
                                <li><strong>Tama√±o del dataset:</strong> Peque√±o, mediano, grande</li>
                                <li><strong>Interpretabilidad vs Performance</strong></li>
                            </ul>
                        </div>
                        <div>
                            <h4>Divisi√≥n de Datos:</h4>
                            <div class="code-block">
Training Set (70-80%)
‚Üì
Validation Set (10-15%)
‚Üì
Test Set (10-15%)
                            </div>
                        </div>
                    </div>
                </div>

                <div class="example">
                    <h3>5Ô∏è‚É£ Evaluaci√≥n y Validaci√≥n</h3>
                    <p><strong>Proceso detallado:</strong></p>
                    <ul>
                        <li><strong>Entrenar modelo</strong> en training set</li>
                        <li><strong>Evaluar rendimiento</strong> en validation set</li>
                        <li><strong>Ajustar hiperpar√°metros</strong> basado en validation</li>
                        <li><strong>Repetir</strong> hasta encontrar mejor configuraci√≥n</li>
                        <li><strong>Evaluaci√≥n final</strong> en test set (solo una vez)</li>
                    </ul>
                    
                    <h4>M√©tricas por tipo de problema:</h4>
                    <div class="two-column">
                        <div>
                            <p><strong>Regresi√≥n:</strong></p>
                            <ul style="font-size: 0.9em;">
                                <li>MSE: Penaliza errores grandes</li>
                                <li>RMSE: Interpretable, mismas unidades</li>
                                <li>MAE: Robusta a outliers</li>
                                <li>R¬≤: Para stakeholders (% explicado)</li>
                            </ul>
                        </div>
                        <div>
                            <p><strong>Clasificaci√≥n:</strong></p>
                            <ul style="font-size: 0.9em;">
                                <li>Accuracy: % predicciones correctas</li>
                                <li>Precision: Evitar falsos positivos</li>
                                <li>Recall: Evitar falsos negativos</li>
                                <li>F1: Balance precision-recall</li>
                            </ul>
                        </div>
                    </div>
                    
                    <div class="warning" style="margin-top: 15px;">
                        <strong>‚ö†Ô∏è Detecci√≥n de problemas:</strong>
                        <ul>
                            <li><strong>Overfitting:</strong> Alto train, bajo validation</li>
                            <li><strong>Underfitting:</strong> Ambos bajos</li>
                            <li><strong>Data leakage:</strong> Resultados "demasiado buenos"</li>
                        </ul>
                    </div>
                </div>

                <div class="example">
                    <h3>6Ô∏è‚É£ Implementaci√≥n y Monitoreo</h3>
                    <ul>
                        <li>Poner el modelo en producci√≥n</li>
                        <li>Monitorear el rendimiento continuo</li>
                        <li>Reentrenar cuando sea necesario</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Slide 11: Overfitting vs Underfitting -->
        <div class="slide">
            <h1>Overfitting vs Underfitting üéØ</h1>
            
            <div class="two-column">
                <div class="warning">
                    <h3>üî¥ Overfitting (Sobreajuste)</h3>
                    <p><strong>¬øQu√© es?</strong> El modelo "memoriza" los datos de entrenamiento pero no generaliza bien</p>
                    
                    <h4>üß† Analog√≠as:</h4>
                    <ul style="font-size: 0.9em;">
                        <li><strong>Estudiante:</strong> Memoriza respuestas del examen de pr√°ctica, falla en el examen real</li>
                        <li><strong>GPS:</strong> Conoce perfectamente CABA pero se pierde en C√≥rdoba</li>
                        <li><strong>Chef:</strong> Cocina perfectamente UN plato pero no puede adaptar la t√©cnica</li>
                    </ul>
                    
                    <h4>üìä S√≠ntomas:</h4>
                    <ul style="font-size: 0.9em;">
                        <li>Training accuracy: 98%</li>
                        <li>Validation accuracy: 65%</li>
                        <li><strong>Gap grande</strong> entre ambos</li>
                        <li>Mejora en training pero empeora en validation</li>
                    </ul>
                    
                    <h4>üîß Soluciones:</h4>
                    <ul style="font-size: 0.9em;">
                        <li>Regularizaci√≥n (Ridge, Lasso)</li>
                        <li>Early stopping</li>
                        <li>M√°s datos de entrenamiento</li>
                        <li>Feature selection</li>
                        <li>Cross-validation</li>
                    </ul>
                </div>

                <div class="success">
                    <h3>üîµ Underfitting (Subajuste)</h3>
                    <p><strong>¬øQu√© es?</strong> El modelo es demasiado simple y no captura los patrones importantes</p>
                    
                    <h4>üß† Analog√≠as:</h4>
                    <ul style="font-size: 0.9em;">
                        <li><strong>Estudiante:</strong> No estudi√≥, no entiende conceptos b√°sicos</li>
                        <li><strong>Mapa b√°sico:</strong> Solo muestra pa√≠ses, necesitamos direcciones espec√≠ficas</li>
                        <li><strong>Receta general:</strong> "Cocinar hasta que est√© listo" no sirve para souffl√©</li>
                    </ul>
                    
                    <h4>üìä S√≠ntomas:</h4>
                    <ul style="font-size: 0.9em;">
                        <li>Training accuracy: 60%</li>
                        <li>Validation accuracy: 58%</li>
                        <li><strong>Ambos valores bajos</strong> y similares</li>
                        <li>Learning curves convergen bajo</li>
                    </ul>
                    
                    <h4>üîß Soluciones:</h4>
                    <ul style="font-size: 0.9em;">
                        <li>Modelo m√°s complejo</li>
                        <li>Feature engineering</li>
                        <li>M√°s tiempo entrenamiento</li>
                        <li>Reducir regularizaci√≥n</li>
                    </ul>
                </div>
            </div>

            <div class="highlight">
                <h3>üéØ El Objetivo: Encontrar el Balance √ìptimo</h3>
                <p><strong>Bias-Variance Tradeoff:</strong></p>
                <ul>
                    <li><strong>Bias (Sesgo):</strong> Error por simplificaciones del modelo ‚Üí Underfitting</li>
                    <li><strong>Variance (Varianza):</strong> Error por sensibilidad a cambios en datos ‚Üí Overfitting</li>
                    <li><strong>Objetivo:</strong> Minimizar bias + variance + ruido</li>
                    <li><strong>Herramientas:</strong> Learning curves, validation curves, cross-validation</li>
                </ul>
            </div>
        </div>

        <!-- Slide 12: Divisi√≥n de Datos -->
        <div class="slide">
            <h1>Divisi√≥n de Datos üìä</h1>
            
            <div class="highlight">
                <h3>üéì Analog√≠a del Examen</h3>
                <ul>
                    <li><strong>Training set:</strong> Material de estudio üìö</li>
                    <li><strong>Validation set:</strong> Examen de pr√°ctica (para saber c√≥mo vamos) üìù</li>
                    <li><strong>Test set:</strong> Examen final (evaluaci√≥n definitiva) üéØ</li>
                </ul>
                <p style="margin-top: 15px;"><em>Si us√°ramos el mismo material para estudiar y para el examen final, ¬°no sabr√≠amos si realmente aprendimos!</em></p>
            </div>

            <div class="metrics-grid">
                <div class="metric-card">
                    <h3>Training Set (70-80%)</h3>
                    <p><strong>Prop√≥sito:</strong> El modelo aprende patrones aqu√≠</p>
                    <p><strong>Regla:</strong> Mientras m√°s datos, mejor el aprendizaje</p>
                </div>

                <div class="metric-card">
                    <h3>Validation Set (10-15%)</h3>
                    <p><strong>Prop√≥sito:</strong> Evaluaci√≥n durante desarrollo, ajuste de hiperpar√°metros</p>
                    <p><strong>Simula:</strong> Datos "no vistos" durante entrenamiento</p>
                </div>

                <div class="metric-card">
                    <h3>Test Set (10-15%)</h3>
                    <p><strong>Prop√≥sito:</strong> Evaluaci√≥n final, estimaci√≥n de rendimiento en producci√≥n</p>
                    <p><strong>‚ö†Ô∏è REGLA DE ORO:</strong> ¬°NUNCA tocar hasta el final!</p>
                </div>
            </div>

            <div class="example">
                <h3>üìà Estrategias Especiales de Divisi√≥n</h3>
                <div class="two-column">
                    <div>
                        <h4>üìÖ Divisi√≥n Temporal:</h4>
                        <div class="code-block">
2020    2021    2022    2023    2024<br>
|----Train----|--Val--|--Test--|
                        </div>
                        <p><strong>Crucial:</strong> Entrenar con pasado, predecir futuro. Evita data leakage.</p>
                        
                        <h4>üë• Divisi√≥n Estratificada:</h4>
                        <p><strong>Problema:</strong> 90% clase A, 10% clase B</p>
                        <p><strong>Soluci√≥n:</strong> Mantener proporci√≥n en todos los sets</p>
                    </div>
                    <div>
                        <h4>üè• Divisi√≥n por Grupos:</h4>
                        <p><strong>Ejemplo:</strong> Datos de pacientes de m√∫ltiples hospitales</p>
                        <p><strong>Estrategia:</strong></p>
                        <div class="code-block">
Train: Hospitales 1-8<br>
Validation: Hospital 9<br>
Test: Hospital 10
                        </div>
                        <p><strong>Objetivo:</strong> Evaluar generalizaci√≥n a nuevas instituciones</p>
                        
                        <h4>‚ö†Ô∏è Consideraciones importantes:</h4>
                        <ul style="font-size: 0.9em;">
                            <li>Datos representativos del problema completo</li>
                            <li>Aleatoriedad en la divisi√≥n</li>
                            <li>Evitar data leakage temporal</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 13: Validaci√≥n Cruzada -->
        <div class="slide">
            <h1>Validaci√≥n Cruzada (Aprendizaje Supervisado) üîÑ</h1>
            
            <div class="highlight">
                <h3>ü§î ¬øPor qu√© una sola divisi√≥n no es suficiente?</h3>
                <p><strong>Problema de la "suerte":</strong> Una divisi√≥n espec√≠fica puede ser "f√°cil" o "dif√≠cil". Los resultados pueden variar significativamente.</p>
                <p><strong>Soluci√≥n:</strong> Probar m√∫ltiples divisiones y promediar resultados.</p>
                <div class="warning" style="margin-top: 15px;">
                    <strong>‚ö†Ô∏è Nota:</strong> Cross-validation es espec√≠fica de supervisado porque necesita targets para evaluar el modelo en cada fold.
                </div>
            </div>

            <div class="example">
                <h3>üìã K-Fold Cross Validation (K=5)</h3>
                <div class="code-block">
Iteraci√≥n 1: [Test][Train][Train][Train][Train] ‚Üí Score‚ÇÅ<br>
Iteraci√≥n 2: [Train][Test][Train][Train][Train] ‚Üí Score‚ÇÇ<br>
Iteraci√≥n 3: [Train][Train][Test][Train][Train] ‚Üí Score‚ÇÉ<br>
Iteraci√≥n 4: [Train][Train][Train][Test][Train] ‚Üí Score‚ÇÑ<br>
Iteraci√≥n 5: [Train][Train][Train][Train][Test] ‚Üí Score‚ÇÖ<br>
<br>
Resultado final: Œº = (Score‚ÇÅ + ... + Score‚ÇÖ)/5<br>
Confianza: œÉ = desviaci√≥n_est√°ndar(Score‚ÇÅ, ..., Score‚ÇÖ)
                </div>
            </div>

            <div class="two-column">
                <div class="success">
                    <h4>‚úÖ Modelo Consistente</h4>
                    <ul>
                        <li>Score promedio: 0.85</li>
                        <li>Desviaci√≥n est√°ndar: 0.02</li>
                        <li><strong>Interpretaci√≥n:</strong> Modelo predecible y confiable</li>
                    </ul>
                </div>
                <div class="warning">
                    <h4>‚ö†Ô∏è Modelo Inconsistente</h4>
                    <ul>
                        <li>Score promedio: 0.85</li>
                        <li>Desviaci√≥n est√°ndar: 0.15</li>
                        <li><strong>Interpretaci√≥n:</strong> Revisar modelo o datos</li>
                    </ul>
                </div>
            </div>
            
            <div class="highlight">
                <h3>üîÑ Variantes de Cross-Validation</h3>
                <div class="two-column">
                    <div>
                        <h4>Stratified K-Fold:</h4>
                        <ul style="font-size: 0.9em;">
                            <li>Mantiene proporci√≥n de clases</li>
                            <li><strong>Cr√≠tico</strong> en problemas desbalanceados</li>
                            <li>Si 20% son positivos ‚Üí cada fold ~20%</li>
                        </ul>
                        
                        <h4>Leave-One-Out (LOO):</h4>
                        <ul style="font-size: 0.9em;">
                            <li>K = n√∫mero de registros</li>
                            <li>M√°ximo uso de datos</li>
                            <li>Computacionalmente costoso</li>
                        </ul>
                    </div>
                    <div>
                        <h4>Time Series CV:</h4>
                        <div class="code-block" style="font-size: 0.8em;">
Fold 1: Train[1:100] ‚Üí Test[101:120]<br>
Fold 2: Train[1:120] ‚Üí Test[121:140]<br>
Fold 3: Train[1:140] ‚Üí Test[141:160]
                        </div>
                        <ul style="font-size: 0.9em;">
                            <li>Respeta orden temporal</li>
                            <li><strong>Esencial</strong> para series temporales</li>
                            <li>Evita mirar al futuro</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 14: Feature Engineering -->
        <div class="slide">
            <h1>Feature Engineering (Supervisado) üîß</h1>
            
            <div class="highlight">
                <h3>üéØ ¬øPor qu√© es crucial en aprendizaje supervisado?</h3>
                <ul>
                    <li><strong>"Garbage in, garbage out":</strong> Los mejores algoritmos supervisados fallan con features pobres</li>
                    <li><strong>"Features determine performance ceiling":</strong> Las variables definen qu√© tan bien PUEDE predecir el target</li>
                    <li><strong>Relaci√≥n con target:</strong> En supervisado, creamos features que capturen mejor la relaci√≥n con la variable objetivo</li>
                </ul>
            </div>

            <div class="example">
                <h3>üí° Ejemplo Pr√°ctico Detallado - E-commerce</h3>
                <div class="two-column">
                    <div>
                        <h4>Variables Originales:</h4>
                        <ul style="font-size: 0.9em;">
                            <li>cantidad_compras: 5</li>
                            <li>monto_total: $50,000</li>
                            <li>dias_como_cliente: 365</li>
                            <li>ultima_compra: 2024-05-15</li>
                            <li>categoria_favorita: "Electr√≥nicos"</li>
                            <li>edad: 28</li>
                        </ul>
                    </div>
                    <div>
                        <h4>Variables Creadas (Feature Engineering):</h4>
                        <ul style="font-size: 0.9em;">
                            <li>monto_promedio_por_compra: $10,000</li>
                            <li>compras_por_mes: 1.64</li>
                            <li>cliente_alta_valor: True (>$5K promedio)</li>
                            <li>dias_desde_ultima_compra: 22</li>
                            <li>estacionalidad_compras: "Alta en feriados"</li>
                            <li>edad_al_cuadrado: 784 (relaci√≥n no-lineal)</li>
                            <li>ratio_gasto_edad: $1,786 por a√±o de edad</li>
                        </ul>
                    </div>
                </div>
                <div class="success" style="margin-top: 15px;">
                    <strong>üí° Resultado:</strong> Las variables creadas capturan patrones m√°s ricos que las originales y mejoran significativamente el rendimiento del modelo.
                </div>
            </div>

            <div class="metrics-grid">
                <div class="metric-card">
                    <h3>üî¢ Transformaciones Matem√°ticas</h3>
                    <p><strong>Logar√≠tmica:</strong> Para variables sesgadas</p>
                    <div class="code-block" style="font-size: 0.8em;">
log_income = log(income + 1)<br>
Efecto: log(1000)=3, log(10000)=4<br>
Comprime valores extremos
                    </div>
                    <p><strong>Polinomial:</strong> Relaciones no-lineales</p>
                    <div class="code-block" style="font-size: 0.8em;">
edad¬≤ para productividad<br>
(primero sube, luego baja)
                    </div>
                    <p><strong>Binning:</strong> Edad ‚Üí Grupos etarios</p>
                    <div class="code-block" style="font-size: 0.8em;">
18-25, 26-35, 36-50, 50+
                    </div>
                </div>

                <div class="metric-card">
                    <h3>üìù Encoding de Variables Categ√≥ricas</h3>
                    <p><strong>One-Hot Encoding:</strong></p>
                    <div class="code-block" style="font-size: 0.8em;">
Color: [Rojo, Verde, Azul]<br>
‚Üì<br>
Color_Rojo: [1, 0, 0]<br>
Color_Verde: [0, 1, 0]<br>
Color_Azul: [0, 0, 1]
                    </div>
                    <p><strong>Ordinal Encoding:</strong></p>
                    <div class="code-block" style="font-size: 0.8em;">
Educaci√≥n: [Primaria, Secundaria,<br>
Universidad, Postgrado]<br>
Encoded: [1, 2, 3, 4]
                    </div>
                    <p><strong>‚ö†Ô∏è Target Encoding:</strong> Relaci√≥n directa con objetivo, pero propenso a overfitting</p>
                </div>

                <div class="metric-card">
                    <h3>‚öñÔ∏è Escalado y Normalizaci√≥n</h3>
                    <p><strong>¬øPor qu√© escalar?</strong></p>
                    <div class="code-block" style="font-size: 0.8em;">
Sin escalar:<br>
Edad: 25-35 (rango: 10)<br>
Salario: 50K-70K (rango: 20K)
                    </div>
                    <p style="font-size: 0.9em;">Algoritmos pueden dar m√°s importancia al salario por n√∫meros m√°s grandes</p>
                    
                    <p><strong>Standardizaci√≥n (Z-Score):</strong></p>
                    <div class="code-block" style="font-size: 0.8em;">
z = (x - Œº) / œÉ<br>
Resultado: Media=0, œÉ=1
                    </div>
                    
                    <p><strong>Min-Max Normalization:</strong></p>
                    <div class="code-block" style="font-size: 0.8em;">
normalized = (x - min) / (max - min)<br>
Resultado: Valores entre 0 y 1
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 10: M√©tricas de Regresi√≥n -->
        <div class="slide">
            <h1>M√©tricas de Evaluaci√≥n - Regresi√≥n üìä</h1>
            
            <div class="highlight">
                <h3>üéØ Principio Fundamental</h3>
                <p><strong>"You optimize what you measure":</strong> El modelo intentar√° maximizar la m√©trica que elijas, as√≠ que hay que elegir bien</p>
            </div>

            <div class="metrics-grid">
                <div class="metric-card">
                    <h3>MSE (Mean Squared Error)</h3>
                    <div class="code-block">MSE = Œ£(real - predicci√≥n)¬≤ / n</div>
                    <p><strong>Cu√°ndo usar:</strong> Cuando errores grandes son especialmente problem√°ticos</p>
                    <p><strong>Ejemplo:</strong> Predecir precios de casas</p>
                    <div class="code-block" style="font-size: 0.8em;">
Casa 1: Error=10K ‚Üí Contribuci√≥n=100M<br>
Casa 2: Error=20K ‚Üí Contribuci√≥n=400M
                    </div>
                    <p><strong>Problema:</strong> Dif√≠cil de interpretar (unidades al cuadrado)</p>
                </div>

                <div class="metric-card">
                    <h3>RMSE (Root MSE)</h3>
                    <div class="code-block">RMSE = ‚àöMSE</div>
                    <p><strong>Ventaja:</strong> Mismas unidades que variable original</p>
                    <p><strong>Interpretaci√≥n:</strong> "En promedio, nos equivocamos por X unidades"</p>
                    <p><strong>Sigue penalizando errores grandes</strong>, pero m√°s interpretable que MSE</p>
                </div>

                <div class="metric-card">
                    <h3>MAE (Mean Absolute Error)</h3>
                    <div class="code-block">MAE = Œ£|real - predicci√≥n| / n</div>
                    <p><strong>Caracter√≠stica:</strong> Todos los errores importan igual</p>
                    <p><strong>Ventaja:</strong> M√°s robusta a outliers que MSE/RMSE</p>
                    <p><strong>Cu√°ndo usar:</strong> Cuando outliers no deben dominar la m√©trica</p>
                </div>

                <div class="metric-card">
                    <h3>R¬≤ (Coef. Determinaci√≥n)</h3>
                    <div class="code-block">
R¬≤ = 1 - (SS_residual / SS_total)<br>
SS_residual = Œ£(real - predicci√≥n)¬≤<br>
SS_total = Œ£(real - media)¬≤
                    </div>
                    <p><strong>Interpretaci√≥n:</strong></p>
                    <ul style="text-align: left; font-size: 0.9em;">
                        <li>R¬≤ = 1: Modelo perfecto</li>
                        <li>R¬≤ = 0: Tan bueno como predecir la media</li>
                        <li>R¬≤ < 0: Peor que predecir la media</li>
                    </ul>
                    <p><strong>Ejemplo:</strong> R¬≤ = 0.85 ‚Üí "Explica 85% de variabilidad"</p>
                </div>
            </div>

            <div class="example">
                <h3>üè† ¬øCu√°l m√©trica usar cu√°ndo?</h3>
                <div class="two-column">
                    <div>
                        <ul>
                            <li><strong>Para stakeholders:</strong> R¬≤ (f√°cil de entender)</li>
                            <li><strong>Optimizar modelo sensible a outliers:</strong> MAE</li>
                        </ul>
                    </div>
                    <div>
                        <ul>
                            <li><strong>Errores grandes importan m√°s:</strong> RMSE</li>
                            <li><strong>Comparar modelos:</strong> Cualquiera, pero ser consistente</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 11: M√©tricas de Clasificaci√≥n -->
        <div class="slide">
            <h1>M√©tricas de Evaluaci√≥n - Clasificaci√≥n üéØ</h1>
            
            <div class="example">
                <h3>üìã Matriz de Confusi√≥n - La Base de Todo</h3>
                <div class="code-block">
                Predicci√≥n<br>
              Pos    Neg<br>
Real    Pos   TP    FN    <br>
        Neg   FP    TN    
                </div>
                <ul>
                    <li><strong>TP (True Positives):</strong> Predijimos positivo y era positivo ‚úì</li>
                    <li><strong>TN (True Negatives):</strong> Predijimos negativo y era negativo ‚úì</li>
                    <li><strong>FP (False Positives):</strong> Predijimos positivo pero era negativo ‚úó (Error Tipo I)</li>
                    <li><strong>FN (False Negatives):</strong> Predijimos negativo pero era positivo ‚úó (Error Tipo II)</li>
                </ul>
            </div>

            <div class="two-column">
                <div class="metric-card">
                    <h3>Precision (Precisi√≥n)</h3>
                    <div class="code-block">Precision = TP / (TP + FP)</div>
                    <p><strong>Pregunta:</strong> "De los que predije como positivos, ¬øcu√°ntos realmente lo eran?"</p>
                    <p><strong>Cu√°ndo importa:</strong> Falsos positivos son costosos</p>
                    <div class="example">
                        <strong>Ejemplos:</strong>
                        <ul style="text-align: left; font-size: 0.9em;">
                            <li><strong>Diagn√≥stico m√©dico:</strong> No asustar pacientes sanos</li>
                            <li><strong>Marketing:</strong> No gastar en clientes que no comprar√°n</li>
                            <li><strong>Spam detection:</strong> No marcar emails importantes</li>
                        </ul>
                    </div>
                </div>

                <div class="metric-card">
                    <h3>Recall (Sensibilidad)</h3>
                    <div class="code-block">Recall = TP / (TP + FN)</div>
                    <p><strong>Pregunta:</strong> "De todos los positivos reales, ¬øcu√°ntos detect√©?"</p>
                    <p><strong>Cu√°ndo importa:</strong> Falsos negativos son costosos</p>
                    <div class="example">
                        <strong>Ejemplos:</strong>
                        <ul style="text-align: left; font-size: 0.9em;">
                            <li><strong>Detecci√≥n de fraude:</strong> No perder transacciones fraudulentas</li>
                            <li><strong>Diagn√≥stico de c√°ncer:</strong> No perder casos positivos</li>
                            <li><strong>Seguridad:</strong> No dejar pasar amenazas</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="highlight">
                <h3>‚öñÔ∏è Precision vs Recall Tradeoff</h3>
                <ul>
                    <li><strong>Alta Precision, Bajo Recall:</strong> Modelo conservador (solo predice positivo cuando est√° MUY seguro)</li>
                    <li><strong>Baja Precision, Alto Recall:</strong> Modelo liberal (predice positivo ante cualquier duda)</li>
                </ul>
            </div>
            
            <div class="success">
                <h3>üîÑ F1-Score: El Balance Perfecto</h3>
                <div class="code-block">
F1 = 2 * (Precision * Recall) / (Precision + Recall)
                </div>
                <p><strong>¬øQu√© mide?</strong> Media arm√≥nica de precision y recall</p>
                <p><strong>¬øPor qu√© media arm√≥nica?</strong> Penaliza modelos extremos m√°s que la media simple</p>
                
                <div class="example">
                    <h4>üìä Ejemplo Comparativo:</h4>
                    <div class="two-column">
                        <div>
                            <p><strong>Modelo A:</strong></p>
                            <ul style="font-size: 0.9em;">
                                <li>Precision: 0.9, Recall: 0.1</li>
                                <li>Media simple: 0.5</li>
                                <li><strong>F1-Score: 0.18</strong> ‚ö†Ô∏è</li>
                            </ul>
                        </div>
                        <div>
                            <p><strong>Modelo B:</strong></p>
                            <ul style="font-size: 0.9em;">
                                <li>Precision: 0.7, Recall: 0.7</li>
                                <li>Media simple: 0.7</li>
                                <li><strong>F1-Score: 0.7</strong> ‚úÖ</li>
                            </ul>
                        </div>
                    </div>
                    <p><strong>Conclusi√≥n:</strong> F1 prefiere modelos balanceados sobre extremos</p>
                </div>
                
                <p><strong>Cu√°ndo usar F1:</strong> Cuando necesitas una sola m√©trica que balance precision y recall, especialmente con clases desbalanceadas</p>
            </div>
        </div>

        <!-- Slide 12: Sesgos en los Datos -->
        <div class="slide">
            <h1>Sesgos en los Datos ‚ö†Ô∏è</h1>
            
            <div class="warning">
                <h3>üö® ¬øPor qu√© son cr√≠ticos los sesgos?</h3>
                <p><strong>Principio fundamental:</strong> Los modelos de ML amplifican los patrones que encuentran en los datos. Si los datos tienen sesgos, el modelo los amplificar√° y automatizar√° a escala masiva.</p>
            </div>

            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
                <div class="example">
                    <h3>üìç Sesgo de Selecci√≥n</h3>
                    <p><strong>Problema:</strong> Datos no representativos de la poblaci√≥n objetivo</p>
                    <h4>Ejemplos Detallados:</h4>
                    <ul style="font-size: 0.9em;">
                        <li><strong>Geogr√°fico:</strong> Credit scoring solo con datos de CABA ‚Üí No funciona en el interior</li>
                        <li><strong>Temporal:</strong> Modelo de ventas con datos pre-pandemia ‚Üí Falla post-pandemia</li>
                        <li><strong>Supervivencia:</strong> Solo datos de clientes activos ‚Üí No puede predecir churn</li>
                        <li><strong>Auto-selecci√≥n:</strong> Encuestas online ‚Üí Solo responden muy motivados</li>
                    </ul>
                    <div class="warning" style="margin-top: 10px;">
                        <strong>‚ö†Ô∏è Impacto:</strong> Modelo funciona bien en desarrollo pero falla en producci√≥n
                    </div>
                </div>

                <div class="example">
                    <h3>üîç Sesgo de Confirmaci√≥n</h3>
                    <p><strong>Problema:</strong> Buscar evidencia que confirme nuestras creencias</p>
                    <h4>En ML ocurre durante:</h4>
                    <ul style="font-size: 0.9em;">
                        <li><strong>Recolecci√≥n:</strong> Solo buscar datos que confirmen hip√≥tesis</li>
                        <li><strong>Feature selection:</strong> Solo incluir variables que "sabemos" importantes</li>
                        <li><strong>Interpretaci√≥n:</strong> Enfocarse en casos exitosos, ignorar errores</li>
                        <li><strong>Validaci√≥n:</strong> Elegir m√©tricas que favorezcan nuestro modelo</li>
                    </ul>
                    <div class="success" style="margin-top: 10px;">
                        <strong>‚úÖ Soluci√≥n:</strong> Equipos diversos, exploraci√≥n exhaustiva, an√°lisis de errores sistem√°tico
                    </div>
                </div>

                <div class="example">
                    <h3>üèõÔ∏è Sesgo Hist√≥rico</h3>
                    <p><strong>Problema:</strong> Perpetuar discriminaciones y patrones problem√°ticos del pasado</p>
                    <h4>Casos Reales Cr√≠ticos:</h4>
                    <ul style="font-size: 0.9em;">
                        <li><strong>Contrataci√≥n:</strong> Historial sesgado contra mujeres en tech ‚Üí Algoritmo las rechaza sistem√°ticamente</li>
                        <li><strong>Justicia penal:</strong> Vigilancia hist√≥rica desigual ‚Üí Predicciones sesgadas de reincidencia</li>
                        <li><strong>Pr√©stamos:</strong> Exclusi√≥n financiera hist√≥rica ‚Üí Perpet√∫a falta de acceso al cr√©dito</li>
                        <li><strong>Salud:</strong> Investigaci√≥n m√©dica hist√≥ricamente en hombres ‚Üí Diagn√≥sticos menos precisos en mujeres</li>
                    </ul>
                    <div class="warning" style="margin-top: 10px;">
                        <strong>üö® C√≠rculo vicioso:</strong> Discriminaci√≥n ‚Üí Datos sesgados ‚Üí Modelo sesgado ‚Üí M√°s discriminaci√≥n
                    </div>
                </div>

                <div class="example">
                    <h3>üìè Sesgo de Medici√≥n</h3>
                    <p><strong>Problema:</strong> Variables proxy que ocultan discriminaci√≥n</p>
                    <h4>Ejemplos Sutiles:</h4>
                    <ul style="font-size: 0.9em;">
                        <li><strong>C√≥digo postal:</strong> Proxy de raza/etnia sin mencionarla directamente</li>
                        <li><strong>Nombre:</strong> Algoritmos infieren origen √©tnico y discriminan</li>
                        <li><strong>Historial crediticio:</strong> Refleja desigualdades socioecon√≥micas pasadas</li>
                        <li><strong>Etiquetado humano:</strong> Radiograf√≠as evaluadas con sesgos de g√©nero</li>
                    </ul>
                    <div class="warning" style="margin-top: 10px;">
                        <strong>‚ö†Ô∏è Peligro:</strong> Discriminaci√≥n con apariencia de neutralidad t√©cnica
                    </div>
                </div>
            </div>
        </div>
    	
	<!-- Slide 20: Bibliograf√≠a Recomendada -->
        <div class="slide">
            <h1>üìö Bibliograf√≠a Recomendada</h1>
            
            <div class="metrics-grid" style="grid-template-columns: 1fr 1fr;">
                <div class="metric-card">
                    <span class="emoji">üêç</span>
                    <h3>Libros Python - Pr√°cticos</h3>
                    
                    <div class="example">
                        <h4>üìñ "Hands-On Machine Learning" - Aur√©lien G√©ron</h4>
                        <p style="font-size: 0.9em;"><strong>üéØ Perfecto para el curso:</strong> Scikit-learn, regresi√≥n, clasificaci√≥n, clustering</p>
                        <p style="font-size: 0.9em;"><strong>Nivel:</strong> Intermedio | <strong>C√≥digo:</strong> Jupyter notebooks completos</p>
                    </div>

                    <div class="example">
                        <h4>üìñ "Python Machine Learning" - Sebastian Raschka</h4>
                        <p style="font-size: 0.9em;"><strong>üéØ Enfoque:</strong> Teor√≠a + implementaci√≥n pr√°ctica</p>
                        <p style="font-size: 0.9em;"><strong>Nivel:</strong> Intermedio-Avanzado | <strong>Fortaleza:</strong> Algoritmos desde cero</p>
                    </div>

                    <div class="example">
                        <h4>üìñ "Introduction to Machine Learning with Python" - Andreas M√ºller</h4>
                        <p style="font-size: 0.9em;"><strong>üéØ Perfecto para principiantes:</strong> Co-autor de scikit-learn</p>
                        <p style="font-size: 0.9em;"><strong>Nivel:</strong> Principiante-Intermedio | <strong>Enfoque:</strong> Pr√°ctico y aplicado</p>
                    </div>

                    <div class="success">
                        <h4>üõ†Ô∏è Bibliotecas clave:</h4>
                        <ul style="text-align: left; font-size: 0.9em;">
                            <li><strong>scikit-learn:</strong> Regresi√≥n, clasificaci√≥n, clustering</li>
                            <li><strong>pandas:</strong> Manipulaci√≥n de datos</li>
                            <li><strong>matplotlib/seaborn:</strong> Visualizaci√≥n</li>
                            <li><strong>XGBoost:</strong> Modelos avanzados</li>
                        </ul>
                    </div>
                </div>

                <div class="metric-card">
                    <span class="emoji">üìä</span>
                    <h3>Libros R - Pr√°cticos</h3>
                    
                    <div class="example">
                        <h4>üìñ "An Introduction to Statistical Learning" - James, Witten, Hastie, Tibshirani</h4>
                        <p style="font-size: 0.9em;"><strong>üéØ EL cl√°sico:</strong> Perfecto balance teor√≠a-pr√°ctica</p>
                        <p style="font-size: 0.9em;"><strong>Nivel:</strong> Principiante-Intermedio | <strong>Bonus:</strong> Disponible gratis online</p>
                    </div>

                    <div class="example">
                        <h4>üìñ "Applied Predictive Modeling" - Kuhn & Johnson</h4>
                        <p style="font-size: 0.9em;"><strong>üéØ Enfoque pr√°ctico:</strong> Feature engineering, validaci√≥n, interpretaci√≥n</p>
                        <p style="font-size: 0.9em;"><strong>Nivel:</strong> Intermedio | <strong>Fortaleza:</strong> Casos reales de negocio</p>
                    </div>

                    <div class="example">
                        <h4>üìñ "Machine Learning with R" - Brett Lantz</h4>
                        <p style="font-size: 0.9em;"><strong>üéØ Pr√°ctico desde cero:</strong> Perfecto para principiantes en R</p>
                        <p style="font-size: 0.9em;"><strong>Nivel:</strong> Principiante | <strong>Enfoque:</strong> Casos de estudio completos</p>
                    </div>

                    <div class="success">
                        <h4>üì¶ Paquetes clave:</h4>
                        <ul style="text-align: left; font-size: 0.9em;">
                            <li><strong>caret:</strong> Framework ML unificado</li>
                            <li><strong>randomForest:</strong> Random Forest</li>
                            <li><strong>glmnet:</strong> Regresi√≥n regularizada</li>
                            <li><strong>cluster:</strong> Algoritmos clustering</li>
                            <li><strong>tidymodels:</strong> Framework moderno</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="highlight">
                <h3>üìñ Libros Conceptuales (Independientes del lenguaje)</h3>
                <div class="two-column">
                    <div>
                        <h4>üß† "The Elements of Statistical Learning" - Hastie, Tibshirani, Friedman</h4>
                        <ul style="font-size: 0.9em;">
                            <li><strong>Nivel:</strong> Avanzado</li>
                            <li><strong>Enfoque:</strong> Fundamentos matem√°ticos profundos</li>
                            <li><strong>Ideal para:</strong> Entender teor√≠a detr√°s de algoritmos</li>
                            <li><strong>Bonus:</strong> Disponible gratis online</li>
                        </ul>
                    </div>
                    <div>
                        <h4>üéØ "Pattern Recognition and Machine Learning" - Christopher Bishop</h4>
                        <ul style="font-size: 0.9em;">
                            <li><strong>Nivel:</strong> Avanzado</li>
                            <li><strong>Enfoque:</strong> Perspectiva bayesiana</li>
                            <li><strong>Ideal para:</strong> Fundamentos probabil√≠sticos</li>
                            <li><strong>Fortaleza:</strong> Clustering y reducci√≥n dimensionalidad</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="metrics-grid">
                <div class="metric-card">
                    <span class="emoji">üåê</span>
                    <h3>Recursos Online Complementarios</h3>
                    
                    <div class="example">
                        <h4>üéì Cursos Especializados:</h4>
                        <ul style="text-align: left; font-size: 0.9em;">
                            <li><strong>fast.ai:</strong> Practical Deep Learning (enfoque top-down)</li>
                            <li><strong>Coursera - Andrew Ng:</strong> Machine Learning Course (cl√°sico)</li>
                            <li><strong>edX - MIT:</strong> Introduction to Machine Learning</li>
                        </ul>
                    </div>

                    <div class="example">
                        <h4>üìÑ Papers y Recursos:</h4>
                        <ul style="text-align: left; font-size: 0.9em;">
                            <li><strong>scikit-learn.org:</strong> Documentaci√≥n excelente con ejemplos</li>
                            <li><strong>Kaggle Learn:</strong> Micro-cursos gratuitos</li>
                            <li><strong>Towards Data Science:</strong> Art√≠culos pr√°cticos</li>
                        </ul>
                    </div>
                </div>

                <div class="metric-card">
                    <span class="emoji">üéØ</span>
                    <h3>Recomendaciones por Tema</h3>
                    
                    <div class="success">
                        <h4>üìä Para Regresi√≥n:</h4>
                        <ul style="text-align: left; font-size: 0.9em;">
                            <li><strong>Python:</strong> Cap√≠tulos 3-4 de G√©ron</li>
                            <li><strong>R:</strong> Cap√≠tulos 3, 6 de ISLR</li>
                            <li><strong>Avanzado:</strong> Cap√≠tulo 3 de ESL</li>
                        </ul>
                    </div>

                    <div class="warning">
                        <h4>üéØ Para Clasificaci√≥n:</h4>
                        <ul style="text-align: left; font-size: 0.9em;">
                            <li><strong>Python:</strong> Cap√≠tulo 3 de M√ºller</li>
                            <li><strong>R:</strong> Cap√≠tulos 4, 8, 9 de ISLR</li>
                            <li><strong>Pr√°ctico:</strong> Kuhn & Johnson Cap 11-16</li>
                        </ul>
                    </div>

                    <div class="example">
                        <h4>üîç Para Clustering:</h4>
                        <ul style="text-align: left; font-size: 0.9em;">
                            <li><strong>Python:</strong> Cap√≠tulo 3 de G√©ron</li>
                            <li><strong>R:</strong> Cap√≠tulo 10 de ISLR</li>
                            <li><strong>Te√≥rico:</strong> Bishop Cap 9, 12</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="warning">
                <h3>üí° Consejos para el Estudio</h3>
                <div class="two-column">
                    <div>
                        <h4>üöÄ Para empezar:</h4>
                        <ul style="font-size: 0.9em;">
                            <li><strong>Python:</strong> M√ºller ‚Üí G√©ron</li>
                            <li><strong>R:</strong> ISLR (gratis!) ‚Üí Kuhn & Johnson</li>
                            <li><strong>Pr√°ctica:</strong> Kaggle datasets + c√≥digo de libros</li>
                        </ul>
                    </div>
                    <div>
                        <h4>üìà Para profundizar:</h4>
                        <ul style="font-size: 0.9em;">
                            <li><strong>Teor√≠a:</strong> ESL o Bishop</li>
                            <li><strong>Implementaci√≥n:</strong> Raschka (Python)</li>
                            <li><strong>Negocio:</strong> Kuhn & Johnson casos reales</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
	
	</div>

    <div class="navigation">
        <button class="nav-btn" id="prev-btn" onclick="previousSlide()">‚¨ÖÔ∏è Anterior</button>
        <button class="nav-btn" id="next-btn" onclick="nextSlide()">Siguiente ‚û°Ô∏è</button>
    </div>

    <script>
        let currentSlide = 0;
        const slides = document.querySelectorAll('.slide');
        const totalSlides = slides.length;
        
        document.getElementById('total-slides').textContent = totalSlides;

        function showSlide(n) {
            slides.forEach(slide => slide.classList.remove('active'));
            slides[n].classList.add('active');
            
            document.getElementById('current-slide').textContent = n + 1;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = (n === 0);
            document.getElementById('next-btn').disabled = (n === totalSlides - 1);
        }

        function nextSlide() {
            if (currentSlide < totalSlides - 1) {
                currentSlide++;
                showSlide(currentSlide);
            }
        }

        function previousSlide() {
            if (currentSlide > 0) {
                currentSlide--;
                showSlide(currentSlide);
            }
        }

        // Keyboard navigation
        document.addEventListener('keydown', function(event) {
            if (event.key === 'ArrowRight' || event.key === ' ') {
                event.preventDefault();
                nextSlide();
            } else if (event.key === 'ArrowLeft') {
                event.preventDefault();
                previousSlide();
            }
        });

        // Initialize
        showSlide(0);

        // Touch/swipe support for mobile
        let touchStartX = 0;
        let touchEndX = 0;

        document.addEventListener('touchstart', function(event) {
            touchStartX = event.changedTouches[0].screenX;
        });

        document.addEventListener('touchend', function(event) {
            touchEndX = event.changedTouches[0].screenX;
            handleSwipe();
        });

        function handleSwipe() {
            const swipeThreshold = 50;
            const diff = touchStartX - touchEndX;
            
            if (Math.abs(diff) > swipeThreshold) {
                if (diff > 0) {
                    // Swipe left (next slide)
                    nextSlide();
                } else {
                    // Swipe right (previous slide)
                    previousSlide();
                }
            }
        }
    </script>
</body>
</html>