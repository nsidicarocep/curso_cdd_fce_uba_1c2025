<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aprendizaje No Supervisado</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', -apple-system, BlinkMacSystemFont, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            overflow: hidden;
            height: 100vh;
        }

        .slideshow-container {
            position: relative;
            width: 100%;
            height: 100vh;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .slide {
            display: none;
            padding: 40px 60px;
            text-align: center;
            max-width: 1100px;
            width: 95%;
            animation: slideIn 0.6s ease-out;
        }

        .slide.active {
            display: block;
        }

        @keyframes slideIn {
            from { opacity: 0; transform: translateY(30px); }
            to { opacity: 1; transform: translateY(0); }
        }

        h1 {
            font-size: 3.2em;
            margin-bottom: 30px;
            background: linear-gradient(45deg, #ffd700, #ffeb3b);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
            font-weight: 700;
        }

        h2 {
            font-size: 2.6em;
            margin-bottom: 35px;
            color: #ffd700;
            text-shadow: 1px 1px 3px rgba(0,0,0,0.3);
            font-weight: 600;
        }

        h3 {
            font-size: 1.9em;
            margin-bottom: 25px;
            color: #87ceeb;
            font-weight: 500;
        }

        p, li {
            font-size: 1.35em;
            line-height: 1.7;
            margin-bottom: 18px;
            text-shadow: 1px 1px 2px rgba(0,0,0,0.2);
        }

        ul {
            text-align: left;
            display: inline-block;
            max-width: 900px;
        }

        li {
            margin-bottom: 15px;
            padding-left: 10px;
        }

        .highlight-box {
            background: rgba(255, 215, 0, 0.15);
            border: 2px solid rgba(255, 215, 0, 0.4);
            border-radius: 15px;
            padding: 30px;
            margin: 30px 0;
            backdrop-filter: blur(10px);
        }

        .concept-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 25px;
            margin: 30px 0;
        }

        .concept-card {
            background: rgba(255, 255, 255, 0.12);
            border-radius: 15px;
            padding: 25px;
            border: 1px solid rgba(255, 255, 255, 0.2);
            backdrop-filter: blur(10px);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .concept-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 25px rgba(0,0,0,0.3);
        }

        .method-comparison {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 20px;
            margin: 25px 0;
        }

        .method-box {
            background: rgba(255, 255, 255, 0.1);
            border-radius: 12px;
            padding: 20px;
            text-align: center;
            border: 2px solid transparent;
            transition: border-color 0.3s ease;
        }

        .method-box.pca { border-color: #ff6b6b; }
        .method-box.kmeans { border-color: #4ecdc4; }
        .method-box.hierarchical { border-color: #ffa726; }

        .timeline {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin: 30px 0;
            position: relative;
        }

        .timeline::before {
            content: '';
            position: absolute;
            top: 50%;
            left: 10%;
            right: 10%;
            height: 3px;
            background: linear-gradient(90deg, #ffd700, #87ceeb);
            z-index: 1;
        }

        .timeline-item {
            background: rgba(255, 255, 255, 0.15);
            border-radius: 50%;
            width: 120px;
            height: 120px;
            display: flex;
            align-items: center;
            justify-content: center;
            text-align: center;
            z-index: 2;
            position: relative;
            border: 3px solid #ffd700;
            font-size: 0.9em;
            font-weight: bold;
        }

        .navigation {
            position: fixed;
            bottom: 30px;
            left: 50%;
            transform: translateX(-50%);
            display: flex;
            gap: 15px;
            z-index: 1000;
        }

        .nav-btn {
            background: rgba(255, 255, 255, 0.2);
            border: 2px solid rgba(255, 255, 255, 0.3);
            color: white;
            padding: 14px 28px;
            border-radius: 50px;
            cursor: pointer;
            font-size: 16px;
            font-weight: 500;
            transition: all 0.3s ease;
            backdrop-filter: blur(10px);
        }

        .nav-btn:hover {
            background: rgba(255, 255, 255, 0.3);
            transform: translateY(-3px);
            box-shadow: 0 8px 20px rgba(0,0,0,0.3);
        }

        .nav-btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
            box-shadow: none;
        }

        .slide-counter {
            position: fixed;
            top: 30px;
            right: 30px;
            background: rgba(0, 0, 0, 0.6);
            padding: 12px 24px;
            border-radius: 25px;
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.2);
            font-weight: 500;
        }

        .emoji {
            font-size: 1.3em;
            margin-right: 8px;
        }

        .pros-cons {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 25px;
            margin-top: 25px;
        }

        .pros, .cons {
            background: rgba(255, 255, 255, 0.1);
            border-radius: 15px;
            padding: 25px;
            backdrop-filter: blur(10px);
        }

        .pros {
            border-left: 5px solid #4CAF50;
        }

        .cons {
            border-left: 5px solid #f44336;
        }

        .visual-diagram {
            background: rgba(255, 255, 255, 0.1);
            border-radius: 15px;
            padding: 30px;
            margin: 25px 0;
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.2);
        }

        .title-slide {
            background: linear-gradient(135deg, rgba(102, 126, 234, 0.8) 0%, rgba(118, 75, 162, 0.8) 100%);
            backdrop-filter: blur(20px);
        }

        @media (max-width: 768px) {
            h1 { font-size: 2.5em; }
            h2 { font-size: 2em; }
            h3 { font-size: 1.6em; }
            p, li { font-size: 1.2em; }
            .concept-grid { grid-template-columns: 1fr; }
            .method-comparison { grid-template-columns: 1fr; }
            .slide { padding: 20px 30px; }
        }
    </style>
</head>
<body>
    <div class="slideshow-container">
        <div class="slide-counter">
            <span id="current-slide">1</span> / <span id="total-slides">25</span>
        </div>

        <!-- Slide 1: T√≠tulo -->
        <div class="slide active title-slide">
            <h1>Aprendizaje No Supervisado</h1>
            <h3>Descubriendo Patrones Ocultos en los Datos</h3>
            <div style="margin-top: 50px;">
                <p style="font-size: 1.4em; font-weight: 500;">PCA ‚Ä¢ K-Means ‚Ä¢ Clustering Jer√°rquico</p>
                <p style="font-size: 1.2em; margin-top: 30px; opacity: 0.9;">
                    Basado en "An Introduction to Statistical Learning"
                </p>
            </div>
        </div>

        <!-- Slide 2: ¬øQu√© es el Aprendizaje No Supervisado? -->
        <div class="slide">
            <h2>¬øQu√© es el Aprendizaje No Supervisado?</h2>
            <div class="highlight-box">
                <p style="font-size: 1.5em; margin-bottom: 25px;">
                    <span class="emoji">üîç</span> <strong>Descubrir patrones sin tener respuestas correctas</strong>
                </p>
            </div>
            <div class="concept-grid">
                <div class="concept-card">
                    <h3><span class="emoji">‚ùå</span>Sin Variable Respuesta</h3>
                    <p>No tenemos un "Y" que queramos predecir. Solo tenemos variables explicativas X‚ÇÅ, X‚ÇÇ, ..., X‚Çö</p>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">üéØ</span>Objetivo Exploratorio</h3>
                    <p>Buscamos entender la estructura subyacente de los datos y descubrir relaciones ocultas</p>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">üß©</span>Principales T√©cnicas</h3>
                    <p>Reducci√≥n de dimensionalidad (PCA) y clustering (agrupamiento de observaciones similares)</p>
                </div>
            </div>
        </div>

        <!-- Slide 3: Panorama General -->
        <div class="slide">
            <h2>Panorama de T√©cnicas</h2>
            <div class="method-comparison">
                <div class="method-box pca">
                    <h3><span class="emoji">üìä</span>PCA</h3>
                    <p><strong>Reducci√≥n de Dimensionalidad</strong></p>
                    <p>Encuentra las direcciones de m√°xima variaci√≥n en los datos</p>
                </div>
                <div class="method-box kmeans">
                    <h3><span class="emoji">üéØ</span>K-Means</h3>
                    <p><strong>Clustering Particional</strong></p>
                    <p>Divide los datos en K grupos compactos y bien separados</p>
                </div>
                <div class="method-box hierarchical">
                    <h3><span class="emoji">üå≥</span>Jer√°rquico</h3>
                    <p><strong>Clustering Jer√°rquico</strong></p>
                    <p>Construye una jerarqu√≠a de clusters mediante fusiones sucesivas</p>
                </div>
            </div>
            <div class="highlight-box">
                <p><strong>Estrategia Combinada:</strong> Estas t√©cnicas se complementan. Pod√©s usar PCA para reducir dimensiones y luego aplicar clustering sobre el espacio reducido.</p>
            </div>
        </div>

        <!-- Slide 4: PCA - Conceptos Fundamentales -->
        <div class="slide">
            <h2>Principal Component Analysis (PCA)</h2>
            <div class="visual-diagram">
                <h3><span class="emoji">üéØ</span>Objetivo Principal</h3>
                <p>Encontrar una representaci√≥n de menor dimensi√≥n que preserve la m√°xima informaci√≥n posible</p>
            </div>
            <ul>
                <li><strong>Componentes Principales:</strong> Direcciones de m√°xima variabilidad en los datos</li>
                <li><strong>Ordenados por Importancia:</strong> PC1 explica la mayor varianza, PC2 la segunda mayor, etc.</li>
                <li><strong>Ortogonales:</strong> Los componentes son independientes entre s√≠</li>
                <li><strong>Combinaci√≥n Lineal:</strong> Cada PC es una combinaci√≥n ponderada de las variables originales</li>
            </ul>
            <div class="highlight-box">
                <p><strong>Analog√≠a:</strong> Es como fotografiar una escultura 3D. Buscamos el √°ngulo que mejor capture la esencia del objeto en 2D.</p>
            </div>
        </div>

        <!-- Slide 4.1: PCA - Metodolog√≠a Paso a Paso -->
        <div class="slide">
            <h2>PCA: ¬øC√≥mo Funciona Metodol√≥gicamente?</h2>
            <div class="timeline">
                <div class="timeline-item">
                    <div>
                        <strong>1</strong><br>
                        Centrar datos
                    </div>
                </div>
                <div class="timeline-item">
                    <div>
                        <strong>2</strong><br>
                        Matriz covarianza
                    </div>
                </div>
                <div class="timeline-item">
                    <div>
                        <strong>3</strong><br>
                        Autovectores
                    </div>
                </div>
                <div class="timeline-item">
                    <div>
                        <strong>4</strong><br>
                        Ordenar
                    </div>
                </div>
                <div class="timeline-item">
                    <div>
                        <strong>5</strong><br>
                        Proyectar
                    </div>
                </div>
            </div>
            <div class="concept-grid">
                <div class="concept-card">
                    <h3><span class="emoji">‚öñÔ∏è</span>Paso 1: Centrado</h3>
                    <p><strong>X_centrado = X - Œº</strong></p>
                    <p>Restamos la media de cada variable para que el origen est√© en el centro de los datos</p>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">üìä</span>Paso 2: Covarianza</h3>
                    <p><strong>C = (1/n-1) X'X</strong></p>
                    <p>Calculamos c√≥mo var√≠an juntas todas las parejas de variables</p>
                </div>
            </div>
        </div>

        <!-- Slide 4.2: PCA - Autovectores y Autovalores -->
        <div class="slide">
            <h2>PCA: El Coraz√≥n Matem√°tico</h2>
            <div class="visual-diagram">
                <h3><span class="emoji">üßÆ</span>Autovectores y Autovalores</h3>
                <p style="font-size: 1.5em; margin: 20px 0;"><strong>C √ó v = Œª √ó v</strong></p>
                <p>Donde C es la matriz de covarianza, v es el autovector, Œª es el autovalor</p>
            </div>
            <div class="concept-grid">
                <div class="concept-card">
                    <h3><span class="emoji">üß≠</span>Autovector (v)</h3>
                    <p>Define la <strong>direcci√≥n</strong> del componente principal</p>
                    <p>Cada elemento indica cu√°nto contribuye cada variable original al componente</p>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">üìè</span>Autovalor (Œª)</h3>
                    <p>Indica cu√°nta <strong>varianza</strong> explica ese componente</p>
                    <p>Autovalores grandes = componentes importantes</p>
                </div>
            </div>
            <div class="highlight-box">
                <p><strong>Interpretaci√≥n Clave:</strong> Los autovectores nos dicen las direcciones √≥ptimas, y los autovalores nos dicen qu√© tan importantes son esas direcciones para explicar la variabilidad total de los datos.</p>
            </div>
        </div>

        <!-- Slide 4.3: PCA - Cargas e Interpretaci√≥n -->
        <div class="slide">
            <h2>PCA: Cargas e Interpretaci√≥n</h2>
            <div class="visual-diagram">
                <h3><span class="emoji">üîç</span>¬øQu√© son las Cargas?</h3>
                <p>Las cargas son los elementos de los autovectores normalizados</p>
                <p style="font-size: 1.3em; margin: 15px 0;"><strong>PC‚ÇÅ = w‚ÇÅ‚ÇÅ√óX‚ÇÅ + w‚ÇÅ‚ÇÇ√óX‚ÇÇ + ... + w‚ÇÅ‚Çö√óX‚Çö</strong></p>
            </div>
            <div class="concept-grid">
                <div class="concept-card">
                    <h3><span class="emoji">üìà</span>Interpretaci√≥n</h3>
                    <ul style="text-align: left;">
                        <li><strong>|w| > 0.5:</strong> Variable importante</li>
                        <li><strong>w positivo:</strong> Correlaci√≥n positiva</li>
                        <li><strong>w negativo:</strong> Correlaci√≥n negativa</li>
                        <li><strong>w ‚âà 0:</strong> Variable irrelevante para este PC</li>
                    </ul>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">üí°</span>Ejemplo Pr√°ctico</h3>
                    <p><strong>PC1 en datos de estudiantes:</strong></p>
                    <p>Matem√°tica: 0.7, F√≠sica: 0.8, Literatura: 0.1</p>
                    <p><em>Interpretaci√≥n: "Habilidad en ciencias exactas"</em></p>
                </div>
            </div>
        </div>

        <!-- Slide 5: PCA - Cu√°ndo y Por Qu√© Usarlo -->
        <div class="slide">
            <h2>¬øCu√°ndo Usar PCA?</h2>
            <div class="concept-grid">
                <div class="concept-card">
                    <h3><span class="emoji">üìê</span>Alta Dimensionalidad</h3>
                    <p>Cuando ten√©s muchas variables correlacionadas que pueden estar midiendo conceptos similares</p>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">üëÅÔ∏è</span>Visualizaci√≥n</h3>
                    <p>Para explorar datos complejos proyect√°ndolos en 2D o 3D de manera informativa</p>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">‚ö°</span>Preprocesamiento</h3>
                    <p>Como paso previo para acelerar otros algoritmos de machine learning</p>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">üîá</span>Reducci√≥n de Ruido</h3>
                    <p>Los componentes menores a menudo capturan ruido que puede ser descartado</p>
                </div>
            </div>
        </div>

        <!-- Slide 6: PCA - Selecci√≥n de Componentes -->
        <div class="slide">
            <h2>¬øCu√°ntos Componentes Conservar?</h2>
            <div class="timeline">
                <div class="timeline-item">
                    <div>
                        <strong>Varianza Explicada</strong><br>
                        80-95% del total
                    </div>
                </div>
                <div class="timeline-item">
                    <div>
                        <strong>Gr√°fico de Codo</strong><br>
                        Buscar el "quiebre"
                    </div>
                </div>
                <div class="timeline-item">
                    <div>
                        <strong>Regla de Kaiser</strong><br>
                        Autovalores > 1
                    </div>
                </div>
                <div class="timeline-item">
                    <div>
                        <strong>Objetivo del An√°lisis</strong><br>
                        Seg√∫n el problema
                    </div>
                </div>
            </div>
            <div class="highlight-box">
                <p><strong>Recomendaci√≥n Pr√°ctica:</strong> Comenz√° con suficientes componentes para explicar al menos 85% de la varianza, despu√©s evalu√° si pod√©s reducir m√°s sin perder informaci√≥n crucial.</p>
            </div>
        </div>

        <!-- Slide 7: K-Means - Conceptos Fundamentales -->
        <div class="slide">
            <h2>K-Means Clustering</h2>
            <div class="visual-diagram">
                <h3><span class="emoji">üéØ</span>Objetivo</h3>
                <p>Dividir n observaciones en k clusters, donde cada observaci√≥n pertenece al cluster con el centroide m√°s cercano</p>
            </div>
            <div class="timeline">
                <div class="timeline-item">
                    <div>
                        <strong>1</strong><br>
                        Elegir K
                    </div>
                </div>
                <div class="timeline-item">
                    <div>
                        <strong>2</strong><br>
                        Inicializar centroides
                    </div>
                </div>
                <div class="timeline-item">
                    <div>
                        <strong>3</strong><br>
                        Asignar puntos
                    </div>
                </div>
                <div class="timeline-item">
                    <div>
                        <strong>4</strong><br>
                        Recalcular centroides
                    </div>
                </div>
                <div class="timeline-item">
                    <div>
                        <strong>5</strong><br>
                        Repetir hasta convergencia
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 7.1: K-Means - Funci√≥n Objetivo -->
        <div class="slide">
            <h2>K-Means: ¬øQu√© Minimiza Exactamente?</h2>
            <div class="visual-diagram">
                <h3><span class="emoji">üéØ</span>Funci√≥n Objetivo (WCSS)</h3>
                <p style="font-size: 1.8em; margin: 25px 0;"><strong>W(C) = ¬Ω ‚àë·µ¢‚Çå‚ÇÅ·¥∑ ‚àë‚Çì‚ààC·µ¢ ||x - Œº·µ¢||¬≤</strong></p>
                <p>Suma de cuadrados dentro de clusters (Within-Cluster Sum of Squares)</p>
            </div>
            <div class="concept-grid">
                <div class="concept-card">
                    <h3><span class="emoji">üìä</span>Componentes</h3>
                    <ul style="text-align: left;">
                        <li><strong>C:</strong> Asignaci√≥n de clusters</li>
                        <li><strong>C·µ¢:</strong> Conjunto de puntos en cluster i</li>
                        <li><strong>Œº·µ¢:</strong> Centroide del cluster i</li>
                        <li><strong>||x - Œº·µ¢||¬≤:</strong> Distancia euclidiana al cuadrado</li>
                    </ul>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">üéØ</span>¬øQu√© Logra?</h3>
                    <p>Busca clusters donde los puntos est√©n lo m√°s cerca posible de su centroide</p>
                    <p><strong>Resultado:</strong> Clusters compactos y bien definidos</p>
                </div>
            </div>
        </div>

        <!-- Slide 7.2: K-Means - ¬øPor Qu√© Solo Distancia Euclidiana? -->
        <div class="slide">
            <h2>K-Means: ¬øPor Qu√© Solo Distancia Euclidiana?</h2>
            <div class="highlight-box">
                <h3><span class="emoji">üîë</span>Conexi√≥n Matem√°tica Fundamental</h3>
                <p>K-means minimiza varianza dentro de clusters, y la varianza se define usando distancia euclidiana</p>
            </div>
            <div class="concept-grid">
                <div class="concept-card">
                    <h3><span class="emoji">üìê</span>Centroide = Media Aritm√©tica</h3>
                    <p><strong>Œº·µ¢ = (1/|C·µ¢|) ‚àë‚Çì‚ààC·µ¢ x</strong></p>
                    <p>El centroide que minimiza la suma de distancias euclidianas al cuadrado es la media aritm√©tica</p>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">‚ö†Ô∏è</span>Otras Distancias</h3>
                    <p><strong>Manhattan:</strong> Centroide = mediana</p>
                    <p><strong>Coseno:</strong> Centroide = vector promedio normalizado</p>
                    <p><em>¬°Requieren algoritmos diferentes!</em></p>
                </div>
            </div>
            <div class="visual-diagram">
                <p><strong>Por eso:</strong> Si necesit√°s otra m√©trica de distancia, us√° <strong>K-medoids</strong> o <strong>clustering jer√°rquico</strong> con la distancia apropiada.</p>
            </div>
        </div>

        <!-- Slide 7.3: K-Means - Algoritmo Detallado -->
        <div class="slide">
            <h2>K-Means: Algoritmo Paso a Paso</h2>
            <div class="concept-grid">
                <div class="concept-card">
                    <h3><span class="emoji">üé≤</span>Inicializaci√≥n</h3>
                    <p><strong>Problema:</strong> Sensible a inicializaci√≥n</p>
                    <p><strong>Soluci√≥n:</strong> K-means++ elige centroides iniciales inteligentemente</p>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">üìè</span>Asignaci√≥n</h3>
                    <p><strong>Para cada punto x:</strong></p>
                    <p>Asignar al cluster j donde:</p>
                    <p><strong>j = argmin ||x - Œº‚±º||¬≤</strong></p>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">üîÑ</span>Actualizaci√≥n</h3>
                    <p><strong>Para cada cluster i:</strong></p>
                    <p>Recalcular centroide:</p>
                    <p><strong>Œº·µ¢ = (1/|C·µ¢|) ‚àë‚Çì‚ààC·µ¢ x</strong></p>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">‚èπÔ∏è</span>Convergencia</h3>
                    <p><strong>Parar cuando:</strong></p>
                    <ul style="text-align: left;">
                        <li>Centroides no cambian</li>
                        <li>Asignaciones no cambian</li>
                        <li>WCSS no mejora</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Slide 8: K-Means - Selecci√≥n de K -->
        <div class="slide">
            <h2>¬øC√≥mo Elegir el N√∫mero de Clusters?</h2>
            <div class="concept-grid">
                <div class="concept-card">
                    <h3><span class="emoji">üìà</span>M√©todo del Codo</h3>
                    <p><strong>Qu√© es:</strong> Graficamos la inercia vs n√∫mero de clusters</p>
                    <p><strong>C√≥mo:</strong> Buscamos el punto donde la mejora se vuelve marginal</p>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">üìä</span>Silhouette Score</h3>
                    <p><strong>Qu√© es:</strong> Mide qu√© tan bien asignado est√° cada punto</p>
                    <p><strong>C√≥mo:</strong> Elegimos K con el score m√°ximo</p>
                </div>
            </div>
            <div class="highlight-box">
                <p><strong>Estrategia Recomendada:</strong> Us√° ambos m√©todos y comparalos. Si coinciden, perfecto. Si no, analiz√° m√°s profundamente tus datos y consider√° el contexto del problema.</p>
            </div>
        </div>

        <!-- Slide 8.1: Silhouette Score - Metodolog√≠a Detallada -->
        <div class="slide">
            <h2>Silhouette Score: An√°lisis Metodol√≥gico</h2>
            <div class="visual-diagram">
                <h3><span class="emoji">üìè</span>F√≥rmula del Silhouette Score</h3>
                <p style="font-size: 1.6em; margin: 20px 0;"><strong>s(i) = (b(i) - a(i)) / max(a(i), b(i))</strong></p>
            </div>
            <div class="concept-grid">
                <div class="concept-card">
                    <h3><span class="emoji">üéØ</span>a(i): Cohesi√≥n</h3>
                    <p><strong>Distancia promedio</strong> del punto i a otros puntos en su mismo cluster</p>
                    <p><em>Menor es mejor ‚Üí clusters compactos</em></p>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">‚ÜîÔ∏è</span>b(i): Separaci√≥n</h3>
                    <p><strong>Distancia promedio</strong> del punto i al cluster m√°s cercano (que no es el suyo)</p>
                    <p><em>Mayor es mejor ‚Üí clusters separados</em></p>
                </div>
            </div>
            <div class="highlight-box">
                <h3><span class="emoji">üìä</span>Interpretaci√≥n del Score</h3>
                <ul style="text-align: left; max-width: 100%;">
                    <li><strong>s(i) ‚âà 1:</strong> Punto muy bien asignado a su cluster</li>
                    <li><strong>s(i) ‚âà 0:</strong> Punto en la frontera entre clusters</li>
                    <li><strong>s(i) ‚âà -1:</strong> Punto probablemente mal asignado</li>
                </ul>
            </div>
        </div>

        <!-- Slide 9: K-Means - Caracter√≠sticas Importantes -->
        <div class="slide">
            <h2>Caracter√≠sticas Clave de K-Means</h2>
            <div class="pros-cons">
                <div class="pros">
                    <h3><span class="emoji">‚úÖ</span>Fortalezas</h3>
                    <ul>
                        <li>Simple y computacionalmente eficiente</li>
                        <li>Funciona bien con clusters esf√©ricos</li>
                        <li>Escalable a datasets grandes</li>
                        <li>Garantiza convergencia</li>
                    </ul>
                </div>
                <div class="cons">
                    <h3><span class="emoji">‚ö†Ô∏è</span>Limitaciones</h3>
                    <ul>
                        <li>Requiere especificar K de antemano</li>
                        <li>Sensible a la inicializaci√≥n</li>
                        <li>Asume clusters de forma esf√©rica</li>
                        <li>Sensible a outliers</li>
                    </ul>
                </div>
            </div>
            <div class="highlight-box">
                <p><strong>Importante:</strong> K-means usa exclusivamente distancia euclidiana. Para otras m√©tricas de distancia, consider√° algoritmos como K-medoids.</p>
            </div>
        </div>

        <!-- Slide 10: Clustering Jer√°rquico - Conceptos -->
        <div class="slide">
            <h2>Clustering Jer√°rquico</h2>
            <div class="concept-grid">
                <div class="concept-card">
                    <h3><span class="emoji">üî∫</span>Aglomerativo</h3>
                    <p><strong>Bottom-up:</strong> Comienza con n clusters individuales y los fusiona sucesivamente</p>
                    <p>M√°s com√∫n en la pr√°ctica</p>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">üîª</span>Divisivo</h3>
                    <p><strong>Top-down:</strong> Comienza con todos los puntos en un cluster y los divide</p>
                    <p>Computacionalmente m√°s complejo</p>
                </div>
            </div>
            <div class="highlight-box">
                <p><strong>Ventaja Principal:</strong> No necesit√°s especificar el n√∫mero de clusters de antemano. El dendrograma te muestra la estructura a todos los niveles.</p>
            </div>
        </div>

        <!-- Slide 11: M√©todos de Enlace -->
        <div class="slide">
            <h2>M√©todos de Enlace (Linkage)</h2>
            <ul style="max-width: 100%;">
                <li><strong><span class="emoji">üîó</span>Complete Linkage:</strong> Distancia m√°xima entre clusters ‚Üí produce clusters compactos</li>
                <li><strong><span class="emoji">üîó</span>Single Linkage:</strong> Distancia m√≠nima ‚Üí puede producir clusters alargados, sensible a outliers</li>
                <li><strong><span class="emoji">üîó</span>Average Linkage:</strong> Distancia promedio ‚Üí compromiso balanceado</li>
                <li><strong><span class="emoji">üîó</span>Ward's Linkage:</strong> Minimiza varianza intra-cluster ‚Üí favorece clusters esf√©ricos</li>
            </ul>
            <div class="highlight-box">
                <p><strong>Recomendaci√≥n:</strong> Complete y Average linkage son los m√°s utilizados en la pr√°ctica. Ward es excelente cuando esper√°s clusters de tama√±o similar.</p>
            </div>
        </div>

        <!-- Slide 12: Dendrograma -->
        <div class="slide">
            <h2>El Dendrograma</h2>
            <div class="visual-diagram">
                <h3><span class="emoji">üå≥</span>Interpretaci√≥n del Dendrograma</h3>
                <ul style="text-align: left; max-width: 100%;">
                    <li><strong>Altura:</strong> Representa la distancia a la cual se fusionan los clusters</li>
                    <li><strong>Hojas:</strong> Las observaciones individuales</li>
                    <li><strong>Nodos internos:</strong> Los clusters formados por fusi√≥n</li>
                    <li><strong>Corte horizontal:</strong> Determina el n√∫mero final de clusters</li>
                </ul>
            </div>
            <div class="highlight-box">
                <p><strong>Clave para la Interpretaci√≥n:</strong> Busc√° "saltos" grandes en las alturas del dendrograma. Estos indican separaciones naturales en tus datos.</p>
            </div>
        </div>

        <!-- Slide 13: Comparaci√≥n de M√©todos -->
        <div class="slide">
            <h2>Comparaci√≥n de M√©todos de Clustering</h2>
            <div class="method-comparison">
                <div class="method-box kmeans">
                    <h3>K-Means</h3>
                    <p><strong>Complejidad:</strong> O(n)</p>
                    <p><strong>Clusters:</strong> Hay que especificar K</p>
                    <p><strong>Forma:</strong> Asume esf√©ricos</p>
                    <p><strong>Dataset:</strong> Grande (&gt;10,000)</p>
                </div>
                <div class="method-box hierarchical">
                    <h3>Jer√°rquico</h3>
                    <p><strong>Complejidad:</strong> O(n¬≥)</p>
                    <p><strong>Clusters:</strong> No hay que especificar K</p>
                    <p><strong>Forma:</strong> Flexible</p>
                    <p><strong>Dataset:</strong> Peque√±o-Mediano</p>
                </div>
                <div class="method-box pca">
                    <h3>PCA + Clustering</h3>
                    <p><strong>Estrategia:</strong> H√≠brida</p>
                    <p><strong>Ventaja:</strong> Reduce ruido</p>
                    <p><strong>Uso:</strong> Alta dimensionalidad</p>
                    <p><strong>Resultado:</strong> M√°s interpretable</p>
                </div>
            </div>
        </div>

        <!-- Slide 14: Cu√°ndo Usar Cada M√©todo -->
        <div class="slide">
            <h2>¬øCu√°ndo Usar Cada M√©todo?</h2>
            <div class="concept-grid">
                <div class="concept-card">
                    <h3><span class="emoji">üìä</span>Us√° PCA cuando...</h3>
                    <ul style="text-align: left;">
                        <li>Ten√©s muchas variables correlacionadas</li>
                        <li>Necesit√°s visualizar datos complejos</li>
                        <li>Quer√©s acelerar otros algoritmos</li>
                        <li>Busc√°s reducir ruido en los datos</li>
                    </ul>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">üéØ</span>Us√° K-Means cuando...</h3>
                    <ul style="text-align: left;">
                        <li>Ten√©s una idea de cu√°ntos grupos esperar</li>
                        <li>Los clusters son aproximadamente esf√©ricos</li>
                        <li>Trabaj√°s con datasets grandes</li>
                        <li>Necesit√°s velocidad de procesamiento</li>
                    </ul>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">üå≥</span>Us√° Jer√°rquico cuando...</h3>
                    <ul style="text-align: left;">
                        <li>No sab√©s cu√°ntos clusters esperar</li>
                        <li>La jerarqu√≠a es importante para tu an√°lisis</li>
                        <li>Ten√©s datasets peque√±os-medianos</li>
                        <li>Quer√©s an√°lisis exploratorio profundo</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Slide 15: Mejores Pr√°cticas -->
        <div class="slide">
            <h2>Mejores Pr√°cticas en Aprendizaje No Supervisado</h2>
            <div class="timeline">
                <div class="timeline-item">
                    <div>
                        <strong>Exploraci√≥n</strong><br>
                        Conoc√© tus datos
                    </div>
                </div>
                <div class="timeline-item">
                    <div>
                        <strong>Preprocesamiento</strong><br>
                        Normalizaci√≥n clave
                    </div>
                </div>
                <div class="timeline-item">
                    <div>
                        <strong>M√∫ltiples Enfoques</strong><br>
                        Prob√° varias t√©cnicas
                    </div>
                </div>
                <div class="timeline-item">
                    <div>
                        <strong>Validaci√≥n</strong><br>
                        Interpretabilidad
                    </div>
                </div>
            </div>
            <ul>
                <li><strong><span class="emoji">‚öñÔ∏è</span>Estandarizaci√≥n:</strong> Siempre estandariz√° variables cuando tienen diferentes escalas</li>
                <li><strong><span class="emoji">üîç</span>Outliers:</strong> Identificalos y decid√≠ si removerlos o usar m√©todos robustos</li>
                <li><strong><span class="emoji">üß™</span>Experimentaci√≥n:</strong> Prob√° diferentes par√°metros y combinaciones de m√©todos</li>
                <li><strong><span class="emoji">üí°</span>Interpretaci√≥n:</strong> Los resultados deben tener sentido en el contexto del problema</li>
            </ul>
        </div>

        <!-- Slide 16: Evaluaci√≥n y Validaci√≥n -->
        <div class="slide">
            <h2>¬øC√≥mo Evaluar Resultados?</h2>
            <div class="concept-grid">
                <div class="concept-card">
                    <h3><span class="emoji">üìä</span>M√©tricas Internas</h3>
                    <ul style="text-align: left;">
                        <li>Silhouette Score</li>
                        <li>Calinski-Harabasz Index</li>
                        <li>Davies-Bouldin Index</li>
                        <li>Varianza explicada (PCA)</li>
                    </ul>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">üëÅÔ∏è</span>Evaluaci√≥n Visual</h3>
                    <ul style="text-align: left;">
                        <li>Gr√°ficos de dispersi√≥n</li>
                        <li>Dendrogramas</li>
                        <li>Heatmaps de correlaci√≥n</li>
                        <li>Biplots (PCA)</li>
                    </ul>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">üß†</span>Validaci√≥n de Dominio</h3>
                    <ul style="text-align: left;">
                        <li>¬øLos clusters tienen sentido?</li>
                        <li>¬øLos componentes son interpretables?</li>
                        <li>¬øLos resultados son estables?</li>
                        <li>¬øSon √∫tiles para el objetivo?</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Slide 17: Aplicaciones Pr√°cticas -->
        <div class="slide">
            <h2>Aplicaciones en el Mundo Real</h2>
            <div class="concept-grid">
                <div class="concept-card">
                    <h3><span class="emoji">üõí</span>Marketing y Negocios</h3>
                    <ul style="text-align: left;">
                        <li>Segmentaci√≥n de clientes</li>
                        <li>An√°lisis de mercado</li>
                        <li>Sistemas de recomendaci√≥n</li>
                    </ul>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">üß¨</span>Ciencias Biol√≥gicas</h3>
                    <ul style="text-align: left;">
                        <li>An√°lisis gen√≥mico</li>
                        <li>Clasificaci√≥n de especies</li>
                        <li>An√°lisis de expresi√≥n g√©nica</li>
                    </ul>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">üåê</span>Tecnolog√≠a</h3>
                    <ul style="text-align: left;">
                        <li>Compresi√≥n de im√°genes</li>
                        <li>Detecci√≥n de anomal√≠as</li>
                        <li>An√°lisis de redes sociales</li>
                    </ul>
                </div>
                <div class="concept-card">
                    <h3><span class="emoji">üí∞</span>Finanzas</h3>
                    <ul style="text-align: left;">
                        <li>Gesti√≥n de riesgo</li>
                        <li>Detecci√≥n de fraude</li>
                        <li>An√°lisis de portafolios</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Slide 18: Conclusiones -->
        <div class="slide">
            <h2>Conclusiones y Reflexiones Finales</h2>
            <div class="highlight-box">
                <h3><span class="emoji">üéØ</span>Puntos Clave para Recordar</h3>
                <ul style="text-align: left; max-width: 100%;">
                    <li><strong>El aprendizaje no supervisado</strong> es fundamental para la exploraci√≥n y comprensi√≥n de datos</li>
                    <li><strong>No hay una t√©cnica √∫nica</strong> que funcione mejor en todos los casos</li>
                    <li><strong>La interpretabilidad</strong> es tan importante como la precisi√≥n t√©cnica</li>
                    <li><strong>El preprocesamiento adecuado</strong> es crucial para obtener resultados significativos</li>
                </ul>
            </div>
            <div class="visual-diagram">
                <h3><span class="emoji">üöÄ</span>El Proceso T√≠pico</h3>
                <p>Exploraci√≥n ‚Üí Preprocesamiento ‚Üí Aplicaci√≥n de t√©cnicas ‚Üí Evaluaci√≥n ‚Üí Interpretaci√≥n ‚Üí Refinamiento</p>
            </div>
            <div style="margin-top: 40px;">
                <p style="font-size: 1.4em; font-style: italic; color: #ffd700;">
                    "El objetivo no es solo agrupar datos o reducir dimensiones, sino descubrir conocimiento que nos ayude a entender mejor nuestro dominio de aplicaci√≥n"
                </p>
            </div>
        </div>
    </div>

    <div class="navigation">
        <button class="nav-btn" id="prevBtn" onclick="changeSlide(-1)">‚Üê Anterior</button>
        <button class="nav-btn" id="nextBtn" onclick="changeSlide(1)">Siguiente ‚Üí</button>
    </div>

    <script>
        let currentSlide = 1;
        const totalSlides = document.querySelectorAll('.slide').length;
        
        document.getElementById('total-slides').textContent = totalSlides;

        function showSlide(n) {
            const slides = document.querySelectorAll('.slide');
            
            if (n > totalSlides) currentSlide = 1;
            if (n < 1) currentSlide = totalSlides;
            
            slides.forEach(slide => slide.classList.remove('active'));
            slides[currentSlide - 1].classList.add('active');
            
            document.getElementById('current-slide').textContent = currentSlide;
            
            // Update navigation buttons
            document.getElementById('prevBtn').disabled = currentSlide === 1;
            document.getElementById('nextBtn').disabled = currentSlide === totalSlides;
        }

        function changeSlide(n) {
            currentSlide += n;
            showSlide(currentSlide);
        }

        // Keyboard navigation
        document.addEventListener('keydown', function(event) {
            if (event.key === 'ArrowLeft') changeSlide(-1);
            if (event.key === 'ArrowRight' || event.key === ' ') changeSlide(1);
            if (event.key === 'Home') {
                currentSlide = 1;
                showSlide(currentSlide);
            }
            if (event.key === 'End') {
                currentSlide = totalSlides;
                showSlide(currentSlide);
            }
        });

        // Initialize
        showSlide(currentSlide);
    </script>
</body>
</html>